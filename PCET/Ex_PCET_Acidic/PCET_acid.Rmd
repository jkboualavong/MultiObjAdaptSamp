---
title: "Proof of concept: CO2 capture with PCET-based pH swings, Acid additions"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook uses a newly developed Gaussian Process-based method to solve for the near-optimal set of solutions for CO2 capture using proton coupled electron transfers (PCET) to drive pH swings. 
The work assumes that the redox molecule is a quinone, and restricts the search to only combinations of properties that a quinone is likely to have.
Tthe performance of CO2 capture, defined by minimum energy demand and the CO2 capture flux, will be determined using thermodynamic and kinetics equations established in the literature.
The resulting Pareto frontier of this bi-objective problem is solved using the GPareto package, then characterized to define acceptable sub-optimal performance in the likely event that a compound with the exact specifications of any of the Pareto frontier estimates not exist.
The set of results that are competitive with MEA-based capture will be solved using the adaptive sampling method (see other mathematical examples for details and description of the algorithm) to describe what criteria the optimal quinone and solution composition should have.

# Code Initialization

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and load packages

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
```

## Parameter space
As established in the preparatory calculations (/../Prep), data for restricting the quinone features is based on results from Huynh et al. J Am Chem Soc. 2016 December 14; 138(49): 15903â€“15910. doi:10.1021/jacs.6b05797.

pKa1: 0 to 13.41
pKa2 = pKa1 + error
error ~ unif(0, 5.5)

The concentrations of quinone are assumed to be a minimum of 10 mM (approximately solubility of BQ from experiments) and 3 M (based on tiron, which has a solubility of about 1 M [Huang 2019])

## Objective Functions

See the preparatory file (/../Prep) for a full description of the model.
For this script, we assume that the additional pH correction can only be an acid, as the concentration of the species spans multiple orders of magnitude.
For consistency in notation with the base addition script, we use "Na" as the variable name, as the most common base would be NaOH.

```{r PCET Explicit functions}
# Direct explicit functions
# Functions are named with the output variable first, then all relevant inputs
DIC.xA.pCO2.pH.A.k.beta = function(xA, pCO2, pH, A.tot, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Proton concentration
  H = 10^-pH
  
  # Inorganic carbonate
  CO3.free = kH * pCO2 * (H^2 + kc1 * H + kc1 * kc2) / H^2
  
  # Bound carbon
  CO2.bound = A.tot*xA *k1*k2*(beta1*pCO2 + 2*beta2*pCO2^2)/((1 + beta1*pCO2 + beta2*pCO2^2)*k1*k2 + k1*H + H^2)
  
  return(CO3.free + CO2.bound)
}

pH.xA.pCO2.A.k.beta.Na = function(xA, P, At, k1, k2, beta1, beta2, Na){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Polynomial root
  x5 = 1
  x4 = k1 + Na + 2*At*xA 
  x3 = k1*k2 - kw + k1*Na + beta1*k1*k2*P - kc1*kH*P +
      beta2*k1*k2*P^2 + 2*At*k1*xA - 2*At*k2*xA
  x2 = (-k1)*kw + k1*k2*Na - k1*kc1*kH*P - 2*kc1*kc2*kH*P + beta1*k1*k2*Na*P + beta2*k1*k2*Na*P^2 -
      2*At*k1*k2*xA + 2*At*beta1*k1*k2*P*xA + 2*At*beta2*k1*k2*P^2*xA
  x1 = (-k1)*k2*kw - k1*k2*kc1*kH*P - 2*k1*kc1*kc2*kH*P - beta1*k1*k2*kw*P -
      beta1*k1*k2*kc1*kH*P^2 - beta2*k1*k2*kw*P^2 - beta2*k1*k2*kc1*kH*P^3
  x0 = - 2*k1*k2*kc1*kc2*kH*P - 2*beta1*k1*k2*kc1*kc2*kH*P^2 -
     2*beta2*k1*k2*kc1*kc2*kH*P^3
  roots = polyroot(c(x0, x1, x2, x3, x4, x5))
  
  # Only the real and positive roots
  H = roots[abs(Im(roots)) < 1e-8]
  H = Re(H[Re(H) > 0])
  
  # It is possible for multiple roots to satisfy the solution. Typical pH is going to be the one closest to 7-8
  # H = H[which.min(abs(-log10(H) - 7))]

  return(-log10(H[1]))
}

pCO2.xA.pH.A.k.beta.Na = function(xA, pH, At, k1, k2, beta1, beta2, Na, pCO2.prev){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  # Proton concentration
  H = 10^-pH
  
  # Polynomial root
  x3 = (-beta2)*H*k1*k2*kc1*kH - 2*beta2*k1*k2*kc1*kc2*kH
  x2 = beta2*H^3*k1*k2 - beta1*H*k1*k2*kc1*kH - 2*beta1*k1*k2*kc1*kc2*kH -
      beta2*H*k1*k2*kw + beta2*H^2*k1*k2*Na + 2*At*beta2*H^2*k1*k2*xA
  x1 = beta1*H^3*k1*k2 - H^3*kc1*kH - H^2*k1*kc1*kH - H*k1*k2*kc1*kH - 2*H^2*kc1*kc2*kH - 2*H*k1*kc1*kc2*kH -
      2*k1*k2*kc1*kc2*kH - beta1*H*k1*k2*kw + beta1*H^2*k1*k2*Na +
      2*At*beta1*H^2*k1*k2*xA
  x0 = H^5 + H^4*k1 + H^3*k1*k2 - H^3*kw - H^2*k1*kw - H*k1*k2*kw +
      H^4*Na + H^3*k1*Na + H^2*k1*k2*Na + 2*At*H^4*xA + 2*At*H^3*k1*xA -
      2*At*H^3*k2*xA - 2*At*H^2*k1*k2*xA
  roots = polyroot(c(x0, x1, x2, x3))
  
  # Only the real and positive roots
  pCO2 = roots[abs(Im(roots)) < 1e-8]
  pCO2 = Re(pCO2[Re(pCO2) > 0])
  # There are cases of multiepl roots. Find the one that is closest to the previous known value
  pCO2 = pCO2[which.min(abs(log10(pCO2) - log10(pCO2.prev)))]
  
  return(pCO2)
}

pH.DIC.xA.pCO2.A.k.beta = function(DIC, xA, P, At, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Polynomial root
  x4 = (DIC - kH*P)
  x3 = (DIC*k1 - k1*kH*P - kc1*kH*P)
  x2 = (DIC*k1*k2 + beta1*DIC*k1*k2*P - k1*k2*kH*P - k1*kc1*kH*P - kc1*kc2*kH*P + beta2*DIC*k1*k2*P^2 - 
    beta1*k1*k2*kH*P^2 - beta2*k1*k2*kH*P^3 - At*beta1*k1*k2*P*xA - 2*At*beta2*k1*k2*P^2*xA)
  x1 = ((-k1)*k2*kc1*kH*P - k1*kc1*kc2*kH*P - beta1*k1*k2*kc1*kH*P^2 - 
        beta2*k1*k2*kc1*kH*P^3)
  x0 = (-k1)*k2*kc1*kc2*kH*P - beta1*k1*k2*kc1*kc2*kH*P^2 - beta2*k1*k2*kc1*kc2*kH*P^3
  roots = polyroot(c(x0, x1, x2, x3, x4))
  
  # Only the real and positive roots
  H = roots[abs(Im(roots)) < 1e-8]
  H = Re(H[Re(H) > 0])
  return(-log10(H))
}

pCO2.DIC.xA.pH.A.k.beta = function(DIC, xA, pH, At, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  H = 10^-pH
  
  # Polynomial root
  x3 = ((-beta2)*H^2*k1*k2*kH - beta2*H*k1*k2*kc1*kH - beta2*k1*k2*kc1*kc2*kH)
  x2 = (beta2*DIC*H^2*k1*k2 - beta1*H^2*k1*k2*kH - beta1*H*k1*k2*kc1*kH - 
        beta1*k1*k2*kc1*kc2*kH - 2*At*beta2*H^2*k1*k2*xA)
  x1 = (beta1*DIC*H^2*k1*k2 - H^4*kH - H^3*k1*kH - H^2*k1*k2*kH - H^3*kc1*kH - 
        H^2*k1*kc1*kH - H*k1*k2*kc1*kH - H^2*kc1*kc2*kH - H*k1*kc1*kc2*kH - 
        k1*k2*kc1*kc2*kH - At*beta1*H^2*k1*k2*xA)
  x0 = DIC*H^4 + DIC*H^3*k1 + DIC*H^2*k1*k2
  roots = polyroot(c(x0, x1, x2, x3))
  
  # Only the real and positive roots
  pCO2 = roots[abs(Im(roots)) < 1e-8]
  pCO2 = Re(pCO2[Re(pCO2) > 0])
  return(pCO2)
}

# There are cases in the process where both pH and pCO2 are unknown. 
# For those cases, both variables can be solved togther, but it leads to coupled nonlinear root finding problems. 
# Initial testing of the equations has found that using an initial guess of pH (such as the pH at the immediately previous state of charge) leads to a good enough estimate of the pH to solve pCO2.
pH.it.guess.DIC.At.k.beta = function(pH.guess, xA.next, DIC, A.tot, k1, k2, beta1, beta2, Na){
  # Iterates to solve the pH and pCO2 at the next electrochemical time step, 
  # given xA and DIC and an initial guess (the pH at the previous time step)
  pCO2.it = c(); pH.it = c(pH.guess)
  pCO2.it = pCO2.DIC.xA.pH.A.k.beta(DIC = DIC, xA = xA.next, pH = pH.it, At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  for(n in 2:74){
    pH.it[n] = pH.xA.pCO2.A.k.beta.Na(xA = xA.next, P = pCO2.it[n - 1], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    pCO2.it[n] = pCO2.DIC.xA.pH.A.k.beta(DIC = DIC, xA = xA.next, pH = pH.it[n], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  } 
  n = 7:5
  pH.it[n] = pH.xA.pCO2.A.k.beta.Na(xA = xA.next, P = pCO2.it[n - 1], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  # Due to some oscillatory instabilities under specific conditions, take the last 25 and use the value that is closest to the guess
  pH.res = pH.it[50:75]
  pH.res = pH.res[which.min(abs(pH.res) - pH.guess)]
  return(pH.res)
}
```

```{r PCET Derived Functions: Process Conditions}
# Derived functions
# DIC difference: CO2/L*cycle - this is a good first check for the condition to ensure that CO2 is, in fact, captured, represented by a positive value.
DIC.diff = function(Na, A, beta1, beta2, k1, k2, pCO2.in, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  # pCO2.in = 0.1; pCO2.out = 1
  
  # Absorption: low P, high xA
  start.soln = data.frame(p.CO2 = pCO2.in, xA = max(xA.lim))
  start.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = start.soln$xA, P = start.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  start.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = start.soln$xA, pCO2 = start.soln$p.CO2, pH = start.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Desorption: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Calculate the difference
  DIC.diff = start.soln$DIC - stop.soln$DIC
  return(DIC.diff)
}

# Minimum partial pressure of the lean gas
pCO2.lean = function(Na, A, beta1, beta2, k1, k2, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  
  # Calculate the DIC of the outlet after complete desorption: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Calculate the pCO2 when fully reduced, holding DIC constant. Due to the need for a previous case, run in ~5 steps
  out.soln = data.frame(DIC = stop.soln$DIC, xA = seq(from = min(xA.lim), to = max(xA.lim), length.out = 5))
  # Loop the pH and pCO2 simultaneously
  loop.pH = pH.it.guess.DIC.At.k.beta(pH.guess = stop.soln$pH[1], xA.next = out.soln$xA[1], DIC = out.soln$DIC[1],
                                      A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  loop.pCO2 = pCO2.xA.pH.A.k.beta.Na(xA = out.soln$xA[1], pH = loop.pH[1],
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, 
                                     pCO2.prev = stop.soln$p.CO2)
  for(i in 2:5){
    loop.pH[i] = pH.it.guess.DIC.At.k.beta(pH.guess = loop.pH[i-1], xA.next = out.soln$xA[i], DIC = out.soln$DIC[i],
                                        A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    loop.pCO2[i] = pCO2.xA.pH.A.k.beta.Na(xA = out.soln$xA[i], pH = loop.pH[i],
                                       At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, 
                                       pCO2.prev = loop.pCO2[i-1])
    
  }
  return(loop.pCO2[i])
}

```


### Penalty function
The penalty function is a logistic weighting function such that for values where <90% of the CO2 is captured, the weight is 1, but when less CO2 is captured, the weight increases based on the relative change on the minimum work of separation.
See the preparatory script (../Prep/) for details on its construction.
A penalty function is not necessary for the flux because insufficient capture would manifest as negative fluxes. It may still show up in the Pareto frontier, but it can easily be filtered out, unlike the cases with the energy demand.

```{r PCET Derived Functions: Energy Demand}
weight.fun = function(pCO2.lean){
  # Ideal
  n.inlet = c(0.15, 0.85)
  n.lean = c(0.85, 0.015*0.85/(1 - 0.015))
  n.enrich = c(0.001*(1 - 0.015*0.85/(1 - 0.015)), 1 - 0.015*0.85/(1 - 0.015))
  
  E.ideal = sum(n.enrich*log(n.enrich/sum(n.enrich)) + 
                  n.lean*log(n.lean/sum(n.lean)) - 
                  n.inlet*log(n.inlet/sum(n.inlet)))

  # Actual: separate into 3 cases:
  E.tru = rep(x = 0, times = length(pCO2.lean))
  # Third case: if the lean gas pressure is above 1, i.e. it pressurized
  pos3 = (pCO2.lean >= 0.99)
  # Set the lean gas pressure to 0.999, 
  # then multiply the weight by the actual pressure to correct the energy; 
  # since the weight is divided by this energy, this means dividing by the pressure
  set.lean = 0.999
  n.enrich.co2 = (set.lean - 0.15)/(1 - set.lean)
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos3] = (- sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))) / pCO2.lean[pos3]

  # Second case: if the lean gas pressure is between 0.15 and 1, 
  # i.e. the CO2 was moved from the pure gas to the lean gas
  pos2 = (pCO2.lean >= 0.15 & pCO2.lean < 0.99)
  # For the mass balance to work, gas must have moved from the enriched stream to the lean gas
  n.enrich.co2 = (pCO2.lean[pos2] - 0.15)/(1 - pCO2.lean[pos2])
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos2] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # First case: if the lean gas pressure is less than 0.15, i.e. some amount of capture happened
  pos1 = (pCO2.lean < 0.15)
  # Mathematically identical to the lean gas case, just adjusting the lean gas and enriched gas mass balance
  n.lean = pCO2.lean[pos1]*0.85/(1 - pCO2.lean[pos1])
  n.enrich.co2 = 0.15 - n.lean
  n.enrich.gas = 0.001*n.enrich.co2
  
  E.tru[pos1] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # Maximum weight
  weight.max = E.ideal/E.tru

  # Logistic function
  L = weight.max - 0.98
  k = 267; x0 = 0.071
  # data.frame(weight.max)
  return(25*L/(1 + exp(-k * (pCO2.lean - x0))) + 1)
}

# Total energy demand - 4-stage process for simplicity
Energy.tot = function(k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out){
  # Constants
  z = 2; R = 8.314; T = 298; F = 96485; resolution = 151;
  # pCO2.in = 0.1; pCO2.out = 1
  xA.lim = c(0.025, 0.975)
  
  # 1 -> 2: Electrochemical oxidation (xA decrease to endpoint), constant DIC
  # Starting solution for initial guess: low P, high xA
  start.soln = data.frame(p.CO2 = pCO2.in, xA = max(xA.lim))
  start.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = start.soln$xA, P = start.soln$p.CO2, 
                                       At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  start.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = start.soln$xA, pCO2 = start.soln$p.CO2, pH = start.soln$pH, 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  # Anode progress
  E.anode = data.frame(DIC = start.soln$DIC, xA = seq(from = start.soln$xA[1], to = min(xA.lim), length.out = resolution))
  # Loop to solve the ieration function
  loop = pH.it.guess.DIC.At.k.beta(pH.guess = start.soln$pH[1], xA.next = E.anode$xA[1], DIC = E.anode$DIC[1], 
                                       A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  for(i in 2:length(E.anode$DIC)){
    loop = c(loop, pH.it.guess.DIC.At.k.beta(pH.guess = loop[i-1], xA.next = E.anode$xA[i], DIC = E.anode$DIC[i], 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na))
  }
  # Some iterations don't converge completely, leading to single points that deviated from the rest of the curve. This is characterized by a single point that is a local maxima or minimum. Endpoints are asusmed to be good
  loop.check.left = loop[1:(resolution-2)] - loop[2:(resolution-1)]
  loop.check.right = loop[2:(resolution-1)] - loop[3:(resolution)]
  # If the signs are different, then it is a local shift
  loop.pos = c(TRUE, (sign(loop.check.left) == sign(loop.check.right)), TRUE)
  for(pos in which(loop.pos == FALSE)){ # Take the average
    loop[pos] = (loop[pos-1] + loop[pos+1])/2
  }
  E.anode$pH = loop; 
  # Loop pCO2 calculation as well, since the pCO2 function relies on the previous point
  loop = pCO2.xA.pH.A.k.beta.Na(xA = E.anode$xA[1], pH = E.anode$pH[1],
                                At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = start.soln$p.CO2)
  for(i in 2:length(E.anode$DIC)){
    loop[i] = pCO2.xA.pH.A.k.beta.Na(xA = E.anode$xA[i], pH = E.anode$pH[i],
                                  At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = loop[i-1])
  }
  E.anode$p.CO2 = loop
  E.anode$q = abs(E.anode$xA - E.anode$xA[1])*A.tot*z*F # Coulombs
  
  # 3 -> 4: Electrochemical reduction (xA increase to endpoint), constant DIC
  # Starting solution for initial guess: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                       At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  # Cathode progress
  E.cathode = data.frame(DIC = stop.soln$DIC[1], xA = seq(from = stop.soln$xA[1], to = max(xA.lim), length.out = resolution))
  # Loop to solve the ieration function
  loop = pH.it.guess.DIC.At.k.beta(pH.guess = stop.soln$pH[1], xA.next = E.cathode$xA[1], DIC = E.cathode$DIC[1], 
                                       A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  for(i in 2:length(E.cathode$DIC)){
    loop = c(loop, pH.it.guess.DIC.At.k.beta(pH.guess = loop[i-1], xA.next = E.cathode$xA[i], DIC = E.cathode$DIC[i], 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na))
  }
  # Some iterations don't converge completely, leading to single points that deviated from the rest of the curve. This is characterized by a single point that is a local maxima or minimum. Endpoints are asusmed to be good
  loop.check.left = loop[1:(resolution-2)] - loop[2:(resolution-1)]
  loop.check.right = loop[2:(resolution-1)] - loop[3:(resolution)]
  # If the signs are different, then it is a local shift
  loop.pos = c(TRUE, (sign(loop.check.left) == sign(loop.check.right)), TRUE)
  for(pos in which(loop.pos == FALSE)){ # Take the average
    loop[pos] = (loop[pos-1] + loop[pos+1])/2
  }
  E.cathode$pH = loop;
  # Loop pCO2 calculation as well, since the pCO2 function relies on the previous point
  loop = pCO2.xA.pH.A.k.beta.Na(xA = E.cathode$xA[1], pH = E.cathode$pH[1],
                                At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = stop.soln$p.CO2)
  for(i in 2:length(E.cathode$DIC)){
    loop[i] = pCO2.xA.pH.A.k.beta.Na(xA = E.cathode$xA[i], pH = E.cathode$pH[i],
                                  At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = loop[i-1])
  }
  E.cathode$p.CO2 = loop
  E.cathode$q = abs(E.cathode$xA - E.cathode$xA[1])*A.tot*z*F # Coulombs
  
  # Equilibrium potential: Deviation from standard reduction potential
  E.anode$H = 10^-E.anode$pH
  E.anode$E = R*T/(z*F) * log( (1 - E.anode$xA)/E.anode$xA * 
                                   ((1 + beta1*E.anode$p.CO2 + beta2*E.anode$p.CO2^2)*k1*k2 + k1*E.anode$H + E.anode$H^2)/(k1*k2))
  E.cathode$H = 10^-E.cathode$pH
  E.cathode$E = R*T/(z*F) * log( (1 - E.cathode$xA)/E.cathode$xA * 
                                   ((1 + beta1*E.cathode$p.CO2 + beta2*E.cathode$p.CO2^2)*k1*k2 + k1*E.cathode$H + E.cathode$H^2)/(k1*k2))
  
  # Total energy
  E.cell = data.frame(q = E.anode$q, V = E.anode$E - E.cathode$E)
  # Only the positive energy demand
  E.cell = filter(E.cell, V > 0)
  len = length(E.cell$q)
  if(len == 0){
    E.cell = data.frame(q = rep(x = 0, times = 10), V = rep(x = 0, times = 10))
    len = 10
  }
  # E.anode$typ = "anode"; E.cathode$typ = "cathode"
  Energy.tot.sep = sum(0.5*(E.cell$V[2:len] + E.cell$V[1:(len-1)])*(E.cell$q[2:len] - E.cell$q[1:(len-1)]))
  
  ## Adjust the total energy by multiplying by the penalty function
  # Calculate the lean gas pressure
  # k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out
  p.lean = pCO2.lean(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.out = pCO2.out)
  penalty = weight.fun(pCO2.lean = p.lean)
  
  # Normalize by the total amount of carbon moved, i.e. units of kJ/mol
  DIC.capture = DIC.diff(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.in = pCO2.in, pCO2.out = pCO2.out)
  return(Energy.tot.sep*penalty/DIC.capture*1e-3)
}

```


```{r PCET Derived Functions: CO2 Flux}
# These equations are based on the framework for determining the CO2 flux as presented in Wilcox 2012.
Enhance.factor = function(pH, pCO2.in, A, k1, k2, beta1, beta2, pCO2){
  # Constants: general
  kw = 1e-14 # M^2
  kH = 3.4e-2; # M/atm
  z = 1 # OH- + CO2 = HCO3-
  # Constants: from Wilcox 2012
  Dco2 = 0.5e-5 # cm2/s, assume slowest due to high ionic strength
  kL = 0.1 # Assume fast mass transfer of typical range
  # Constants: average of Pocker 1997, Zeman 2007, Stolaroff 2008, Wilcox 2012
  k.rate = (6.03e3 + 6.745e3 + 8.5e3 + 12.1e3)/4
  # Constants: Lvov2012
  Doh = 5.2e-5 # cm2/s

  # Base concentration = OH + HQ- + 2Q--
  H = 10^-pH
  OH = kw/H
  base = OH + A*(2*k1*k2 + H*k1) / (H^2 + H*k1 + k1*k2*(1 + beta1*pCO2 + beta2*pCO2^2))
  
  # Interface CO2 concentration
  CO2.int = pCO2.in*kH
  
  # Hatta number: reaction rate / mass transfer rate
  Ha = sqrt(Dco2*base*k.rate)/kL
  # Instantaneous enhancement factor
  Ei = 1 + Doh*base / (z*Dco2*CO2.int)
  
  # return(c(Ei, Ha / tanh(Ha), Ha))
  # Check the extreme cases for E to simplify the equations
  if(Ha > 10*Ei){ # Instantaneous
    E = Ei
  } else if(Ha < Ei/2){ # Pseudo-1st order
    E = Ha / tanh(Ha)
  } else if(Ha > 3){ # 1st order
    E = Ha
  } else{ # No simplification - Solve the root that is less than Ei, as Ei is the upper bound
    x.guess = c(0.9, 0.95)*Ei
    for(i in 1:5){ # Newton's method
      y.guess = (Ha*(Ei - x.guess) / (Ei - 1)) / tanh(Ha*(Ei - x.guess) / (Ei - 1)) - x.guess
      slp.fit = (y.guess[1] - y.guess[2]) / (x.guess[1] - x.guess[2])
      E.guess = -y.guess[1]/slp.fit + x.guess[1]
      x.guess = c(0.975, 1.025)*E.guess
    }
    E = E.guess
  }

  return(E)
}

# Calculate the average kinetic driving force over the course of absorption (stage 4 -> 1)
kinetic.force = function(k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  kH = 3.4e-2; # M/atm
  # Calculate the pCO2 of the fully reduced species prior to equilibration with the gas
  out.pCO2 = pCO2.lean(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.out = pCO2.out)
  # If the minimum outlet pCO2 is greater than the target capture:
  # if(out.pCO2 > 0.1*pCO2.in | is.na(out.pCO2)){
  if(is.na(out.pCO2)){
    return(0)
  } else{
    # Calculate the pH at the start of desorption
    soln41 = data.frame(xA = max(xA.lim), p.CO2 = out.pCO2)
    # Solve pH with multiple cores
    soln41$pH = pH.xA.pCO2.A.k.beta.Na(xA = soln41$xA, P = soln41$p.CO2, 
                         At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    soln41$E = Enhance.factor(pH = soln41$pH, pCO2.in = pCO2.in, A = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, pCO2 = soln41$p.CO2)

    # Calculate the concentration difference between the interface and the bulk.
    soln41$delC = (pCO2.in - out.pCO2)*kH
    # Flux: delC * kL * E, assume kL = 0.1 cm/s
    # Unit conversion: L to cm3, cm2 to m2
    flux = soln41$delC*soln41$E*0.1 * (1/1e3) * (100^2)
    # return(soln41)
    return(signif(flux, 5)) # Highest driving force at outlet
    # return(soln41)
  }
}

```


All of the above calculations are robust to both acid and base addition. The wrapper function used for GPareto is the primary point at which the sign of the concentration of "Na" is flipped to denote it is an acid.

```{r}
PCET.obj.flu = function(inputs){
  # Inputs is a matrix where each row is an instance and each column is a specific variable:
  # From left to right, the columns are:
  # pka1, err.pka2, log10(A.tot), Na/A.tot
  # For the functions, the variables should be:
  # k1, k2, A.tot, Na
  # The use of log units and ratios helps alleviate resolution issues associated with spanning multiple orders of magnitude
  # The use of the error of pKa2 removes the correlation between the two variables
  
  # Storing the proper information.
  if(is.matrix(inputs)){
    dat = data.frame(k1 = 10^-inputs[,1],
                     k2 = 10^-(inputs[,1] + inputs[,2]),
                     A.tot = 10^inputs[,3],
                     Na = -10^inputs[,4]*10^inputs[,3]) # Acidic conditions - Na is "negative"
  } else{
    # The optimization function sometimes stores as a vector instead of as a matrix if it is just a single point
    dat = data.frame(k1 = 10^-inputs[1],
                     k2 = 10^-(inputs[1] + inputs[2]),
                     A.tot = 10^inputs[3],
                     Na = -10^inputs[4]*10^inputs[3]) # Acidic conditions - Na is "negative"
  }
  # The following conditions are assumed for PCET from flue gas
  beta1 = 0; beta2 = 0;
  pCO2.in = 0.15; pCO2.out = 1;
  
  # The functions require substantial computation, so each row has to be processed independently
  energy = c(); flux = c()
  for(i in 1:nrow(dat)){
    energy[i] = Energy.tot(k1 = dat$k1[i], k2 = dat$k2[i], 
                           beta1 = 0, beta2 = 0, 
                           A.tot = dat$A.tot[i], Na = dat$Na[i], 
                           pCO2.in = 0.15, pCO2.out = 1)
    flux[i] = kinetic.force(k1 = dat$k1[i], k2 = dat$k2[i], 
                            beta1 = 0, beta2 = 0, 
                            A.tot = dat$A.tot[i], Na = dat$Na[i], 
                            pCO2.in = 0.15, pCO2.out = 1)
  }
  # Obtain the negative of the flux so it is a minimization function for the optimization search
  return(c(energy, -flux)) 
}

```

# Pareto frontier search

Using the GPareto package, using Gaussian Processes to find the Pareto frontier. The initial sampling design is based on the following constraints:

pKa1: 2 to 13.5
pKa2: linearly related to pKa1 with error term on (0, +5.5)
A.tot: 10 mM to 3.1 M (-2 to 0.5 in log10 units)
Na: 10^-7 to 10^0.7 times the concentration of A.tot

While pKa1 can feasibly extend down to -8.3, the pH of the system is not going to extend below approximately 3 as a lowest estimate due to the strength of carbonic acid. As a result, pKa values below 2 are effectively identical.
A.tot represents the total concentration of quinone. 
Na represents the concentration of NaOH that was added to the system; a negative value instead represents HCl.
Since the capture process requires basic conditions, negative Na conditions (HCl addition) should be less extreme if they are present at all.
It is assumed that the background electrolyte (NaCl) is at extreme excess and accounts for all transport across the membrane.

The initial design finds all 'corners' of the 4-dimensional hypercube, the center of each face (extreme of 1, midpoints of the rest), and 7*4=28 random points.

```{r Pareto Search Initial Design}
# Initial ranges
pka1.rng = c(2, 13.5)
pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5)
Na.A.rng = c(-7, 0.7)

# Corners = extreme points
corners = expand.grid(pka1.rng, pka2.rng, logA.rng, Na.A.rng)
corners = corners[,c(1:4)]
names(corners) = c('pka1', 'pka2', 'logA', 'Na.A')
# # Faces = extreme of 1 variable; midpoint of the rest
faces = data.frame(pka1 = c(pka1.rng, rep(mean(pka1.rng), 6)),
                   pka2 = c(rep(mean(pka2.rng), 2), pka2.rng, rep(mean(pka2.rng), 4)),
                   logA = c(rep(mean(logA.rng), 4), logA.rng, rep(mean(logA.rng), 2)),
                   Na.A = c(rep(mean(Na.A.rng), 6), Na.A.rng) )

# Random samples: number of variables times 7
nsamp = length(names(corners))*7
samples = data.frame(pka1 = runif(n = nsamp, min = min(pka1.rng), max = max(pka1.rng)),
                     pka2 = runif(n = nsamp, min = min(pka2.rng), max = max(pka2.rng)),
                     logA = runif(n = nsamp, min = min(logA.rng), max = max(logA.rng)),
                     Na.A = runif(n = nsamp, min = min(Na.A.rng), max = max(Na.A.rng)) )

design.start = rbind(corners, samples, faces)
# Output as a list
result.start = PCET.obj.flu(inputs = as.matrix(design.start))
result.start = matrix(result.start, ncol = 2)
Pareto.budget = 100
rm(corners, samples, faces)

# Store the initial design
design.start$Energy.kJ.mol = result.start[,1]
design.start$Flux.mol.m2s = result.start[,2]
write.csv(design.start, file = 'PCET_StartDesign.csv')

```

```{r Pareto Search: GPareto}
# Load design
design.start = read.csv(file = 'PCET_StartDesign.csv')
result.start = design.start[,c('Energy.kJ.mol', 'Flux.mol.m2s')]
design.start = design.start[,c('pka1', 'pka2', 'logA', 'Na.A')]
Pareto.budget = 100

res = easyGParetoptim(fn = PCET.obj.flu, budget = Pareto.budget, 
                      lower = c(2, 0, -2, -7), upper = c(13.5, 5.5, 0.5, 0.7), 
                      par = as.matrix(design.start), value = as.matrix(result.start), ncores = 2)
plotGPareto(res)

# Format into dataframe for easier plotting
GPar.front = data.frame(pka1 = res$par[,1], pka2 = res$par[,2], logA = res$par[,3], Na.A = res$par[,4],
                        Energy.kJ.mol = res$value[,1], Flux.mol.m2s = -res$value[,2])
GPar.all =  data.frame(pka1 = res$history$X[,1], pka2 = res$history$X[,2], logA = res$history$X[,3], 
                       Na.A = res$history$X[,4], 
                       Energy.kJ.mol = res$history$y[,1], Flux.mol.m2s = -res$history$y[,2])
GPar.all$order = c(rep(0, nrow(design.start)), seq(from = 1, to = Pareto.budget, by = 1))
# rm(res)
write.csv(GPar.all, file = 'GPar_all_data.csv')
write.csv(GPar.front, file = 'GPar_fnt_data.csv')
```

```{r Visualize the Pareto Front}
# rm(res)
GPar.all = read.csv(file = 'GPar_all_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
ggplot() +
  geom_point(filter(GPar.all, abs(Flux.mol.m2s) < 0.1, Energy.kJ.mol < 150), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s)) +
  geom_line(filter(GPar.front, abs(Flux.mol.m2s) < 1, Energy.kJ.mol < 150), 
            mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  geom_point(filter(GPar.front, abs(Flux.mol.m2s) < 1, Energy.kJ.mol < 150), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  labs(x = 'CO2 Flux (mol/m^2/s)', y = 'Energy Demand (kJ/mol)', subtitle = 'Pareto Frontier')

ggplot() +
  geom_point(filter(GPar.all, Flux.mol.m2s > 0), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s)) +
  geom_line(filter(GPar.front, Flux.mol.m2s > 0), 
            mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  geom_point(filter(GPar.front, Flux.mol.m2s > 0), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  labs(x = 'CO2 Flux (mol/m^2/s)', y = 'Energy Demand (kJ/mol)', subtitle = 'Pareto Frontier')

```

GPareto did not provide a very well defined Pareto frontier, although it did provide some useful data. The Pareto frontier does not appear to be substantially different than that of adding base.

# Requirements for CO2 capture

A substantial fraction of points were not able to capture any carbon, as noted by their negative CO2 flux. 
The first step is to see if the size of the search space can be reduced for future optimization to only those that capture CO2.
This will be done by making a GP model of the kinetic rate, adaptively sampling points near the flux of 0 (signifying any amount of capture), then marginalizating the probability that the result has positive flux.

The GP model sometimes does not converge, leading to a model that is unrepresentative. 
The following function checks that the resulting model has adequate variance variability and outputs only models that make sense for the training data.

```{r CO2 Capture Refinement Functions}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  
  # The flux has an issue where the negative fluxes have a much higher maximum
  # order of magnitude (-10 vs 1e-3), leading to skewed model results.
  # Adjusting the negative order of magnitude such that they have the same order of magnitude
  resp = GPar.data[, output.name]
  resp[resp < 0] = resp[resp < 0]*abs(max(resp)/min(resp))
  nug = 0.1*min(abs(resp))
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = resp,
                 # Result is more accurate with log units, but need to account for negative values
                 # covtyp = 'gauss', # Gaussian uncertainty
                 # optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50,
                                max.generations = 20), # Increase robustness
                 nugget = nug)
    
    # Randomly sample 1000 points from the search space.
    pt = 1000; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}

fill.sample.obj.nec = function(x, model.flux){
  # Evaluate the Kriging model function at x 
  res.flux = predict(object = model.flux, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')

  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.flux$mean, sd = res.flux$sd)
  
  # Variance based on propagation of errors, assuming independent measures
  sd = res.flux$sd

  # Objective result
  return(sd*(prob*(1-prob) + 0.25/9))
}

# Next point search function
fill.sample.nec = function(GPar.data){
  # Model
  mod.flux = fill.sample.mod(GPar.data = GPar.data, input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                             output.name = 'Flux.mol.m2s')
  
  # Next point by genetic algorithm
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj.nec(x, model = mod.flux)},
               lower = c(2, 0, -2, -7), upper = c(13.5, 5.5, 0.5, 0.7),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  point.next = GA.pred@solution[1,]
  GPar.new = data.frame(pka1 = point.next[1],
                        pka2 = point.next[2],
                        logA = point.next[3],
                        Na.A = point.next[4])

  # True result for both the energy and kinetics to add this to the dataset
  res = PCET.obj.flu(inputs = point.next)
  GPar.new$Energy.kJ.mol = res[1]
  GPar.new$Flux.mol.m2s  = -res[2] # Flip sign because optimization function minimizes
  GPar.new$order = max(GPar.data$order) + 1

  # Also add the fitness to the dataframe for iteration cutoffs
  GPar.new$fit = max(GA.pred@fitness)
  
  # Return the new point and the fitness
  return(GPar.new)
}

```

Applying the functions with an iterative adaptive sampling method

```{r CO2 Capture Refinement Iterations, warning=FALSE, message=FALSE}
# Load data
GPar.all = read.csv(file = 'GPar_all_data.csv')
# Remove indices
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]

# First iteration to set the baseline of how much improvement there is to find.
newpoint = fill.sample.nec(GPar.data = GPar.all)
start.fit = newpoint$fit; 
current.fit = newpoint$fit; 

# Repeat for a maximum of 200 iterations, or until the fitness drops below 1/1000 of the starting fitness, 
# indicating little further improvement
max.iter = max(GPar.all$order) + 100
while(max(GPar.all$order) < max.iter & current.fit*1e3 > start.fit){
  GPar.all = rbind(GPar.all, newpoint[,names(newpoint) %in% names(GPar.all)])
  newpoint = fill.sample.nec(GPar.data = GPar.all)
  current.fit = newpoint$fit
}

# Store the data
write.csv(GPar.all, file = 'GPar_90Cap_data.csv')


```


```{r CO2 Capture Refinement Iteration Visualization}
# Plot the results to show refinement
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
ggplot(filter(GPar.all, Energy.kJ.mol < 100)) +
  geom_point(mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = (order > 100))) +
  facet_wrap(~(Flux.mol.m2s > 0), scales = 'free_x') +
  labs(x = 'CO2 flux (mol/m^2/s)', y = 'Energy demand (kJ/mol C)') +
  scale_color_manual(labels = c('Initial', 'Refinement'), values = c('red', 'blue'), name = '')

# Show on a log scale (separating the positive and negative fluxes) to see how low the magnitude of the flux can be
ggplot(filter(GPar.all, Energy.kJ.mol < 100)) +
  geom_point(mapping = aes(x = abs(Flux.mol.m2s), y = Energy.kJ.mol, color = (order > 100))) +
  facet_wrap(~(Flux.mol.m2s > 0), scales = 'free_x') +
  scale_x_log10() +
  labs(x = 'CO2 flux (mol/m^2/s)', y = 'Energy demand (kJ/mol C)') +
  scale_color_manual(labels = c('Initial', 'Refinement'), values = c('red', 'blue'), name = '')

# Display the characteristics of points with very high flux
g1 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4), 
               mapping = aes(x = pka1, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = pka1, color = 'bad')) +
  labs(x = 'pKa1', y = 'Probability Density') + guides(color = FALSE) + 
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g2 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = pka1 + pka2, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = pka1 + pka2, color = 'bad')) +
  labs(x = 'pKa2', y = 'Probability Density') + guides(color = FALSE) + 
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g3 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = 10^logA, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = 10^logA, color = 'bad')) +
  labs(x = '[Quinone] (M)', y = 'Probability Density') + guides(color = FALSE) +
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g4 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = 10^logA*10^Na.A, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = 10^logA*10^Na.A, color = 'bad')) +
  labs(x = 'Additional Acid (M)', y = 'Probability Density') +
  scale_x_log10() +
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
(g1 + g3) / (g2 + g4)

rm(g1, g2, g3, g4)
```

In addition to refining knowledge of the boundary where the flux is zero, I show the distribution of "good" points (above 10^-4 mol/m2/s flux) and "bad" points (no flux).
The distinct differences between the two suggests that high and low flux conditions are very distinct in their pKa and concentration profiles, but the amount of acid or base is not a substantial factor.

After performing the marginalization to determine if the search space can be constrained, the Pareto front will be re-calculated with this new information.

Based on the distributions of the high flux conditions compared to the no capture conditions, it is likely that the result falls near an optimum rather than any of the extremes of the sample region.

For marginalization, assume a uniform distribution. 
While this is unrealistic, it is just to establish a constraint on the bounds rather than to identify real compounds.
Additionally, the interest is in removing the conditions that have a <5% likelihood of capturing >90% CO2, regardless of the other variables.

```{r CO2Capture Marginalization}
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
mod.flux = fill.sample.mod(GPar.data = GPar.all, input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                           output.name = 'Flux.mol.m2s')

# Set up the marginalization
resolution = 50; MCsamp = 2000
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-7, 0.7)
# Note: for the Na/A ratio, the interest is in the order of magnitude shifts, not the absolute shifts, going down to 10^-7.
# Since it is being tested for both positive and negative values, but they are of similar order of magnitude, half will be dedicated to each side

# Set the ranges
kin.constrain = data.frame(pka1 = seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                           pka2 = seq(from = pka2.rng[1], to = pka2.rng[2], length.out = resolution),
                           logA = seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                           Na.A = seq(from = Na.A.rng[1], to = Na.A.rng[2], length.out = resolution),
                           p.pka1 = NaN, p.pka2 = NaN, p.logA = NaN, p.Na.A = NaN, # Probability acceptance median
                           l.pka1 = NaN, l.pka2 = NaN, l.logA = NaN, l.Na.A = NaN, # lower bound
                           h.pka1 = NaN, h.pka2 = NaN, h.logA = NaN, h.Na.A = NaN) # upper bound
lower = 0.25; upper = 0.75

for(i in 1:resolution){
  # pka1
  fill.frame = data.frame(pka1 = kin.constrain$pka1[i],
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  kin.constrain$p.pka1[i] = mean(fill.frame$p.accept)
  kin.constrain$h.pka1[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.pka1[i] = quantile(fill.frame$p.accept, probs = lower)

  # pka2
  fill.frame = data.frame(pka2 = kin.constrain$pka2[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  kin.constrain$p.pka2[i] = mean(fill.frame$p.accept)
  kin.constrain$h.pka2[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.pka2[i] = quantile(fill.frame$p.accept, probs = lower)

  # log Quinone
  fill.frame = data.frame(logA = kin.constrain$logA[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  kin.constrain$p.logA[i] = mean(fill.frame$p.accept)
  kin.constrain$h.logA[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.logA[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # Na/A
  fill.frame = data.frame(Na.A = kin.constrain$Na.A[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame - 1e-5, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  kin.constrain$p.Na.A[i] = mean(fill.frame$p.accept)
  kin.constrain$h.Na.A[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.Na.A[i] = quantile(fill.frame$p.accept, probs = lower)
}

```

```{r}
kin.constrain
g1 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = pka1, y = p.pka1)) +
  geom_line(mapping = aes(x = pka1, y = h.pka1), linetype = 2) +
  geom_line(mapping = aes(x = pka1, y = l.pka1), linetype = 2) +
  labs(x = 'pKa1', y = 'Probability of CO2 Capture')
g2 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = pka2, y = p.pka2)) +
  geom_line(mapping = aes(x = pka2, y = h.pka2), linetype = 2) +
  geom_line(mapping = aes(x = pka2, y = l.pka2), linetype = 2) +
  labs(x = 'pKa2 - pKa1', y = 'Probability of CO2 Capture')
g3 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = 10^logA, y = p.logA)) +
  geom_line(mapping = aes(x = 10^logA, y = h.logA), linetype = 2) +
  geom_line(mapping = aes(x = 10^logA, y = l.logA), linetype = 2) +
  labs(x = 'Quinone Concentration', y = 'Probability of CO2 Capture') +
  scale_x_log10()
g4 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = 10^Na.A, y = p.Na.A)) +
  geom_line(mapping = aes(x = 10^Na.A, y = h.Na.A), linetype = 2) +
  geom_line(mapping = aes(x = 10^Na.A, y = l.Na.A), linetype = 2) +
  scale_x_log10() +
  labs(x = 'Na/Quinone Concentration Ratio', y = 'Probability of CO2 Capture')

(g1 + g3) / (g2 + g4)
rm(g1, g2, g3, g4)
```

Dotted lines represent the asymmetric 50% confidence interval, i.e. 50% of outcomes where the other variables are randomly selected will fall between those two lines.
None of the partial dependence probability curves indicate an unambiguous region of the search space that can be omitted, it does indicate some interesting features:

* Lower pKa1 values are less likely to capture sufficient carbon, consistent with the distributions.
* Higher concentrations of quinone have higher variance, and thus are more likely to be unambiguously viable or inviable.
* The difference between the two pKa's does not matter significantly, as the probability is mostly flat. However, the variance does decrease as the difference increases.

While these cannot be used to remove areas of the search space from sampling, it does give some indication of what to consider when comparing options.

One possible improvement to this presentation is to marginalize against 2 variables instead of just 1. 
It is likely that the combination of both pKas or of both concentrations is what matters, and this may provide more insight into restricting the space further since many inputs are co-correlated.

```{r CO2 Capture 2D Marginal}
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
mod.flux = fill.sample.mod(GPar.data = GPar.all, input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                           output.name = 'Flux.mol.m2s')

# Set up the marginalization
resolution = 35; MCsamp = 1500
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-7, 0.7)
lower = 0.25; upper = 0.75

# Set up the grid search
pkaX.grid = expand.grid(seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                        seq(from = pka2.rng[1], to = pka2.rng[2], length.out = resolution))
pkaX.grid = pkaX.grid[,c(1:2)]
names(pkaX.grid) = c('pka1', 'pka2')
pkaX.grid$p = NaN; pkaX.grid$l = NaN; pkaX.grid$s = NaN # Probability and the high and low

# Note: for the Na/A ratio, the interest is in the order of magnitude shifts, not the absolute shifts, going down to 10^-7.
# Since it is being tested for both positive and negative values, but they are of similar order of magnitude, half will be dedicated to each side
conc.grid = expand.grid(seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                        seq(from = Na.A.rng[1], to = Na.A.rng[2], length.out = resolution))
conc.grid = conc.grid[,c(1:2)]
names(conc.grid) = c('logA', 'Na.A')
conc.grid$p = NaN; conc.grid$l = NaN; conc.grid$s = NaN # Probability and the high and low

# Loop
for(i in 1:nrow(pkaX.grid)){
  # pKa grid
  fill.frame = data.frame(pka1 = pkaX.grid$pka1[i],
                          pka2 = pkaX.grid$pka2[i],
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  pkaX.grid$p[i] = mean(fill.frame$p.accept)
  pkaX.grid$h[i] = quantile(fill.frame$p.accept, probs = upper)
  pkaX.grid$l[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # Concentration grid
  fill.frame = data.frame(pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = conc.grid$logA[i],
                          Na.A = conc.grid$Na.A[i])
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = 1-pnorm(q = 0, mean = res$mean, sd = res$sd)
  conc.grid$p[i] = mean(fill.frame$p.accept)
  conc.grid$h[i] = quantile(fill.frame$p.accept, probs = upper)
  conc.grid$l[i] = quantile(fill.frame$p.accept, probs = lower)
}

```

```{r}
# Input variables: pKa
g1 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = p)) +
  labs(x = 'pka1', y = '', fill = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(fill = FALSE)
g2 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = h)) +
  labs(x = '', y = '', fill = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = l)) +
  labs(x = '', y = 'pka2 - pka1', fill = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(fill = FALSE)
g3 + g1 + g2

# Input variables: Concentrations
g1 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = logA, y = Na.A, color = p), size = 2) +
  labs(x = 'log10 [Quinone]', y = '', color = 'P[Flux > 0]', subtitle = 'Mean') +
  guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = logA, y = Na.A, color = l), size = 2) +
  labs(x = '', y = 'log [pH Corrector]/[Quinone]', color = 'P[Flux > 0]', subtitle = '25th Percentile') +
  guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = logA, y = Na.A, color = h), size = 2) +
  labs(x = '', y = '', color = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 + g1 + g3

# Natural variables: pka
g1 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = p)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  geom_vline(xintercept = c(10.3, 6.3)) +
  labs(x = 'pka1', y = '', color = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(color = FALSE)
g2 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = h)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  geom_vline(xintercept = c(10.3, 6.3)) +
  labs(x = '', y = '', color = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = l)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  geom_vline(xintercept = c(10.3, 6.3)) +
  labs(x = '', y = 'pka2', color = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(color = FALSE)
g3 + g1 + g2

# Input variables: Concentrations
g1 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*10^Na.A, color = p)) +
  labs(x = '[Quinone]', y = '', color = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_y_log10() + scale_x_log10() + guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*10^Na.A, color = l)) +
  labs(x = '', y = 'pH Corrector', color = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_y_log10() + scale_x_log10() + guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*10^Na.A, color = h)) +
  labs(x = '', y = '', color = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_y_log10() + scale_x_log10() +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 + g1 + g3

rm(g1, g2, g3)
```

Insights from these 2D plots:

* Lower pKa1 values have the potential to lead to better outcomes, but also worse outcomes. High pKa1 values are more consistently able to capture enough CO2.
* There is a definitive band of pKa combinations that do not yield good results, mostly when the lower pKa2 value is at or below the pKa of carbonic acid.
* Generally speaking, the concentration of additional acid or base has next to no impact on the flux. There is a slight trend towards lower concentrations, but it is difficult to tell if that is caused by the edge effects (less sampling).
* There is a band of quinone concentrations of the order 0.1 to 1 M that more often than not leads to good outcomes. Below that region, the probability is typically low, while above that region, the variance is high.

# Refinement to optimal conditions

In addition to refining the model close to 0 to separate the conditions that are viable and inviable, the above process may have found some points that were previously unexplored which outperformed the Pareto front.
Therefore, the Pareto front needs to be updated.

```{r Updated Pareto Front}
# Load data
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
# Compare the matrices of just the outputs - need as a matrix
test = as.matrix(GPar.all[GPar.all$Flux.mol.m2s > 0,names(GPar.all) %in% c('Energy.kJ.mol', 'Flux.mol.m2s')])
# For comparison, need both to minimize
test[,2] = -test[,2]

par.front = t(nondominated_points(points = t(test)))
par.front[,2] = -par.front[,2]
ggplot() +
  geom_vline(xintercept = 2.2e-3, linetype = 2) +
  geom_point(filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100),
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  # New front
  geom_line(data.frame(par.front), mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'new')) +
  geom_point(data.frame(par.front), mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'new')) +
  # Old front
  geom_line(GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'old')) +
  geom_point(GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'old')) +
  labs(x = 'CO2 flux (mol/m2/s)', y = 'Energy Demand (kJ/mol C)') +
  scale_color_manual(values = c('new' = 'red', 'old' = 'cyan'), 
                     labels = c('new' = 'New Front', 'old' = 'Old Front'),
                     name = '')
  

# Identify the conditions leading to the Pareto front
GPar.front = filter(GPar.all, Energy.kJ.mol %in% par.front[,1], Flux.mol.m2s %in% par.front[,2])
GPar.front = GPar.front[,!names(GPar.front) %in% 'X']
write.csv(GPar.front, file = 'GPar_fnt_data_90Cap.csv')

```

The energy target for CO2 capture from flue gas is approximately 20-30 kJe/mol C to achieve a < 35% increase in cost relative to a power plant not capturing carbon.
The majority of the Pareto front satisfies this criteria, and the points that do not are still relatively low (up to around 40 kJe/mol C) and competitive with temperature-swing absorption.
The entire Pareto front also falls entirely above 10% of the maximum flux (vertical dotted line), suggesting that the flux does not have substantial variability in this energy regime.
The selection criteria is therefore going to be all points that meet both an energy and flux cutoff threshold set by the 1-variable optima.
To give some amount of tolerance, the flux cutoff is 10% of the maximum flux of capture with MEA, and the energy cutoff is 40 kJ/mol C.

Based on prior tests with mathematical test functions, this type of criteria is both robust to modeling and converges relatively quickly.
An initial pass on the inputs conditions that meet this criteria:


```{r Optimal Capture Marginals Before Refinement: Functions}
# The energy GP model has difficulty converging due to nonzero eigenvalues
# This is corrected with a higher nugget (enforced single variable variance) term
# Since energy and flux have different orders of magnitude, this will only be enforced for the energy GP model;
# the flux values are too small compared to the necessary nugget

fill.sample.ener = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = log10(GPar.data[, output.name]), 
                 # covtyp = 'gauss', # Gaussian uncertainty
                 # optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE), # Turn off tracking to simplify output
                                # pop.size = 50), # Increase robustness
                 nugget = 5e-2, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 1000 points from the search space.
    pt = 1000; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}


```


```{r Optimal Capture Marginals: Refinement Functions}
fill.sample.obj.opt = function(x, model.flux, model.ener){
  # Evaluate the Kriging model function at x 
  res.flux = predict(object = model.flux, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')
  res.ener = predict(object = model.ener, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')

  # Probability distribution fits a Gaussian distribution.
  # Want the probability that the energy is below 40 kJ/mol C, 
  # flux is faster than 10% of the maximum (which still meets the energy criteria)
  E.cutof = log10(33); #kJ/mol C, log scale to account for multiple order of magnitude span
  # F.cutof = 0.1*max(filter(GPar.all, Energy.kJ.mol < 40)$Flux.mol.m2s)
  F.cutof = 22e-3

  prob = c(1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd) *
             pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
           )
  
  # Variance based on propagation of errors, assuming independent measures.
  # Since the optimization looks for relative differences, the square root is not necessary
  sd = (res.flux$sd^2 * res.ener$mean^2 + res.ener$sd^2 * res.flux$mean^2)

  # Acquisition function result. Weight the probability slightly such that the maximum is 
  # 2 orders of magnitude higher than the minimum (and the minimum is not zero)
  # Given the number of points already collected, favor exploitation over exploration.
  return(sd*(prob*(1-prob) + 0.25/99))
}

# Next point search function
fill.sample.opt = function(GPar.data){
  # Models
  mod.flux = fill.sample.mod(GPar.data = GPar.data, input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                             output.name = 'Flux.mol.m2s')
  # Energy model has substantial outliers above 1e3 kJ/mol that skew the model; 
  # a more accurate model is found when restricted
  mod.ener = fill.sample.ener(GPar.data = GPar.data, 
                              input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                              output.name = 'Energy.kJ.mol')

  # Next point by genetic algorithm
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj.opt(x, model.flux = mod.flux, model.ener = mod.ener)},
               lower = c(2, 0, -2, -7), upper = c(13.5, 5.5, 0.5, 0.7),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  point.next = GA.pred@solution[1,]
  GPar.new = data.frame(pka1 = point.next[1],
                        pka2 = point.next[2],
                        logA = point.next[3],
                        Na.A = point.next[4])
  
  # True result for both the energy and kinetics to add this to the dataset
  res = PCET.obj.flu(inputs = point.next)
  GPar.new$Energy.kJ.mol = res[1]
  GPar.new$Flux.mol.m2s  = -res[2] # Flip sign because optimization function minimizes
  GPar.new$order = max(GPar.data$order) + 1

  # Also add the fitness to the dataframe for iteration cutoffs
  GPar.new$fit = max(GA.pred@fitness)
  
  # Return the new point and the fitness
  return(GPar.new)
}

```

```{r Optimal Capture Marginals: Refinement Iterations, warning=FALSE, message=FALSE}
# Load data
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
# Remove indices
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]

# First iteration to set the baseline of how much improvement there is to find.
newpoint = fill.sample.opt(GPar.data = GPar.all)
start.fit = newpoint$fit; 
current.fit = newpoint$fit; 

# Repeat for a maximum of 200 iterations, or until the fitness drops below 1/1000 of the starting fitness, 
# indicating little further improvement
max.iter = max(GPar.all$order) + 100
while(max(GPar.all$order) < max.iter & current.fit*1e3 > start.fit){
  GPar.all = rbind(GPar.all, newpoint[,names(newpoint) %in% names(GPar.all)])
  newpoint = fill.sample.opt(GPar.data = GPar.all)
  current.fit = newpoint$fit
}

# Store the data
write.csv(GPar.all, file = 'GPar_90CapOpt_data.csv')
# Updated Pareto front
test = as.matrix(GPar.all[GPar.all$Flux.mol.m2s > 0,names(GPar.all) %in% c('Energy.kJ.mol', 'Flux.mol.m2s')])
# For the Pareto front determination, need both to minimize
test[,2] = -test[,2]
par.front = t(nondominated_points(points = t(test)))
par.front[,2] = -par.front[,2]
# Identify the conditions leading to the Pareto front
GPar.front = filter(GPar.all, Energy.kJ.mol %in% par.front[,1], Flux.mol.m2s %in% par.front[,2])
GPar.front = GPar.front[,!names(GPar.front) %in% 'X']
write.csv(GPar.front, file = 'GPar_fnt_data_90CapOpt.csv')

```

It appears that the GP model for the flux predictions often reaches the upper limit of iterations for convergence, but a 50% training/test split shows that the error rarely exceeds 10%.
That error is good enough for such small values (10^-5 at the highest) which span multiple orders of magnitude and both positive and negative signs (preventing the use of log-scale values).

```{r Optimal Capture Marginals: Refinement Plot}
GPar.all = read.csv(file = 'GPar_90CapOpt_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data_90CapOpt.csv')

ggplot() +
  # Bounds
  geom_hline(yintercept =  33, linetype = 2) +
  geom_vline(xintercept = 2.2e-3, linetype = 2) +
  # Data
  geom_point(data = filter(GPar.all, Flux.mol.m2s > -0.1, Energy.kJ.mol < 100, order <= 200), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  geom_point(data = filter(GPar.all, Flux.mol.m2s > -0.1, Energy.kJ.mol < 100, order > 200), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'cyan') +
  facet_grid(.~Flux.mol.m2s > 0, scales = 'free_x') +
  geom_point(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'red') +
  geom_line(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'red') +
  labs(x = 'CO2 Flux (mol/m2/s)', y = 'Energy (kJ/mol C)')

ggplot() +
  # Bounds
  geom_hline(yintercept =  33, linetype = 2) +
  geom_vline(xintercept = 2.2e-3, linetype = 2) +
  # Data
  geom_point(data = filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100, order <= 200), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  geom_point(data = filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100, order > 200), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'cyan') +
  geom_point(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'red') +
  geom_line(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol), color = 'red') +
  labs(x = 'CO2 Flux (mol/m2/s)', y = 'Energy (kJ/mol C)')

```

The Pareto front is very similar to that of base addition, suggesting that the distinction between adding an acid or a base is inconsequential to peak performance.
As a result, it can be said that the addition of base or acid is irrelevant.

# Post-refinement marginals
Marginalize with 2 2D grids (pKa and concentration), as the 1D metrics did not provide useful insights.

The GP models are able to handle correlated inputs; the primary reason for de-correlating inputs is to re-map the search space into a rectangular form for easier sampling.
This is not necessary when doing feature importance or assessing the marginals.
Since the natural forms of the variables are easier to interpret, the feature importance and marginals will use the natural variables, i.e. not the difference between the pKas and the ratio of NaOH to quinone.

```{r Optimal Capture Marginals Natural GP Model: 2D}
E.cutof = log10(33); #kJ/mol C, log units
F.cutof = 2.2e-3

# Natural variables conversion
GPar.nat = GPar.all
GPar.nat$pka2 = GPar.nat$pka1 + GPar.nat$pka2
GPar.nat$Na.A = GPar.nat$logA + GPar.nat$Na.A

# Models
mod.flux = fill.sample.mod(GPar.data = GPar.nat, input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                           output.name = 'Flux.mol.m2s')
mod.ener = fill.sample.ener(GPar.data = GPar.nat, 
                            input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                            output.name = 'Energy.kJ.mol')

# Set up the marginalization. Same grid because the same limits on physical systems
resolution = 35; MCsamp = 2000
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-7, 0.7)
lower = 0.25; upper = 0.75

# Set up the grid search
pkaX.grid = expand.grid(seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                        seq(from = pka2.rng[1], to = pka2.rng[2], length.out = resolution))
pkaX.grid = pkaX.grid[,c(1:2)]
names(pkaX.grid) = c('pka1', 'pka2')
pkaX.grid$p = NaN; pkaX.grid$l = NaN; pkaX.grid$s = NaN # Probability and the high and low

# Note: for the Na/A ratio, the interest is in the order of magnitude shifts, not the absolute shifts, going down to 10^-7.
# Since it is being tested for both positive and negative values, but they are of similar order of magnitude, half will be dedicated to each side
conc.grid = expand.grid(seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                        seq(from = Na.A.rng[1], to = Na.A.rng[2], length.out = resolution))
conc.grid = conc.grid[,c(1:2)]
names(conc.grid) = c('logA', 'Na.A')
conc.grid$p = NaN; conc.grid$l = NaN; conc.grid$s = NaN # Probability and the high and lowlower = 0.25; upper = 0.75

for(i in 1:nrow(pkaX.grid)){
  # pKas
  fill.frame = data.frame(pka1 = pkaX.grid$pka1[i],
                          pka2 = pkaX.grid$pka1[i] + pkaX.grid$pka2[i],
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  fill.frame$Na.A = fill.frame$Na.A + fill.frame$logA
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  pkaX.grid$p[i] = mean(fill.frame$p.accept)
  pkaX.grid$h[i] = quantile(filter(fill.frame, !is.nan(p.accept))$p.accept, probs = upper)
  pkaX.grid$l[i] = quantile(filter(fill.frame, !is.nan(p.accept))$p.accept, probs = lower)

  # Concentrations
  fill.frame = data.frame(pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          logA = conc.grid$logA[i],
                          Na.A = conc.grid$logA[i] + conc.grid$Na.A[i])
  fill.frame$pka2 = fill.frame$pka1 + fill.frame$pka2
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  conc.grid$p[i] = mean(fill.frame$p.accept)
  conc.grid$h[i] = quantile(filter(fill.frame, !is.nan(p.accept))$p.accept, probs = upper)
  conc.grid$l[i] = quantile(filter(fill.frame, !is.nan(p.accept))$p.accept, probs = lower)
}


```


```{r Optimal Capture Marginals Natural GP Model: 2D Plots}
lim = c(floor(min(pkaX.grid$l)*10)/10, ceiling(max(pkaX.grid$h)*10)/10); mid = mean(lim)
g1 = ggplot(filter(pkaX.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = l)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'blue', size = 0.1) +
  labs(x = 'pka1', y = 'pka2', color = 'P[Optimal]', subtitle = '25th Percentile') +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +
  theme(panel.background = element_rect(fill = 'grey')) +
  guides(color = FALSE)
g2 = ggplot(filter(pkaX.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = p)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'blue', size = 0.1) +
  labs(x = 'pka1', y = 'pka2', color = 'P[Optimal]', subtitle = 'Mean') +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +
  theme(panel.background = element_rect(fill = 'grey')) +
  guides(color = FALSE)
g3 = ggplot(filter(pkaX.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = h)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = pka1, y = pka1 + pka2), color = 'blue', size = 0.1) +
  labs(x = 'pka1', y = 'pka2', color = 'P[Optimal]', subtitle = '75th Percentile') +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +
  theme(panel.background = element_rect(fill = 'grey'))
g1 + g2 + g3

lim = c(floor(min(conc.grid$l)*10)/10, ceiling(max(conc.grid$h)*10)/10); mid = mean(lim)
g1 = ggplot(filter(conc.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = 10^logA, y = 10^Na.A*10^logA, color = l)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'blue', size = 0.1) +
  labs(x = '[Quinone]', y = '[pH Correctant]', color = 'P[Optimal]', subtitle = '25th Percentile') +
  scale_x_log10() + scale_y_log10() +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +
  theme(panel.background = element_rect(fill = 'grey')) +
  guides(color = FALSE)
g2 = ggplot(filter(conc.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = 10^logA, y = 10^Na.A*10^logA, color = p)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'blue', size = 0.1) +
  labs(x = '[Quinone]', y = '[pH Correctant]', color = 'P[Optimal]', subtitle = 'Mean') +
  scale_x_log10() + scale_y_log10() +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +  
  theme(panel.background = element_rect(fill = 'grey')) +
  guides(color = FALSE)
g3 = ggplot(filter(conc.grid, !is.nan(p))) +
  geom_point(mapping = aes(x = 10^logA, y = 10^Na.A*10^logA, color = h)) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol > 40 | Flux.mol.m2s < 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'red', size = 0.1) +
  geom_point(data = filter(GPar.all, Energy.kJ.mol < 40, Flux.mol.m2s > 0.1*max(Flux.mol.m2s)), 
             mapping = aes(x = 10^logA, y = 10^Na.A*10^logA), color = 'blue', size = 0.1) +
  labs(x = '[Quinone]', y = '[pH Correctant]', color = 'P[Optimal]', subtitle = '75th Percentile') +
  scale_x_log10() + scale_y_log10() +
  scale_color_gradient2(low = 'purple', high = 'orange', mid = 'white', midpoint = mid, limits = lim) +
  theme(panel.background = element_rect(fill = 'grey'))
g1 + g2 + g3

```

The 2D marginal highlights two key features:
* There is a minimum pKa2 for capture at around 9.5. Below this point, capture is very unlikely.
* There is a minimum concentration of quinone at around 30 mM.
* Excess acid addition leads to poor performance. The maximum amount of acid is approximately 1-tenth of the amount of quinone.

```{r Optimal Capture Marginals Natural variables: 1D}
# Define the cutoff values
E.cutof = log10(33); #kJ/mol C, log units
# F.cutof = 0.1*max(GPar.all$Flux.mol.m2s)
F.cutof = 2.2e-3

# Set up the marginalization
resolution = 50; MCsamp = 2000
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-7, 0.7)

# Set the ranges
post.optim = data.frame(pka1 = seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                        pka2 = seq(from = pka2.rng[1] + pka1.rng[1], to = pka2.rng[2] + pka1.rng[2], 
                                   length.out = resolution),
                        logA = seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                        Na.A = seq(from = Na.A.rng[1] + logA.rng[1], to = Na.A.rng[2] + logA.rng[2], 
                                   length.out = resolution),
                        p.pka1 = NaN, p.pka2 = NaN, p.logA = NaN, p.Na.A = NaN, # Probability acceptance median
                        s.pka1 = NaN, s.pka2 = NaN, s.logA = NaN, s.Na.A = NaN, # variance for importance ranking
                        l.pka1 = NaN, l.pka2 = NaN, l.logA = NaN, l.Na.A = NaN, # lower bound
                        h.pka1 = NaN, h.pka2 = NaN, h.logA = NaN, h.Na.A = NaN) # upper bound
lower = 0.25; upper = 0.75

for(i in 1:resolution){
  # pka1
  fill.frame = data.frame(pka1 = post.optim$pka1[i],
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]) + post.optim$pka1[i],
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  fill.frame$Na.A = fill.frame$logA + fill.frame$Na.A
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  post.optim$p.pka1[i] = mean(fill.frame$p.accept)
  post.optim$s.pka1[i] = sd(fill.frame$p.accept)
  post.optim$h.pka1[i] = quantile(fill.frame$p.accept, probs = upper)
  post.optim$l.pka1[i] = quantile(fill.frame$p.accept, probs = lower)

  # pka2
  # Define the min/max
  pka1.testrng = c(max(pka1.rng[1], post.optim$pka2[i] - pka2.rng[2]),
                   min(pka1.rng[2], post.optim$pka2[i] - pka2.rng[1]))
  
  fill.frame = data.frame(pka2 = post.optim$pka2[i],
                          pka1 = runif(n = MCsamp, min = pka1.testrng[1], max = pka1.testrng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  fill.frame$Na.A = fill.frame$logA + fill.frame$Na.A
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  post.optim$p.pka2[i] = mean(fill.frame$p.accept)
  post.optim$s.pka2[i] = sd(fill.frame$p.accept)
  post.optim$h.pka2[i] = quantile(fill.frame$p.accept, probs = upper)
  post.optim$l.pka2[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # log Quinone
  fill.frame = data.frame(logA = post.optim$logA[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  fill.frame$pka2 = fill.frame$pka1 + fill.frame$pka2
  fill.frame$Na.A = fill.frame$logA + fill.frame$Na.A
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  post.optim$p.logA[i] = mean(fill.frame$p.accept)
  post.optim$s.logA[i] = sd(fill.frame$p.accept)
  post.optim$h.logA[i] = quantile(fill.frame$p.accept, probs = upper)
  post.optim$l.logA[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # Na/A
  logA.testrng = c(max(logA.rng[1], post.optim$Na.A[i] - Na.A.rng[2]),
                   min(logA.rng[2], post.optim$Na.A[i] - Na.A.rng[1]))

  fill.frame = data.frame(Na.A = post.optim$Na.A[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = runif(n = MCsamp, min = logA.testrng[1], max = logA.testrng[2]))
  fill.frame$pka2 = fill.frame$pka1 + fill.frame$pka2
  res.flux = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  res.ener = predict(object = mod.ener, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = (1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd)) *
    pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
  post.optim$p.Na.A[i] = mean(fill.frame$p.accept)
  post.optim$s.Na.A[i] = sd(fill.frame$p.accept)
  post.optim$h.Na.A[i] = quantile(fill.frame$p.accept, probs = upper)
  post.optim$l.Na.A[i] = quantile(fill.frame$p.accept, probs = lower)
}


```


```{r Optimal Capture Marginals Natural variables: 1D Plots}
# Function variables
g1 = ggplot(post.optim) +
  geom_line(mapping = aes(x = pka1, y = p.pka1)) +
  geom_line(mapping = aes(x = pka1, y = h.pka1), linetype = 2) +
  geom_line(mapping = aes(x = pka1, y = l.pka1), linetype = 2) +
  labs(x = 'pka1', y = 'P[Optimal]') +
  scale_y_continuous(limits = c(0, ceiling(max(post.optim$h.pka1)*10)/10) ) +
  scale_x_continuous(limits = c(min(post.optim$pka1), max(post.optim$pka2)))
g2 = ggplot(post.optim) +
  geom_line(mapping = aes(x = pka2, y = p.pka2)) +
  geom_line(mapping = aes(x = pka2, y = h.pka2), linetype = 2) +
  geom_line(mapping = aes(x = pka2, y = l.pka2), linetype = 2) +
  labs(x = 'pka2', y = 'P[Optimal]') +
  scale_y_continuous(limits = c(0, ceiling(max(post.optim$h.pka2)*10)/10) ) +
  scale_x_continuous(limits = c(min(post.optim$pka1), max(post.optim$pka2)))
(g1 / g2)

g3 = ggplot(post.optim) +
  geom_line(mapping = aes(x = 10^logA, y = p.logA)) +
  geom_line(mapping = aes(x = 10^logA, y = h.logA), linetype = 2) +
  geom_line(mapping = aes(x = 10^logA, y = l.logA), linetype = 2) +
  labs(x = '[Quinone]', y = 'P[Optimal]') +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, ceiling(max(post.optim$h.logA)*10)/10) )
g4 = ggplot(post.optim) +
  geom_line(mapping = aes(x = 10^Na.A, y = p.Na.A)) +
  geom_line(mapping = aes(x = 10^Na.A, y = h.Na.A), linetype = 2) +
  geom_line(mapping = aes(x = 10^Na.A, y = l.Na.A), linetype = 2) +
  labs(x = '[Additional Acid]', y = 'P[Optimal]') +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, ceiling(max(post.optim$h.Na.A)*10)/10) )
(g3 / g4)

```

The 1D and 2D marginals show slight differences compared to the marginals when adding a base (eg. NaOH) instead of an acid (eg. HCl).
The primary difference is that pKa2 appears more important when considering acid addition, while pKa1 is more important when adding a base;
pKa values that are too low cannot meet the desired criteria regardless of the concentrations of quinone or acid, particularly for the higher pKa.
The local peak in the quinone concentration still exists at around 30 mM, but the higher concentrations are just as viable under acid addition.
This may suggest that the more basic quinones are extending the operating pHs too high, and acid addition can compensate.

```{r Natural Variables: Importance Ranking}
import = data.frame(rank = c(diff(range(post.optim$p.pka1))/sum(post.optim$s.pka1),
  diff(range(post.optim$p.pka2))/sum(post.optim$s.pka2),
  diff(range(post.optim$p.logA))/sum(post.optim$s.logA),
  diff(range(post.optim$p.Na.A))/sum(post.optim$s.Na.A)),
  var = c('pka1', 'pka2', 'logA', 'Na.A'))

ggplot(import[order(import$rank, decreasing = TRUE),]) +
  geom_col(mapping = aes(x = 1:4, y = rank/max(rank), fill = var)) +
  labs(x = '', y = '', color = '', subtitle = 'Feature Importance') +
  scale_fill_discrete(labels = c('pka1' = expression('p'*italic(K)['a,1']),
                                  'pka2' = expression('p'*italic(K)['a,2']), 
                                  'logA' = '{Quinone}',
                                  'Na.A' = '{NaOH}'),
                      breaks = import$var[order(import$rank, decreasing = TRUE)],
                      name = '') +
  scale_x_discrete(labels = c()) +
  scale_y_continuous(breaks = c(0, 1),
                     expand = expansion(mult = c(0, .1)),
                     labels = c('Least', 'Most'), name = 'Importance') +
  theme_classic() +
  theme(legend.position = c(0.9, 0.8))

```

# Sub-regions of interest

Applying the refinement process to further refine information about high fluxes and low energy regions independently.

The high flux region is fluxes above 22 mmol/m^2*s, while the low energy region is below 20 kJ/mol:

```{r Refinement Subregions}
GPar.all = read.csv(file = 'GPar_90CapOpt_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data_90CapOpt.csv')

region.flux = data.frame(flux.max = max(GPar.front$Flux.mol.m2s), flux.min = 22e-3,
                     ener.max = 45, ener.min = min(GPar.front$Energy.kJ.mol)
                     )

region.ener = data.frame(flux.max = max(GPar.front$Flux.mol.m2s), flux.min = 0.1*max(GPar.front$Flux.mol.m2s),
                     ener.max = 20, ener.min = min(GPar.front$Energy.kJ.mol)
                     )

ggplot() +
  # Bounds
  geom_rect(region.flux, mapping = aes(xmax = flux.max, xmin = flux.min, ymax = ener.max, ymin = ener.min,
                                       color = 'flux'),
            fill = 'green', linetype = 1, alpha = 0.5) +
  geom_rect(region.ener, mapping = aes(xmax = flux.max, xmin = flux.min, ymax = ener.max, ymin = ener.min,
                                       color = 'ener'),
            fill = 'blue', linetype = 1, alpha = 0.5) +
  # Data
  geom_point(data = filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  geom_point(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'pareto')) +
  geom_line(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'pareto')) +
  scale_color_manual(values = c('pareto' = 'red', 'flux' = 'green', 'ener' = 'blue'),
                     labels = c('pareto' = 'Pareto Front', 'flux' = 'High Flux', 'ener' = 'Low Energy'),
                     name = '') +
  guides(colour = guide_legend(override.aes = list(shape = 15, size = 2, alpha = c(0.5, 0.5, 1),
                                                   fill = NA, linetype = c(0, 0, 0)))) +
  labs(x = 'CO2 Flux (mol/m2/s)', y = 'Energy (kJ/mol C)', subtitle = 'Subregions for refinement')


```

These regions are relatively small and poorly populated, so further refinement will be useful.
Since the interest is in lower energy demands and higher fluxes, only the minimum flux and maximum energy boundaries will be used in the refinement process.
The two refinement processes will be conducted "simultaneously" in that they will be performed on the same initial dataset to fine one point each, then the next points for each process added together in the next iteration.

For the iteration fitness stop criteria, use the maximum fitness among the two new points.

```{r Sub-region Capture Refinement: Functions}
# High flux condition
fill.sample.sub.flux = function(x, model.flux, model.ener){
  # Evaluate the Kriging model function at x 
  res.flux = predict(object = model.flux, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')
  res.ener = predict(object = model.ener, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')

  # Probability distribution fits a Gaussian distribution.
  # Want the probability that the energy is below 40 kJ/mol C, 
  # flux is faster than 10% of the maximum (which still meets the energy criteria)
  E.cutof = log10(45); #kJ/mol C, log scale to account for multiple order of magnitude span
  F.cutof = 22e-3 # mol/m2s, flux at which there is a concavity in the Pareto front

  prob = c(1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd) *
             pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
           )
  
  # Variance based on propagation of errors, assuming independent measures.
  # Since the optimization looks for relative differences, the square root is not necessary
  sd = (res.flux$sd^2 * res.ener$mean^2 + res.ener$sd^2 * res.flux$mean^2)

  # Acquisition function result. Weight the probability slightly such that the maximum is 
  # 2 orders of magnitude higher than the minimum (and the minimum is not zero)
  # Given the number of points already collected, favor exploitation over exploration.
  return(sd*(prob*(1-prob) + 0.25/99))
}

# High flux condition
fill.sample.sub.ener = function(x, model.flux, model.ener){
  # Evaluate the Kriging model function at x 
  res.flux = predict(object = model.flux, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')
  res.ener = predict(object = model.ener, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')

  # Probability distribution fits a Gaussian distribution.
  # Want the probability that the energy is below 40 kJ/mol C, 
  # flux is faster than 10% of the maximum (which still meets the energy criteria)
  E.cutof = log10(20); # approximately halfway between the minimum energy and the top of the concavity in the Pareto front
  # F.cutof = 0.1*max(GPar.front$Flux.mol.m2s)
  F.cutof = 2.2e-3
  
  prob = c(1 - pnorm(q = 0, mean = res.flux$mean - F.cutof, sd = res.flux$sd) *
             pnorm(q = 0, mean = res.ener$mean - E.cutof, sd = res.ener$sd)
           )
  
  # Variance based on propagation of errors, assuming independent measures.
  # Since the optimization looks for relative differences, the square root is not necessary
  sd = (res.flux$sd^2 * res.ener$mean^2 + res.ener$sd^2 * res.flux$mean^2)

  # Acquisition function result. Weight the probability slightly such that the maximum is 
  # 2 orders of magnitude higher than the minimum (and the minimum is not zero)
  # Given the number of points already collected, favor exploitation over exploration.
  return(sd*(prob*(1-prob) + 0.25/99))
}

# Next point search function
fill.sample.sub = function(GPar.data){
  # Models
  mod.flux = fill.sample.mod(GPar.data = GPar.data, input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                             output.name = 'Flux.mol.m2s')
  # Energy model has substantial outliers above 1e3 kJ/mol that skew the model; 
  # a more accurate model is found when restricted
  mod.ener = fill.sample.ener(GPar.data = GPar.data, 
                              input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                              output.name = 'Energy.kJ.mol')

  # Next point by genetic algorithm: Max flux region
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.sub.flux(x, model.flux = mod.flux, model.ener = mod.ener)},
               lower = c(2, 0, -2, -7), upper = c(13.5, 5.5, 0.5, 0.7),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  point.next = GA.pred@solution[1,]
  GPar.flux = data.frame(pka1 = point.next[1],
                        pka2 = point.next[2],
                        logA = point.next[3],
                        Na.A = point.next[4])
  
  # True result for both the energy and kinetics to add this to the dataset
  res = PCET.obj.flu(inputs = point.next)
  GPar.flux$Energy.kJ.mol = res[1]
  GPar.flux$Flux.mol.m2s  = -res[2] # Flip sign because optimization function minimizes
  GPar.flux$order = max(GPar.data$order) + 1

  # Also add the fitness to the dataframe for iteration cutoffs
  GPar.flux$fit = max(GA.pred@fitness)
  
  # Next point by genetic algorithm: Min energy region
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.sub.ener(x, model.flux = mod.flux, model.ener = mod.ener)},
               lower = c(2, 0, -2, -7), upper = c(13.5, 5.5, 0.5, 0.7),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  point.next = GA.pred@solution[1,]
  GPar.ener = data.frame(pka1 = point.next[1],
                        pka2 = point.next[2],
                        logA = point.next[3],
                        Na.A = point.next[4])
  
  # True result for both the energy and kinetics to add this to the dataset
  res = PCET.obj.flu(inputs = point.next)
  GPar.ener$Energy.kJ.mol = res[1]
  GPar.ener$Flux.mol.m2s  = -res[2] # Flip sign because optimization function minimizes
  GPar.ener$order = max(GPar.data$order) + 1

  # Also add the fitness to the dataframe for iteration cutoffs
  GPar.ener$fit = max(GA.pred@fitness)
  
  # Return the new point and the fitness
  return(GPar.new = rbind(GPar.flux, GPar.ener))
}

```

```{r Sub-region Capture Refinement: Refinement Iterations, warning=FALSE, message=FALSE}
# Load data
GPar.all = read.csv(file = 'GPar_90CapOpt_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data_90CapOpt.csv')
# Remove indices
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]
GPar.front = GPar.front[, !(names(GPar.front) %in% c("X"))]

# First iteration to set the baseline of how much improvement there is to find.
newpoint = fill.sample.sub(GPar.data = GPar.all)
start.fit = max(newpoint$fit); 
current.fit = max(newpoint$fit); 

# Repeat for a maximum of 50 iterations, or until the fitness drops below 1/1000 of the starting fitness, 
# indicating little further improvement (50 iterations = 100 points)
max.iter = max(GPar.all$order) + 50
while(max(GPar.all$order) < max.iter & current.fit*1e3 > start.fit){
  GPar.all = rbind(GPar.all, newpoint[,names(newpoint) %in% names(GPar.all)])
  newpoint = fill.sample.sub(GPar.data = GPar.all)
  current.fit = max(newpoint$fit)
}

# Store the data
write.csv(GPar.all, file = 'GPar_SubRef_data.csv')
# Updated Pareto front
test = as.matrix(GPar.all[GPar.all$Flux.mol.m2s > 0,names(GPar.all) %in% c('Energy.kJ.mol', 'Flux.mol.m2s')])
# For the Pareto front determination, need both to minimize
test[,2] = -test[,2]
par.front = t(nondominated_points(points = t(test)))
par.front[,2] = -par.front[,2]
# Identify the conditions leading to the Pareto front
GPar.front = filter(GPar.all, Energy.kJ.mol %in% par.front[,1], Flux.mol.m2s %in% par.front[,2])
GPar.front = GPar.front[,!names(GPar.front) %in% 'X']
write.csv(GPar.front, file = 'GPar_fnt_data_SubRef.csv')

```


```{r Sub-region Capture Refinement: Plots}
GPar.all = read.csv(file = 'GPar_SubRef_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data_SubRef.csv')

region.flux = data.frame(flux.max = max(GPar.front$Flux.mol.m2s), flux.min = 22e-3,
                     ener.max = 45, ener.min = min(GPar.front$Energy.kJ.mol)
                     )

region.ener = data.frame(flux.max = max(GPar.front$Flux.mol.m2s), flux.min = 0.1*max(GPar.front$Flux.mol.m2s),
                     ener.max = 20, ener.min = min(GPar.front$Energy.kJ.mol)
                     )

ggplot() +
  # Bounds
  geom_rect(region.flux, mapping = aes(xmax = flux.max, xmin = flux.min, ymax = ener.max, ymin = ener.min,
                                       color = 'flux'),
            fill = 'green', linetype = 1, alpha = 0.5) +
  geom_rect(region.ener, mapping = aes(xmax = flux.max, xmin = flux.min, ymax = ener.max, ymin = ener.min,
                                       color = 'ener'),
            fill = 'blue', linetype = 1, alpha = 0.5) +
  # Data
  geom_point(data = filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100), 
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  geom_point(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'pareto')) +
  geom_line(data = GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'pareto')) +
  scale_color_manual(values = c('pareto' = 'red', 'flux' = 'green', 'ener' = 'blue'),
                     labels = c('pareto' = 'Pareto Front', 'flux' = 'High Flux', 'ener' = 'Low Energy'),
                     name = '') +
  guides(colour = guide_legend(override.aes = list(shape = 15, size = 2, alpha = c(0.5, 0.5, 1),
                                                   fill = NA, linetype = c(0, 0, 0)))) +
  labs(x = 'CO2 Flux (mol/m2/s)', y = 'Energy (kJ/mol C)', subtitle = 'Subregions for refinement')


```

Sampling of these subregions led to substantially better resolution to the Pareto frontier and the region near the borders, and thus likely improves the estimation of the broader criteria based on MEA capture.

Further analysis (eg. suggested ranges for the variables) on the marginals will be conducted after combining this dataset with that of adding acid instead of adding base. See /../PCET_Process

