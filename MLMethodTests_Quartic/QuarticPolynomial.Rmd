---
title: "Pattern Recognition Method Comparison"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook takes the data generated by the script in /Ex_Quartic, which describes the newly developed method, and compares it existing pattern recognition algorithms. 
This multi-objective problem was selected as a simple illustration, as it only takes 2 inputs and provides 2 outputs, but has the complication of a local minimum and maximum besides the Pareto optimum.

The pattern recognition algorithms are:
* Support Vector Machines: example supervised learning problem
* Gaussian mixture models: example unsupervised learning problem

These are compared to the process of characterizing the optimal inputs using Gaussian Processes refined by iterative sampling.
The acceptance criteria for the GP-based method and the SVM method are identical for direct comparison. 
For ease of calculation, the iterative refinement of the GP models are not included here, nor are the calculations of the conditional probabilities.
The SVM and GMM methods are tested with both the data after finding the Pareto front and the data after iterative refinement to see if the refinement step is useful for these other processes as well.

Comparison among the supervised methods is done by looking at the error rate across the entire space. 
Since the test function is a simple polynomial, the solution of what is acceptable can be found explicitly. 
For the GP method, since acceptance is defined by a probability, the error rate is weighted by the probability of acceptance. 
For the SVM method, the error rate is simply the number of incorrectly categorized points divided by the total number of test points.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
# Gaussian Mixture Models
library(mclust)
# Support Vector Machines
library(e1071)
```

Relevant functions

The quartic functions to optimize.

```{r Functions}
# Set (x1, x2) on range of [0, 5]

# Objective functions are based on the quadratic functions used previously, but with an additional local minimum
f1 = function(x1, x2){
  return(20*(x1 - 0.75)^2 + 190 + 11.58*x2^4 - 115.85*x2^3 + 383.13*x2^2 - 463.50*x2)
}
# The second objective function is also partially rotated so the local optimum is not perfectly aligned
f2 = function(x1, x2){
  # Remap both variables: rotate 30 degrees counterclockwise
  ang = -pi/24
  n1 = x1*cos(ang) - x2*sin(ang)
  n2 = x1*sin(ang) + x2*cos(ang)
  # return((x1 - 2.5)^2 + 80 + 1.778*x2^4 - 20*x2^3 + 78.573*x2^2 - 124.664*x2)
  return((n1 - 2.5)^2 + 80 + 1.778*n2^4 - 20*n2^3 + 78.573*n2^2 - 124.664*n2)
}


# Using GPareto: Need inputs and outputs as vectors/matrices not as dataframes. Coarse initial design
fun = function(x){
  x1 = x[1]; x2 = x[2]
  return(c(f1(x1, x2), f2(x1, x2)))
}

```

Normalization-related functions
```{r Functions part 2}
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))

  return(GPar.data)
}

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  # Check if the angle is the same as a point on the Pareto front
  if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}
```

Gaussian process parameter tuning
```{r}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 covtyp = 'gauss', # Gaussian uncertainty
                 optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50, # Increase robustness
                                max.generations = 400), # Some convergence issues
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}
```


## Comparison of GP-based Boundaries with Different Acceptance Criteria

Comparison of the boundaries to show how changing the acceptance criteria changes the shape of the near-Pareto set. Showcases the robustness to different selection criteria, indicating flexibility in the design objectives.

```{r Comparison of Acceptably Optimal Sets}
# Load datasets for obtaining the refined probability functions
data.delta = read.csv('GPar_Accept_Delta1.csv')
data.cutof = read.csv('GPar_Accept_Threshold.csv')
data.radan = read.csv('GPar_Accept_Radius.csv')
# Compare to the estimate of the Pareto frontier
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

##
# Grid of the relevant region to visualize
lower = c(0, 0); upper = c(5,5); grid.sz = 100 # Based on previous samples, no boundary is greater than 3.5
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = grid.sz), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = grid.sz))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')

##
# Normalized distance
mod.dist = fill.sample.mod(GPar.data = data.delta, input.name = c('x1', 'x2'), output.name = 'dist')
res = predict(object = mod.dist, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd
fine.grid$prob.delta = pnorm(q = 0, mean = fine.grid$dist.mean - 1, sd = fine.grid$dist.sd)

##
# Threshold cutoff
mod.f1 = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f2.norm')

# Apply Kriging functions
res = predict(object = mod.f1, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob.cutof = pnorm(q = 0, mean = fine.grid$f1.mean - 1, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 1, sd = fine.grid$f2.sd)

##
# Radius-angle
mod.rad = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'theta')

# Apply Kriging functions
res = predict(object = mod.rad, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob.radan = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

sep = 0
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta, color = 'delta'), breaks = c(0.5 + sep, 0.5 - sep)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof, color = 'cutof'), breaks = c(0.5 + sep, 0.5 - sep)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan, color = 'radan'), breaks = c(0.5 + sep, 0.5 - sep)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), level = 0.95) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Acceptance Criteria') +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'blue', 'radan' = 'green', 'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'radan' = expression('r < 1, '*theta*' > 18'^'o'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'radan', 'Pareto')) +
  theme_classic() + theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))


```

A closer inspection of each GP-discovered boundary compared to the actual boundary based on fine-resolution function evaluation.

```{r GP-True Comparison: Distance}
fine.grid$prob.delta05 = pnorm(q = 0, mean = fine.grid$dist.mean - 0.5, sd = fine.grid$dist.sd)
fine.grid$prob.delta1 = pnorm(q = 0, mean = fine.grid$dist.mean - 1, sd = fine.grid$dist.sd)
fine.grid$prob.delta2 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$prob.delta4 = pnorm(q = 0, mean = fine.grid$dist.mean - 4, sd = fine.grid$dist.sd)

sep = 0.1

# Compare to actual
fine.grid$f1 = f1(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid$f2 = f2(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid = n.obj(GPar.data = fine.grid, GPar.front = GPar.front)
fine.grid$dist = NaN
for(row in 1:nrow(fine.grid)){
  fine.grid$dist[row] = n.dist(f1.norm = fine.grid$f1.norm[row], 
                               f2.norm = fine.grid$f2.norm[row], 
                               GPar.front = GPar.front)
}

g.est = ggplot() +
  # Boundaries: where the probability is 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta05, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta1, color = '1'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta2, color = '2'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta4, color = '4'), breaks = c(0.5)) +
  # Uncertainty around it: +/- the separation
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta05, color = '0.5'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta1, color = '1'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta2, color = '2'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta4, color = '4'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Normalized\nPareto Dist', 
       subtitle = 'Estimated Boundaries') +
  theme_classic() + theme(legend.position = c(0.9, 0.5), 
                          legend.background = element_rect(fill = alpha(colour = 'white', alpha = 0.5))) + 
  scale_color_brewer(palette = 'BrBG') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5))

g.tru = ggplot() +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = '1'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = '2'), breaks = c(2)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = '4'), breaks = c(4)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  theme_classic() +  scale_color_brewer(palette = 'BrBG') + guides(color = FALSE) +
  labs(x = expression('x'[1]), y = '', subtitle = 'True Boundaries') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5))
g.est + g.tru

# Single variable conditional
Infer.plt = read.csv('Marginals_Delta.csv')
ggplot(Infer.plt) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(ncond))) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Calculate the error rate
error.rate = data.frame(error = 
                        mean(c(1 - filter(fine.grid, dist <= 1)$prob.delta, 
                               filter(fine.grid, dist > 1)$prob.delta)),
                        method = 'GP',
                        criteria = 'Pareto Distance')

```

```{r GP-True Comparison: Threshold}
fine.grid$prob.ff05 = pnorm(q = 0, mean = fine.grid$f1.mean - 0.5, sd = fine.grid$f1.sd) *
  pnorm(q = 0, mean = fine.grid$f2.mean - 0.5, sd = fine.grid$f2.sd)
fine.grid$prob.ff1 = pnorm(q = 0, mean = fine.grid$f1.mean - 1, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 1, sd = fine.grid$f2.sd)
fine.grid$prob.ff2 = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$prob.ff4 = pnorm(q = 0, mean = fine.grid$f1.mean - 4, sd = fine.grid$f1.sd) *
  pnorm(q = 0, mean = fine.grid$f2.mean - 4, sd = fine.grid$f2.sd)

sep = 0.1

# Compare to actual: set up to pass/fail 
fine.grid$ff05 = 0; fine.grid$ff1 = 0; fine.grid$ff2 = 0; fine.grid$ff4 = 0
fine.grid$ff05[fine.grid$f1.norm <= 0.5 & fine.grid$f2.norm <= 0.5] = 1; 
fine.grid$ff1[fine.grid$f1.norm <= 1 & fine.grid$f2.norm <= 1] = 1; 
fine.grid$ff2[fine.grid$f1.norm <= 2 & fine.grid$f2.norm <= 2] = 1; 
fine.grid$ff4[fine.grid$f1.norm <= 4 & fine.grid$f2.norm <= 4] = 1

g.est = ggplot() +
  # Boundaries: where the probability is 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff05, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff1, color = '1'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff2, color = '2'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff4, color = '4'), breaks = c(0.5)) +
  # Uncertainty around it: +/- the separation
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff05, color = '0.5'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff1, color = '1'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff2, color = '2'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.ff4, color = '4'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Normalized\nCutoff', 
       subtitle = 'Estimated Boundaries') +
  theme_classic() + theme(legend.position = c(0.9, 0.5), 
                          legend.background = element_rect(fill = alpha(colour = 'white', alpha = 0.5))) + 
  scale_color_brewer(palette = 'BrBG') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5))

g.tru = ggplot() +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff05, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1, color = '1'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff2, color = '2'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff4, color = '4'), breaks = c(0.5)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  theme_classic() +  scale_color_brewer(palette = 'BrBG') + guides(color = FALSE) +
  labs(x = expression('x'[1]), y = '', subtitle = 'True Boundaries') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5))
g.est + g.tru

# Single variable conditional
Infer.plt = read.csv('Marginals_Threshold.csv')
ggplot(Infer.plt) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(ncond))) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if f'[1]*'* < 1 & f'[2]*'* < 1'), 
       color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Error rate
error.rate = rbind(error.rate, data.frame(error = 
                        mean(c(1 - filter(fine.grid, ff1 == 1)$prob.cutof, 
                               filter(fine.grid, ff1 == 0)$prob.cutof)),
                        method = 'GP',
                        criteria = 'Threshold'))


```

```{r GP-True Comparison: Radius}
fine.grid$prob.r05 = pnorm(q = 0, mean = fine.grid$rad.mean - 0.5, sd = fine.grid$rad.sd)
fine.grid$prob.r1 = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd)
fine.grid$prob.r2 = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd)
fine.grid$prob.r4 = pnorm(q = 0, mean = fine.grid$rad.mean - 4, sd = fine.grid$rad.sd) 

sep = 0.1

# Compare to actual: set up to pass/fail 
fine.grid$rad = sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2)

g.est = ggplot() +
  # Boundaries: where the probability is 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r05, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r1, color = '1'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r2, color = '2'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r4, color = '4'), breaks = c(0.5)) +
  # Uncertainty around it: +/- the separation
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r05, color = '0.5'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r1, color = '1'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r2, color = '2'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.r4, color = '4'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Utopia\nDist', 
       subtitle = 'Estimated Boundaries') +
  theme_classic() + theme(legend.position = c(0.9, 0.5), 
                          legend.background = element_rect(fill = alpha(colour = 'white', alpha = 0.5))) + 
  scale_color_brewer(palette = 'BrBG') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5))

g.tru = ggplot() +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = '0.5'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = '1'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = '2'), breaks = c(2)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = '4'), breaks = c(4)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  theme_classic() +  scale_color_brewer(palette = 'BrBG') + guides(color = FALSE) +
  labs(x = expression('x'[1]), y = '', subtitle = 'True Boundaries') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5))
g.est + g.tru


```

```{r GP-True Comparison: Angle}

fine.grid$prob.t05 = pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd)
fine.grid$prob.t1 = pnorm(q = 0, mean = fine.grid$ang.mean - 40, sd = fine.grid$ang.sd)
fine.grid$prob.t2 = pnorm(q = 0, mean = fine.grid$ang.mean - 60, sd = fine.grid$ang.sd)
fine.grid$prob.t4 = pnorm(q = 0, mean = fine.grid$ang.mean - 80, sd = fine.grid$ang.sd) 

sep = 0.1

# Compare to actual: set up to pass/fail 
fine.grid$ang = atan(fine.grid$f2.norm/fine.grid$f1.norm)*180/pi*10/9

g.est = ggplot() +
  # Boundaries: where the probability is 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t05, color = '20%'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t1, color = '40%'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t2, color = '60%'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t4, color = '80%'), breaks = c(0.5)) +
  # Uncertainty around it: +/- the separation
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t05, color = '20%'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t1, color = '40%'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t2, color = '60%'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.t4, color = '80%'), 
               breaks = c(-sep, sep)+0.5, linetype = 2) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = expression('f'[2]*' Priority'), 
       subtitle = 'Estimated Boundaries') +
  theme_classic() + theme(legend.position = c(0.9, 0.5), 
                          legend.background = element_rect(fill = alpha(colour = 'white', alpha = 0.5))) + 
  scale_color_brewer(palette = 'BrBG') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5))

g.tru = ggplot() +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = '0.5'), breaks = c(20)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = '1'), breaks = c(40)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = '2'), breaks = c(60)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = '4'), breaks = c(80)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  theme_classic() +  scale_color_brewer(palette = 'BrBG') + guides(color = FALSE) +
  labs(x = expression('x'[1]), y = '', subtitle = 'True Boundaries') +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5))
g.est + g.tru
```

```{r GP-True Comparison: Radius-Theta}
fine.grid$prob.r05 = pnorm(q = 0, mean = fine.grid$rad.mean - 0.5, sd = fine.grid$rad.sd)
fine.grid$prob.r1 = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd)
fine.grid$prob.r2 = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd)
fine.grid$prob.r4 = pnorm(q = 0, mean = fine.grid$rad.mean - 4, sd = fine.grid$rad.sd) 

sep = 0.1

# Compare to actual: set up to pass/fail 
fine.grid$radan = 0
fine.grid$radan[fine.grid$rad <= 1 & fine.grid$ang > 20] = 1

ggplot() +
  # Boundaries: where the probability is 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan, color = 'est'), 
               breaks = c(0.5)) +
  # Uncertainty around it: +/- the separation
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan, color = 'est'), 
               breaks = c(0.5 + sep, 0.5 - sep), linetype = 2) +
  # Actual result
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan, color = 'tru'), 
               breaks = c(0.5)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black', 
              level = 0.95, method = 'loess', formula = y~x) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Boundaries') +
  theme_classic() + theme(legend.position = c(0.9, 0.5), 
                          legend.background = element_rect(fill = alpha(colour = 'white', alpha = 0.5))) + 
  scale_color_manual(values = c('est' = 'red', 'tru' = 'black'), labels = c('est' = 'Estimate', 'tru' = 'True')) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5))

Infer.plt = read.csv('Marginals_Radius.csv')
ggplot(Infer.plt) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(ncond))) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if r < 1, f'[1]*' priority < 80%'), 
       color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Error rate
error.rate = rbind(error.rate, data.frame(error = 
                        mean(c(1 - filter(fine.grid, radan == 1)$prob.radan, 
                               filter(fine.grid, radan == 0)$prob.radan)),
                        method = 'GP',
                        criteria = 'Utopia + Priority'))

```
Compared to the actual results, the model estimates match very well, even for boundaries that were farther or closer cutoffs and therefore not refined explicitly. 
The worst performance is in estimating the distance to the Pareto frontier and the relative prioritization, which are the criteria where there are nonlinear calculations on the function values of f1 and f2 to determine. 
This is likely because the GP model for the quartic polynomials are comparatively simple and easy to converge, but the nonlinear combinations of them to obtain the angle or Pareto distance are harder to tune the hyper-parameters for.

# Existing Method: Support Vector Machines

The method should use only the data from the initial Pareto front calculation. 
Additional samples may be necessary to refine the results, but there is no established method for this refinement as of yet.

SVM methods require existing labels for the points; in this case, the labels are whether or not the point meets the same three selection criteria as tested with the new GP method.

```{r SVM: Distance}
GPar.all = read.csv(file = 'GPar_all_start.csv')

# names(GPar.all)
GPar.all$cat = 0;
GPar.all$cat[GPar.all$dist <= 1] = 1
fit.rad = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "radial", cost = 5)
fit.lin = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "linear", cost = 5)
fit.pol = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "polynomia", cost = 5)
fit.sig = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "sigmoid", cost = 5)

res = predict(object = fit.rad, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.rad = res
res = predict(object = fit.lin, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.lin = res
res = predict(object = fit.pol, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.pol = res
res = predict(object = fit.sig, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.sig = res
sz = 1.5
g.rad = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.rad, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Radial') + guides(color = FALSE, shape = FALSE)
g.lin = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.lin, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Linear') + guides(color = FALSE, shape = FALSE)
g.pol = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.pol, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE, shape = FALSE)
g.sig = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.sig, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Sigmoid', shape = 'True Class', color = 'SVM Class')

(g.rad + g.lin) / (g.pol + g.sig)

res = predict(object = fit.lin, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.lin = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Linear') + guides(color = FALSE)
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.rad = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Radial') + guides(color = FALSE)
res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.pol = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE)
res = predict(object = fit.sig, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.sig = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), breaks = c(1)) +
  labs(subtitle = 'Sigmoid') + guides(color = FALSE)

(g.rad + g.lin) / (g.pol + g.sig)

# The only good quality fit is the radial and polynomial kernels. Find the error rates
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (dist <= 1 & res == 0) | (dist > 1 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Radial',
                        criteria = 'Pareto Distance'))

res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (dist <= 1 & res == 0) | (dist > 1 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Polynomial',
                        criteria = 'Pareto Distance'))


```
Graphically, the selection of the kernel form has a large impact on the quality of the result. 
The polynomial and radial kernels appear to roughly match the shape of the noramlized distance = 1 criterion; 
the linear and sigmoidal kernels do not appear to match anything. 
Further tuning of the polynomial degree or coefficient could improve the results, but without a training-testing set delineation, there is no way to validate the results.

Closer inspection of the SVM model with a fine-resolution grid shows the radial function produces an irregularly shaped result that is difficult to describe with means and standard deviations. 
The polynomial kernel model does not have this issue at the cost of accuracy.
The black-box function is not clearly invertible, making describing the boundary of what is accepted/rejected difficult.
Using a Monte Carlo method to marginalize to find the probability given only a single input variable first.

```{r}
x.rng = seq(from = 0, to = 5, length.out = 50)
svm.margin = data.frame()
nsamp = 2000
for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method)) +
  facet_wrap(~var, nrow = 2) + theme_classic() + theme(legend.position = c(0.9, 0.85)) +
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

p.cut.x1.pol = 0.5*max(filter(svm.margin, var == 'x1', method == 'pol')$prob)
p.cut.x2.pol = 0.5*max(filter(svm.margin, var == 'x2', method == 'pol')$prob)
x1.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'pol')$x)
x2.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'pol')$x)
p.cut.x1.rad = 0.5*max(filter(svm.margin, var == 'x1', method == 'rad')$prob)
p.cut.x2.rad = 0.5*max(filter(svm.margin, var == 'x2', method == 'rad')$prob)
x1.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'rad')$x)
x2.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'rad')$x)

for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.pol.rng), max = max(x2.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.rad.rng), max = max(x2.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.pol.rng), max = max(x1.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.rad.rng), max = max(x1.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method, linetype = as.factor(ncond))) +
  facet_wrap(~var, nrow = 2) + theme_classic() + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel', linetype = '# Conditions') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

write.csv(svm.margin, 'Margin_SVM_dist.csv')

```

The single-variable marginals roughly match expectation for x1, although the probabilities estimated by SVM are higher than those estimated by the GP. This is expected since the GP automatically accounts for model uncertainty, but the SVM Monte Carlo does not account for the inaccuracy of the model.
For x2, the marginal for the polynomial kernel does not capture the multimodal nature of the curve, consistent with the shape of the fine-resolution depiction of the model. The radial kernel captures this feature, but smooths it out compared to the GP estimate.
The partial conditional, where the secondary variable is bounded to its "optimal" region shows obvious improvement in the probability, but not in the the shape of the curves.

Repeating for the f1 and f2 cutoffs.

```{r SVM: Threshold}
GPar.all = read.csv(file = 'GPar_all_start.csv')

# names(GPar.all)
GPar.all$cat = 0;
GPar.all$cat[GPar.all$f1.norm <= 1 & GPar.all$f2.norm <= 1] = 1
fit.rad = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "radial", cost = 5)
fit.lin = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "linear", cost = 5)
fit.pol = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "polynomia", cost = 5)
fit.sig = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "sigmoid", cost = 5)

res = predict(object = fit.rad, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.rad = res
res = predict(object = fit.lin, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.lin = res
res = predict(object = fit.pol, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.pol = res
res = predict(object = fit.sig, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.sig = res
sz = 1.5
g.rad = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.rad, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Radial') + guides(color = FALSE, shape = FALSE)
g.lin = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.lin, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Linear') + guides(color = FALSE, shape = FALSE)
g.pol = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.pol, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE, shape = FALSE)
g.sig = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.sig, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Sigmoid', shape = 'True Class', color = 'SVM Class')

(g.rad + g.lin) / (g.pol + g.sig)

res = predict(object = fit.lin, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.lin = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Linear') + guides(color = FALSE)
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.rad = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Radial') + guides(color = FALSE)
res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.pol = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE)
res = predict(object = fit.sig, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.sig = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), breaks = c(1)) +
  labs(subtitle = 'Sigmoid') + guides(color = FALSE)

(g.rad + g.lin) / (g.pol + g.sig)

# The only good quality fit is the radial and polynomial kernels. Find the error rates
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (ff1 == 1 & res == 0) | 
                                      (ff1 == 0 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Radial',
                        criteria = 'Threshold'))

res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (ff1 == 1 & res == 0) | 
                                      (ff1 == 0 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Polynomial',
                        criteria = 'Threshold'))

```
```{r}
x.rng = seq(from = 0, to = 5, length.out = 50)
svm.margin = data.frame()
nsamp = 2000
for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method)) +
  facet_wrap(~var, nrow = 2) + theme_classic() + theme(legend.position = c(0.9, 0.85)) +
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

p.cut.x1.pol = 0.5*max(filter(svm.margin, var == 'x1', method == 'pol')$prob)
p.cut.x2.pol = 0.5*max(filter(svm.margin, var == 'x2', method == 'pol')$prob)
x1.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'pol')$x)
x2.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'pol')$x)
p.cut.x1.rad = 0.5*max(filter(svm.margin, var == 'x1', method == 'rad')$prob)
p.cut.x2.rad = 0.5*max(filter(svm.margin, var == 'x2', method == 'rad')$prob)
x1.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'rad')$x)
x2.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'rad')$x)

for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.pol.rng), max = max(x2.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.rad.rng), max = max(x2.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.pol.rng), max = max(x1.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.rad.rng), max = max(x1.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method, linetype = as.factor(ncond))) +
  facet_wrap(~var, nrow = 2) + theme_classic() + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel', linetype = '# Conditions') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

write.csv(svm.margin, 'Margin_SVM_threshold.csv')

```

Graphically, the results are the same as the distance cutoff: the polynomial and radial kernels perform best, with the radial kernel over-complicated and the polynomial kernel oversimplified compared to the true boundary.

Repeat for radius-angle cutoff
```{r SVM: Radius and Priority}
GPar.all = read.csv(file = 'GPar_all_start.csv')

# names(GPar.all)
GPar.all$cat = 0;
GPar.all$cat[sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2) <= 1 & GPar.all$theta > 20] = 1
fit.rad = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "radial", cost = 5)
fit.lin = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "linear", cost = 5)
fit.pol = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "polynomia", cost = 5)
fit.sig = svm(as.factor(cat) ~ x1*x2, data = GPar.all[,c('x1', 'x2', 'cat')], 
          scale = FALSE, kernel = "sigmoid", cost = 5)

res = predict(object = fit.rad, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.rad = res
res = predict(object = fit.lin, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.lin = res
res = predict(object = fit.pol, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.pol = res
res = predict(object = fit.sig, newdata = GPar.all[,c('x1', 'x2')])
GPar.all$fit.sig = res
sz = 1.5
g.rad = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.rad, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(1)) +
  labs(subtitle = 'Radial') + guides(color = FALSE, shape = FALSE)
g.lin = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.lin, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(1)) +
  labs(subtitle = 'Linear') + guides(color = FALSE, shape = FALSE)
g.pol = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.pol, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(1)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE, shape = FALSE)
g.sig = ggplot() + 
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = fit.sig, shape = as.factor(cat)),
             size = sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(1)) +
  labs(subtitle = 'Sigmoid', shape = 'True Class', color = 'SVM Class')

(g.rad + g.lin) / (g.pol + g.sig)

res = predict(object = fit.lin, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.lin = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(0.5)) +
  labs(subtitle = 'Linear') + guides(color = FALSE)
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.rad = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(0.5)) +
  labs(subtitle = 'Radial') + guides(color = FALSE)
res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.pol = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(0.5)) +
  labs(subtitle = 'Polynomial') + guides(color = FALSE)
res = predict(object = fit.sig, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
g.sig = ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = res)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan), breaks = c(0.5)) +
  labs(subtitle = 'Sigmoid') + guides(color = FALSE)

(g.rad + g.lin) / (g.pol + g.sig)

# The only good quality fit is the radial and polynomial kernels. Find the error rates
res = predict(object = fit.rad, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (radan == 1 & res == 0) | 
                                      (radan == 0 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Radial',
                        criteria = 'Utopia + Priority'))

res = predict(object = fit.pol, newdata = fine.grid[,c('x1', 'x2')])
fine.grid$res = res
error.rate = rbind(error.rate, data.frame(error =
                        nrow(filter(fine.grid, (radan == 1 & res == 0) | 
                                      (radan == 0 & res == 1) )) /
                          nrow(fine.grid),
                        method = 'SVM-Polynomial',
                        criteria = 'Utopia + Priority'))


```
```{r}
x.rng = seq(from = 0, to = 5, length.out = 50)
svm.margin = data.frame()
nsamp = 2000
for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = 0, max = 5))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'pol'))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 0, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method)) +
  facet_wrap(~var, nrow = 2) + theme_classic() + theme(legend.position = c(0.9, 0.85)) +
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

p.cut.x1.pol = 0.5*max(filter(svm.margin, var == 'x1', method == 'pol')$prob)
p.cut.x2.pol = 0.5*max(filter(svm.margin, var == 'x2', method == 'pol')$prob)
x1.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'pol')$x)
x2.pol.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'pol')$x)
p.cut.x1.rad = 0.5*max(filter(svm.margin, var == 'x1', method == 'rad')$prob)
p.cut.x2.rad = 0.5*max(filter(svm.margin, var == 'x2', method == 'rad')$prob)
x1.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x1', method == 'rad')$x)
x2.rad.rng = range(filter(svm.margin, prob > p.cut.x1.pol, var == 'x2', method == 'rad')$x)

for(x in x.rng){
  # x1 marginal
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.pol.rng), max = max(x2.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = min(x2.rad.rng), max = max(x2.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
  # x1 marginal
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.pol.rng), max = max(x1.pol.rng)))
  res = as.numeric(predict(object = fit.pol, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'pol'))
  temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = min(x1.rad.rng), max = max(x1.rad.rng)))
  res = as.numeric(predict(object = fit.rad, newdata = temp.frame))-1
  svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', ncond = 1, prob = sum(res)/length(res), method = 'rad'))
}

ggplot(svm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = method, linetype = as.factor(ncond))) +
  facet_wrap(~var, nrow = 2) + theme_classic() + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'SVM Kernel', linetype = '# Conditions') +
  scale_color_discrete(labels = c('pol' = 'Polynomial', 'rad' = 'Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

write.csv(svm.margin, 'Margin_SVM_radius.csv')


```

Compared to the training data, the results work very well for the radial kernel only. However, when compared to the full system evaluation with a fine resolution grid, it is evident that there is some overfitting. This leads to unusual curves for the conditional probabilities. This generally indicates that more complicated selection criteria, particularly those that lead to discontinuous regions of acceptance, are not likely to behave well with the SVM.

# Existing Method: Gaussian Mixtures

Since the GM models are unsupervised, there cannot be control for what the selection criteria are. A superficial analysis will be conducted to check what the ML algorithm is converging to, but the results will be interpreted solely in terms of descriptive statistics.

For the purposes of analysis, three models will be tested based on the input to the ML algorithm:
* (x1, x2, f1, f2): the full set of inputs and outputs - since it has the most data, this will likely be the most robust
* (x1, x2): negative control of just the inputs - it should group the points based on the sampling densities
* (f1, f2): outputs only - this should have the best distinction between good and bad performing groups

Up to 12 categories will be allowed, and any type of Gaussian models will be accepted. This will lead to longer fitting time, but should help with accuracy.

```{r}
GPar.all = read.csv(file = 'GPar_all_start.csv')

# names(GPar.all)
max.cat = 12
cluster.all = Mclust(data = GPar.all[,c('x1', 'x2', 'f1', 'f2')], G = 2:max.cat)
cluster.in = Mclust(data = GPar.all[,c('x1', 'x2')], G = 2:max.cat)
cluster.out = Mclust(data = GPar.all[,c('f1', 'f2')], G = 2:max.cat)

```

```{r}
# summary(cluster, parameters = TRUE)

GPar.all$typ.all = cluster.all$classification
GPar.all$typ.in = cluster.in$classification
GPar.all$typ.out = cluster.out$classification

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.all)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.in)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(x1, x2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.out)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(f1, f2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

```

The complete input (x1, x2, f1, f2) overcomplicates the system, providing 8 groups. One of these groups is clearly the Pareto frontier, but it does not include any points that are of similar performance. There are points to either side of it suggesting similar performance, but how far they extend is difficult to interpret.

The negative control (x1, x2) gives the expected result of largely grouping based on sampling density. This means the highly sampled Pareto frontier and local minimum are their own groups, and the rest of the space is divided based around the boundary between the highly sampled regions.

The output-only model gives the most useful results, as it groups the Pareto frontier inside of another high-performing group, which also includes the local optimum and the space spanning to it. It roughly lines up with the Pareto distance or threshold criteria; it does not appear to prioritize the angle or utopia distance. Showing only this model at finer resolution as it is the only relevant performing model

```{r}
res = predict(object = cluster.out, newdata = fine.grid[c('f1', 'f2')])
fine.grid$cluster.out = res$classification
ggplot() +
  # Cluster results
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = as.factor(cluster.out))) +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), color = 'black', breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), color = 'red', breaks = c(0.5)) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(f1, f2)') +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

res = predict(object = cluster.all, newdata = fine.grid[c('x1', 'x2', 'f1', 'f2')])
fine.grid$cluster.all = res$classification
ggplot() +
  # Cluster results
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = as.factor(cluster.all)), size = 4) +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), color = 'black', breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ff1), color = 'red', breaks = c(0.5)) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Cluster results
  geom_point(data = filter(fine.grid, f2 < 50, f1 < 200), mapping = aes(x = f1, y = f2, color = as.factor(cluster.all)), size = 2) +
  labs(x = expression('f'[1]), y = expression('f'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  theme_classic() +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

```

The (f1, f2) model appears to give a cluster that is somewhere between a threshold cutoff and the Pareto distance cutoff. 
In contrast, the behavior of the (x1, x2, f1, f2) model provides many more groups, including splitting the Pareto front into about three different categories with no obvious analog to why the boundaries are where they are. 
It appears to roughly fit the same boundaries of Pareto distance or threshold cutoffs, but not very well. 
A rough approximation is that groups (5, 6) are the Pareto front, groups (2, 3, 7) make up the region close to the Pareto front, and (1, 4, 8) are the region far from the front.

The model for (f1, f2) is going to give the same conditional probabilities as the Pareto distance or threshold acceptance criteria based on this similarity in the shape of the boundary. 
The interesting result to interpret is the (x1, x2, f1, f2), particularly when marginalizing to (x1, x2). 
Assuming that calculating (f1, f2) is expensive, the best approximation is that found from the GP models used to find the Pareto front itself. 
These will give (f1, f2) as a bivariate Gaussian distribution, which can be sampled from to estimate the likelihood that it falls into the Pareto front, the region close to it, or the region far from it.
This is achievable with a sequential Monte Carlo: given x1, sample x2 from the range, find the distribution of (f1, f2), sample (f1, f2), and solve the classification into the three groups.

```{r}
f1.mod = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f1')
f2.mod = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f2')

x.rng = seq(from = 0, to = 5, length.out = 50)
nsamp.x = 100; nsamp.f = 100
gm.margin = data.frame()
for(x in x.rng){
  # x1
  # Sample x2
  inframe = data.frame(x1 = x, x2 = runif(n = nsamp.x, min = 0, max = 5))
  classes = c()
  for(n in 1:nrow(inframe)){
    # Estimate distribution for f1, f2
    f1.est = predict(object = f1.mod, newdata = inframe, type = 'UK')
    f2.est = predict(object = f2.mod, newdata = inframe, type = 'UK')
    test.frame = data.frame(x1 = inframe$x1, x2 = inframe$x2,
                            f1 = rnorm(n = nsamp.f, mean = f1.est$mean, sd = f1.est$sd),
                            f2 = rnorm(n = nsamp.f, mean = f2.est$mean, sd = f2.est$sd))
    res = predict(object = cluster.all, newdata = test.frame)
    classes = c(classes, res$classification)
  }
  classes = data.frame(group = classes)
  gm.margin = rbind(gm.margin, data.frame(x = x, var = 'x1', 
                         p.pare = nrow(filter(classes, group == 5 | group == 6))/(nsamp.f*nsamp.x), 
                         p.near = nrow(filter(classes, group == 2 | group == 3 | group == 7))/(nsamp.f*nsamp.x), 
                         p.dist = nrow(filter(classes, group == 1 | group == 4 | group == 8))/(nsamp.f*nsamp.x)))
  # x2
  # Sample x2
  inframe = data.frame(x2 = x, x1 = runif(n = nsamp.x, min = 0, max = 5))
  classes = c()
  for(n in 1:nrow(inframe)){
    # Estimate distribution for f1, f2
    f1.est = predict(object = f1.mod, newdata = inframe, type = 'UK')
    f2.est = predict(object = f2.mod, newdata = inframe, type = 'UK')
    test.frame = data.frame(x1 = inframe$x1, x2 = inframe$x2,
                            f1 = rnorm(n = nsamp.f, mean = f1.est$mean, sd = f1.est$sd),
                            f2 = rnorm(n = nsamp.f, mean = f2.est$mean, sd = f2.est$sd))
    res = predict(object = cluster.all, newdata = test.frame)
    classes = c(classes, res$classification)
  }
  classes = data.frame(group = classes)
  gm.margin = rbind(gm.margin, data.frame(x = x, var = 'x2', 
                         p.pare = nrow(filter(classes, group == 5 | group == 6))/(nsamp.f*nsamp.x), 
                         p.near = nrow(filter(classes, group == 2 | group == 3 | group == 7))/(nsamp.f*nsamp.x), 
                         p.dist = nrow(filter(classes, group == 1 | group == 4 | group == 8))/(nsamp.f*nsamp.x)))
}

write.csv(gm.margin, 'Margin_GM.csv')
```

```{r}
gm.margin = read.csv('Margin_GM.csv')
gm.margin = data.frame(x = rep(gm.margin$x, 3),
                       var = rep(gm.margin$var, 3),
                       prob = c(gm.margin$p.pare, gm.margin$p.near, gm.margin$p.dist),
                       typ = c(rep('Pareto', nrow(gm.margin)), 
                               rep('Near-Pareto', nrow(gm.margin)), 
                               rep('Suboptimal', nrow(gm.margin))))
ggplot(gm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = typ)) +
  facet_wrap(~var, nrow = 2) +
  scale_color_brewer(palette = 'Dark2') + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'Region')

gm.margin = read.csv('Margin_GM.csv')
gm.margin = data.frame(x = rep(gm.margin$x),
                       var = rep(gm.margin$var),
                       prob = gm.margin$p.pare + gm.margin$p.near,
                       typ = 'Optimal')
ggplot(gm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = typ)) +
  facet_wrap(~var, nrow = 2) +
  scale_color_brewer(palette = 'Dark2') + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'Region')

```

The estimated probability of being optimal (Pareto front group or the near-Pareto group) is similar to that calculated through the other methods for the Pareto distance or threshold criteria. However, because of the numerous variables required to generate the model, a larger and more complicated sampling procedure is necessary for accurate estimates.


# Comaprison of method accuracies

```{r}
error.rate
ggplot(error.rate) +
  geom_col(mapping = aes(x = method, y = error, fill = method)) +
  facet_grid(~criteria) +
  labs(x = '', y = 'Error Rate', fill = 'Model Method') +
  scale_x_discrete(labels = c(), breaks = c()) + scale_y_continuous(expand = c(0, 0))

p = aov(error~method*criteria, error.rate)
summary(p)

```




# Overall:
* The unsupervised ML model (Gaussian mixture) can only tend towards the criteria of having output values similar to that of the Pareto front. However, in order to obtain the model, all of the inputs and outputs are used, meaning to solve the single-variable probabilities, one must perform a time-consuming nested sampling procedure.
* The supervised ML model (Support Vector Machines) are flexible to selection criteria because those are part of the user input into generating the model. It is very sensitive to the kernel function that was selected, with the radial and polynomial forms working best for this specific case. When compared to the actual function, these good-fitting kernels appeared to be over-fit to the data and cannot capture the full dynamics of the space. In practice, separate training and testing sets should be used, which may be difficult for more complex objective functions due to more function evaluations. 
* The GP method requires iterative sampling, and therefore requires more time to compute initially. However, it accounts for the uncertainty in the model explicitly, whereas the SVM methods rely purely on frequentist sampling. This means the single-variable probabilities from the GP model estimate are generally lower than those estimated from the SVM.
* More complicated conditions, such as only accepting points that prioritize one objective over the other by a certain margin, are difficult for both supervised learning methods to estimate, but the results from the SVM appear to be worse compared to the actual boundaries.
* The SVM methods give single-variable probabilities that are 

