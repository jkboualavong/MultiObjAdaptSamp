---
title: "Pattern Recognition Method Comparison: Quartic Polynomial"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook takes the data generated by the script in /Ex_Quartic, which describes the newly developed adaptive sampling and classification method, and compares it existing classification algorithms. 

The classification algorithms are:
* Support Vector Machines: example supervised learning problem
* Gaussian mixture models: example unsupervised learning problem

For ease of calculation, the iterative refinement of the GP models are not included here, nor are the calculations of the marginalized probabilities.
The SVM and GMM methods are tested with (1) the data after finding the Pareto front (before the new adaptive sampling process), (2) the data after adaptive sampling refinement, and (3) the data after the Pareto front search with additional random sampling to see if the new sampling method is useful for these other processes as well.

Comparison among the supervised methods is done by looking at the error rate across the entire space. 
Since the test function is a simple polynomial, the solution of what is acceptable can be found explicitly.
Since the GP method produces a fuzzy classifier, the error rate is the average misclassification probability.
For the SVM method, the error rate is simply the number of incorrectly categorized points divided by the total number of test points.

In addition to the accuracy comparison, a comparison of the importance ranking metric developed for using GP in classification problems will be compared to Shapley values.
As with the accuracy comparison, this will be tested against the data before refinement, after refinement, or after a random sample.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
# Gaussian Mixture Models
library(mclust)
# Support Vector Machines
library(e1071)
```

# Relevant functions

The quartic objective functions

```{r Functions}
# Set (x1, x2) on range of [0, 5]

# Objective functions are based on the quadratic functions used previously, but with an additional local minimum
f1 = function(x1, x2){
  return(20*(x1 - 0.75)^2 + 190 + 11.58*x2^4 - 115.85*x2^3 + 383.13*x2^2 - 463.50*x2)
}
# The second objective function is also partially rotated so the local optimum is not perfectly aligned
f2 = function(x1, x2){
  # Remap both variables: rotate 30 degrees counterclockwise
  ang = -pi/24
  n1 = x1*cos(ang) - x2*sin(ang)
  n2 = x1*sin(ang) + x2*cos(ang)
  # return((x1 - 2.5)^2 + 80 + 1.778*x2^4 - 20*x2^3 + 78.573*x2^2 - 124.664*x2)
  return((n1 - 2.5)^2 + 80 + 1.778*n2^4 - 20*n2^3 + 78.573*n2^2 - 124.664*n2)
}


# Using GPareto: Need inputs and outputs as vectors/matrices not as dataframes. Coarse initial design
fun = function(x){
  x1 = x[1]; x2 = x[2]
  return(c(f1(x1, x2), f2(x1, x2)))
}

```

Normalization-related functions
```{r Functions part 2}
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))

  return(GPar.data)
}

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  # Check if the angle is the same as a point on the Pareto front
  if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}
```

Gaussian process parameter tuning
```{r}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = DiceKriging::km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 covtyp = 'gauss', # Gaussian uncertainty
                 optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50, # Increase robustness
                                max.generations = 400), # Some convergence issues
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}
```

# Comparison of GP-based Boundaries with Different Acceptance Criteria

Comparison of the boundaries to show how changing the acceptance criteria changes the shape of the near-Pareto set. Showcases the robustness to different selection criteria, indicating flexibility in the design objectives.

```{r GP: Loading Data}
## Loading
# Load datasets for obtaining the refined probability functions
data.delta = read.csv('../Ex_Quartic/GPar_Accept_Delta1.csv')
data.cutof = read.csv('../Ex_Quartic/GPar_Accept_Threshold.csv')
data.radan = read.csv('../Ex_Quartic/GPar_Accept_Radius.csv')
# Load datasets prior to refinement
data.paret = read.csv('../Ex_Quartic/GPar_all_start.csv')
data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
# Compare to the estimate of the Pareto frontier
GPar.front = read.csv(file = '../Ex_Quartic/GPar_fnt_start.csv')

# Add a random samples to simulate the effect of sample size rather than adaptive sampling
# Have it stored so for consistency, but also create multiple random sample sets
# to get a sense of the variance
if(file.exists('GPar_Random.csv') == FALSE){
  nsamp = max(nrow(data.delta), nrow(data.cutof), nrow(data.radan)) - nrow(data.paret)
  nsamp = nsamp*10 # Collect more than needed for sample size variance testing
  data.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                          x2 = runif(n = nsamp, min = 0, max = 5))
  data.rando$f1 = f1(x1 = data.rando$x1, x2 = data.rando$x2)
  data.rando$f2 = f2(x1 = data.rando$x1, x2 = data.rando$x2)
  # Fill in the remaining calculations: normalized outputs, distance, theta, order
  data.rando = n.obj(GPar.data = data.rando, GPar.front = GPar.front)
  cl = makeCluster(2)
  registerDoParallel(cl)
  dist = foreach(row = 1:nrow(data.rando)) %dopar%
    n.dist(f1.norm = data.rando$f1.norm[row], f2.norm = data.rando$f2.norm[row], GPar.front = GPar.front)
  stopCluster(cl)
  data.rando$dist = unlist(dist)
  data.rando$rad = sqrt(data.rando$f1.norm^2 + data.rando$f2.norm^2)
  data.rando$theta = atan(data.rando$f2.norm/data.rando$f1.norm)*180/pi*10/9
  data.rando$order = seq(from = max(data.paret$order) + 1, to = max(data.paret$order) + nsamp, by = 1)
  data.rando = rbind(data.paret[, names(data.paret) %in% names(data.rando)], data.rando)

  # Store the random data
  write.csv(data.rando, file = 'GPar_Random.csv', row.names = F)
}
data.rando = read.csv(file = 'GPar_Random.csv')
data.rando = data.rando[1:nrow(data.delta),] # Keep same sample size for initial testing
# Extra random samples are used later for Monte Carlo

##
# Grid of the relevant region to visualize and compare
lower = c(0, 0); upper = c(5,5); grid.sz = 100
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = grid.sz), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = grid.sz))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')
# Calculate the actual results
fine.grid$f1 = f1(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid$f2 = f2(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid = n.obj(GPar.data = fine.grid, GPar.front = GPar.front)
fine.grid$dist = NaN
for(row in 1:nrow(fine.grid)){
  fine.grid$dist[row] = n.dist(f1.norm = fine.grid$f1.norm[row], 
                               f2.norm = fine.grid$f2.norm[row], 
                               GPar.front = GPar.front)
}
fine.grid$ang = atan(fine.grid$f2.norm/fine.grid$f1.norm)*180/pi*10/9
fine.grid$rad = sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2)
```

```{r GP: Generating Models}
##
# Normalized distance
fine.grid$delta = 0
fine.grid$delta[fine.grid$dist <= 1] = 1

mod.dist.paret = fill.sample.mod(GPar.data = data.paret, input.name = c('x1', 'x2'), output.name = 'dist')
mod.dist.adapt = fill.sample.mod(GPar.data = data.delta, input.name = c('x1', 'x2'), output.name = 'dist')
mod.dist.rando = fill.sample.mod(GPar.data = data.rando, input.name = c('x1', 'x2'), output.name = 'dist')

res = predict(object = mod.dist.paret, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$prob.delta.paret = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
res = predict(object = mod.dist.adapt, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$prob.delta.adapt = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
res = predict(object = mod.dist.rando, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$prob.delta.rando = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)

##
# Threshold cutoff
fine.grid$cutof = 0
fine.grid$cutof[fine.grid$f1.norm <= 1 & fine.grid$f2.norm <= 1] = 1

mod.f1.paret = fill.sample.mod(GPar.data = data.paret, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2.paret = fill.sample.mod(GPar.data = data.paret, input.name = c('x1', 'x2'), output.name = 'f2.norm')
mod.f1.adapt = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2.adapt = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f2.norm')
mod.f1.rando = fill.sample.mod(GPar.data = data.rando, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2.rando = fill.sample.mod(GPar.data = data.rando, input.name = c('x1', 'x2'), output.name = 'f2.norm')

res1 = predict(object = mod.f1.paret, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.f2.paret, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.cutof.paret = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
res1 = predict(object = mod.f1.adapt, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.f2.adapt, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.cutof.adapt = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
res1 = predict(object = mod.f1.rando, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.f2.rando, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.cutof.rando = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)

##
# Radius-angle
fine.grid$radan = 0
fine.grid$radan[fine.grid$rad <= 1 & fine.grid$ang > 20] = 1

mod.rad.paret = fill.sample.mod(GPar.data = data.paret, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang.paret = fill.sample.mod(GPar.data = data.paret, input.name = c('x1', 'x2'), output.name = 'theta')
mod.rad.adapt = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang.adapt = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'theta')
mod.rad.rando = fill.sample.mod(GPar.data = data.rando, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang.rando = fill.sample.mod(GPar.data = data.rando, input.name = c('x1', 'x2'), output.name = 'theta')

res1 = predict(object = mod.rad.paret, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.ang.paret, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.radan.paret = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
res1 = predict(object = mod.rad.adapt, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.ang.adapt, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.radan.adapt = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
res1 = predict(object = mod.rad.rando, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
res2 = predict(object = mod.ang.rando, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$prob.radan.rando = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
  (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))

rm(res1, res2)
```

```{r GP: Plotting True Results}
sep = 0
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.paret, color = 'delta'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.paret, color = 'cutof'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.paret, color = 'radan'), 
               breaks = c(sep, -sep)+0.5) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), level = 0.95, method = 'loess') + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Acceptance Criteria', subtitle = 'Before Refinement') +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'skyblue2', 'radan' = 'green', 'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'radan' = expression('r < 1, '*theta*' > 18'^'o'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'radan', 'Pareto')) +
  theme_classic() + theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.adapt, color = 'delta'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.adapt, color = 'cutof'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.adapt, color = 'radan'), 
               breaks = c(sep, -sep)+0.5) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), level = 0.95, method = 'loess') + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Acceptance Criteria', subtitle = 'After Adaptive Sampling Refinement') +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'skyblue2', 'radan' = 'green', 'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'radan' = expression('r < 1, '*theta*' > 18'^'o'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'radan', 'Pareto')) +
  theme_classic() + theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.rando, color = 'delta'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.rando, color = 'cutof'), 
               breaks = c(sep, -sep)+0.5) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.rando, color = 'radan'), 
               breaks = c(sep, -sep)+0.5) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), level = 0.95, method = 'loess') + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Acceptance Criteria', subtitle = 'After Random Sampling') +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'skyblue2', 'radan' = 'green', 'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'radan' = expression('r < 1, '*theta*' > 18'^'o'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'radan', 'Pareto')) +
  theme_classic() + theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

```

```{r GP: Comparison of sets to the real result}
# Plotting variables
sep = 0.05; trans = 0.25; ln.sz = 1.25

## Distance
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.paret, 
               fill = 'paret'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.adapt, 
               fill = 'adapt'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.rando, 
               fill = 'rando'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  # Actual boundaries
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.paret, color = 'paret'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.adapt, color = 'adapt'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta.rando, color = 'rando'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = delta, color = 'tru'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  scale_color_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  scale_fill_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  labs(x = expression('x'[1]), x = expression('x'[2]), subtitle = 'Criteria: Pareto Distance', color = '') +
  theme_classic() + guides(fill = 'none') + theme(legend.position = c(0.85, 0.85)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) + scale_y_continuous(limits = c(0, 5), expand = c(0, 0))

## Threshold
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.paret, 
               fill = 'paret'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.adapt, 
               fill = 'adapt'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.rando, 
               fill = 'rando'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  # Actual boundaries
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.paret, color = 'paret'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.adapt, color = 'adapt'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof.rando, color = 'rando'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof, color = 'tru'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  scale_color_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  scale_fill_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  labs(x = expression('x'[1]), x = expression('x'[2]), subtitle = 'Criteria: Objective function values', color = '') +
  theme_classic() + guides(fill = 'none') + theme(legend.position = c(0.85, 0.85)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) + scale_y_continuous(limits = c(0, 5), expand = c(0, 0))

## Radius-angle
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.paret, 
               fill = 'paret'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.adapt, 
               fill = 'adapt'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.rando, 
               fill = 'rando'), 
               breaks = c(sep, -sep)+0.5, linetype = 2, alpha = trans) +
  # Actual boundaries
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.paret, color = 'paret'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.adapt, color = 'adapt'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan.rando, color = 'rando'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = radan, color = 'tru'), 
               breaks = 0.5, linetype = 1, size = ln.sz) +
  scale_color_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  scale_fill_manual(breaks = c('tru', 'paret', 'adapt', 'rando'),
                     labels = c('tru' = 'True Boundary', 'paret' = 'Starting Dataset', 
                                'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'paret' = 'skyblue2', 'adapt' = 'red', 'rando' = 'green')) +
  labs(x = expression('x'[1]), x = expression('x'[2]), color = '',
       subtitle = expression('Criteria: Utopia point distance and f'[1]*' Priority > 80%')) +
  theme_classic() + guides(fill = 'none') + theme(legend.position = c(0.85, 0.85)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) + scale_y_continuous(limits = c(0, 5), expand = c(0, 0))


```

A closer inspection of each GP-discovered boundary compared to the actual boundary based on fine-resolution function evaluation.

```{r GP Marginals Comparison}
# Single variable conditionals compared to the expected conditionals by integration
Infer.plt = read.csv('../Ex_Quartic/Marginals_delta.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
        'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
        'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Pareto Distance'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

Infer.plt = read.csv('../Ex_Quartic/Marginals_cutof.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
        'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
        'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Cutoff Thresholds'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

Infer.plt = read.csv('../Ex_Quartic/Marginals_radan.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2, labeller = label_parsed) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
        'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
        'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Utopia Distance + Priority'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()


```

# GP-based Classifier Error Rates

The error rate in the GP model estimate should account for the built-in uncertainty in the model.
The probability that the model gives the wrong result given the $(x_1, x_2)$ coordinates is the probability that it accepts the point when it should reject it, or vice versa.
Since all $(x_1, x_2)$ are equally likely in this mathematical function, the error rate of the GP model is the average of these probabilities.
There should be an evident decrease in the error rate between the pre- and post-refinement models.

The Monte Carlo uncertainty in the error rate is $\sqrt{\frac{p(1-p)}{N}}$.

To save on computational cost, using the fine grid as a surrogate for Monte Carlo sampling.
The calculation separates the terms into type I (P[should reject | accepted]) and type II error (P[should accept | rejected]) error rates.
These conditionals are calcultaed by Bayes rule,
$P[X|Y] = P[X, Y] P[X] / P[Y]$

Calculating this error rate for as the sample size increases and comparing it to the range of possibilities when randomly sampling.

```{r GP: Sample Size Effects on Error - Functions}
mod.err.n = function(data.dist, data.f, data.rad, test.vector){
  # Given a training and testing dataset, gives the error of the model
  # Intent is to use n samples in the training dataset to compare effect of increasing
  # the sample size of different sampling approaches
  
  # Given data of size n, create all of the relevant models
  mod.dist = fill.sample.mod(GPar.data = data.dist, input.name = c('x1', 'x2'),
                             output.name = 'dist')
  mod.f1 = fill.sample.mod(GPar.data = data.f, input.name = c('x1', 'x2'),
                             output.name = 'f1.norm')
  mod.f2 = fill.sample.mod(GPar.data = data.f, input.name = c('x1', 'x2'),
                             output.name = 'f2.norm')
  mod.rad = fill.sample.mod(GPar.data = data.rad, input.name = c('x1', 'x2'), 
                            output.name = 'rad')
  mod.ang = fill.sample.mod(GPar.data = data.rad, input.name = c('x1', 'x2'), 
                            output.name = 'theta')
  
  # Calculate the predictions
  res.dist = predict(object = mod.dist,newdata = test.vector[, c('x1', 'x2')], type = 'UK')
  res.f1   = predict(object = mod.f1,  newdata = test.vector[, c('x1', 'x2')], type = "UK")
  res.f2   = predict(object = mod.f2,  newdata = test.vector[, c('x1', 'x2')], type = "UK")
  res.rad  = predict(object = mod.rad, newdata = test.vector[, c('x1', 'x2')], type = "UK")
  res.ang  = predict(object = mod.ang, newdata = test.vector[, c('x1', 'x2')], type = "UK")

  # Calculate the error likelihoods
  test.vector$prob.delta = pnorm(q = 0, mean = res.dist$mean - 1, sd = res.dist$sd)
  test.vector$prob.cutof = pnorm(q = 0, mean = res.f1$mean - 1, sd = res.f1$sd) * 
    pnorm(q = 0, mean = res.f2$mean - 1, sd = res.f2$sd)
  test.vector$prob.radan = pnorm(q = 0, mean = res.rad$mean - 1, sd = res.rad$sd) * 
    (1 - pnorm(q = 0, mean = res.ang$mean - 20, sd = res.ang$sd))
  
  # Calculate total, false positive, and false negative error rates
  tot.err = c(mean(c(1 - dplyr::filter(test.vector, delta == 1)$prob.delta, 
                       dplyr::filter(test.vector, delta == 0)$prob.delta)),
            mean(c(1 - dplyr::filter(test.vector, cutof == 1)$prob.cutof, 
                       dplyr::filter(test.vector, cutof == 0)$prob.cutof)),
            mean(c(1 - dplyr::filter(test.vector, radan == 1)$prob.radan, 
                       dplyr::filter(test.vector, radan == 0)$prob.radan)))

  neg.err = 
    c(sum(1 - dplyr::filter(test.vector, delta == 1)$prob.delta)*
        nrow(dplyr::filter(test.vector, delta == 1))/nrow(test.vector)^2/
        mean(1 - test.vector$prob.delta),
      
      sum(1 - dplyr::filter(test.vector, cutof == 1)$prob.cutof)*
        nrow(dplyr::filter(test.vector, cutof == 1))/nrow(test.vector)^2/
        mean(1 - test.vector$prob.cutof),
      
      sum(1 - dplyr::filter(test.vector, radan == 1)$prob.radan)*
        nrow(dplyr::filter(test.vector, radan == 1))/nrow(test.vector)^2/
        mean(1 - test.vector$prob.radan))

  pos.err = 
    c(sum(dplyr::filter(test.vector, delta == 0)$prob.delta)*
        nrow(dplyr::filter(test.vector, delta == 0))/nrow(test.vector)^2/
        mean(test.vector$prob.delta),
      
      sum(dplyr::filter(test.vector, cutof == 0)$prob.cutof)*
        nrow(dplyr::filter(test.vector, cutof == 0))/nrow(test.vector)^2/
        mean(test.vector$prob.cutof),
      
      sum(dplyr::filter(test.vector, radan == 0)$prob.radan)*
        nrow(dplyr::filter(test.vector, radan == 0))/nrow(test.vector)^2/
        mean(test.vector$prob.radan))
  
  return(data.frame(prob = c(tot.err, neg.err, pos.err), 
            class = c('Pareto Distance', 'Threshold Cutoff', 'Utopia Distance'),
            typ = c(rep('Total', 3), rep('False Negative', 3), 
                    rep('False Positive', 3))))
}

mod.err.nSamp = function(data.dist, data.f, data.rad, data.rando, 
                         nParet, nTest, test.vector){
  # Calculate adaptively sampled and not-adaptively sampled conditions, then
  # compare them to obtain the percentile
  # Adaptive samples
  delta = data.delta[1:(nParet + nTest),]
  cutof = data.cutof[1:(nParet + nTest),]
  radan = data.radan[1:(nParet + nTest),]
  err = mod.err.n(data.dist = delta, data.f = cutof, data.rad = radan, 
                  test.vector = test.vector)
  names(err)[1] = 'adapt'

  # Random samples
  res.rando = data.frame()
  # Only need a smaller test set for the random subset because there will be so many
  rando.subset = test.vector[sample(1:nrow(test.vector), 
                              size = round(nrow(test.vector)/10)),]
  for(j in 1:1000){
    # Collect multiple random datasets to get a sense of the variance
    rando = data.rando[c(1:nParet, 
                         sample(x = (nParet+1):nrow(data.rando), size = nTest)),]
    res = mod.err.n(data.dist = rando, data.f = rando, data.rad = rando, 
                    test.vector = rando.subset)
    res.rando = rbind(res.rando, res)
  }
  
  # Aggregated information: median, 95% CI window, and percentile
  err$unc.min = NaN;  err$unc.med = NaN
  err$unc.max = NaN;  err$ptl = NaN
  # Uncertainty correction based on the variance - in this case want the 95% CI
  for(i in 1:nrow(err)){
    cls = err$class[i]
    tp = err$typ[i]
    sub = dplyr::filter(res.rando, class == cls, typ == tp)
    err$unc.min[i] = unname(quantile(sub$prob, 0.025))
    err$unc.max[i] = unname(quantile(sub$prob, 0.975))
    err$unc.med[i] = median(sub$prob)
    err$ptl[i] = nrow(dplyr::filter(sub, prob > err$adapt[i])) / nrow(sub)
  }
  # Further labels
  err$n = nTest
  return(err)
}

```

```{r GP: Sample Size Effects on Error - Calculations}
# Monte Carlo sampling - skip if the calculations were previously performed
if(!file.exists('ErrorRates-Nsamp.csv')){
  test.vec = fine.grid[, names(fine.grid) %in% c('x1', 'x2', 'delta', 'cutof', 'radan')]
  
  # Using the full random dataset to get its variance
  data.rando = read.csv(file = 'GPar_Random.csv')
  # Number of samples collected adaptively
  nSet = nrow(data.delta) - nrow(data.paret)
  
  # Run in parallel
  n.cores <- parallel::detectCores() - 1
  my.cluster <- parallel::makeCluster(
    n.cores, 
    type = "PSOCK"
    )
  doParallel::registerDoParallel(cl = my.cluster)
  # foreach::getDoParRegistered()
  
  err.n = foreach(i = seq(from = 4, to = nSet, by = 4), .combine = 'rbind') %dopar% {
    mod.err.nSamp(data.dist = data.delta, data.f = data.cutof, data.rad = data.radan, 
                  data.rando = data.rando, nParet = nrow(data.paret), nTest = i,
                  test.vector = test.vec)
  }
  parallel::stopCluster(cl = my.cluster)
  
  # Zero sample state
  err0 = mod.err.n(data.dist = data.paret, data.f = data.paret, data.rad = data.paret, 
                  test.vector = test.vec)
  err0$n = 0
  names(err0)[1] = 'adapt'
  err0$unc.min = err0$adapt; err0$unc.med = err0$adapt; 
  err0$unc.max = err0$adapt; err0$ptl = NaN
  err.n = rbind(err0, err.n)
  
  # Normalized to the starting point for visualization
  update = c('class', 'typ', 'ptl', 'n')
  err.norm = err.n[, !names(err.n) %in% update]
  err.norm = err.norm / err0$adapt
  names(err.norm) = lapply(names(err.norm), FUN = function(x){paste(x, '.n', sep = "")})
  rm(update)
  err.n = cbind(err.n, err.norm)
  
  # err.n
  write.csv(err.n, 'ErrorRates-Nsamp.csv', row.names = F)
}

```

```{r GP: Sample Size Effects on Error - Plotting}
err.n = read.csv('ErrorRates-Nsamp.csv')
# Absolute errors
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min, rev(err.n$unc.max)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Error Rate', color = NULL)

# Normalized errors
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL)

# Percentiles
ggplot() +
  geom_line(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  geom_point(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  facet_grid(typ~class, scale = 'free') +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Adaptive Sampling Percentile', color = NULL)

```

Generally, finding that the adaptive sample procedure is in the 90th+ percentile of random sample configurations.

# GP-based Importance of Variables: Sobol Method

Compares the contribution to the variance

Using the total-effect index as an aggregate metric that combines all first and higher-order contributions to the variance.

Monte Carlo method based on:

I.M Sobol, "Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates,"
Mathematics and Computers in Simulation, Vol 55, 2001, pp. 271-280, doi:10.1016/S0378-4754(00)00270-6.

```{r GP: Importance by Sobol Method}
# Sobol method - Total effect index
sobol.GP = function(nsamp, mod.dist, mod.f1, mod.f2, mod.rad, mod.ang){
  # Set up the random points - 2 sets
  tot.rand = nsamp*50 # Repeat 50 times to get the standard error like the other importance metrics

  # tot.rand = nsamp
  xA = data.frame(x1 = runif(n = tot.rand, min = 0, max = 5),
                  x2 = runif(n = tot.rand, min = 0, max = 5))
  xB = data.frame(x1 = runif(n = tot.rand, min = 0, max = 5),
                  x2 = runif(n = tot.rand, min = 0, max = 5))
  # Single variable exchanges
  z1 = data.frame(x1 = xA$x1, x2 = xB$x2)
  z2 = data.frame(x1 = xB$x1, x2 = xA$x2)

  # Calculate the probabilities
  # Pareto distance
  res = predict(object = mod.dist, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  
  res = predict(object = mod.dist, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  
  res = predict(object = mod.dist, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp))
  delta = data.frame(var = c('x1', 'x2'), typ = 'Pareto Distance',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol),
                     r.sd = sobol.sd/max(sobol))

  # Threshold Cutoff
  res1 = predict(object = mod.f1, newdata = xA, type = 'UK')
  res2 = predict(object = mod.f2, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  
  res1 = predict(object = mod.f1, newdata = z1, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  
  res1 = predict(object = mod.f1, newdata = z2, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp))
  cutof = data.frame(var = c('x1', 'x2'), typ = 'Threshold Cutoff',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol),
                     r.sd = sobol.sd/max(sobol))

  # Utopia distance + Priority
  res1 = predict(object = mod.rad, newdata = xA, type = 'UK')
  res2 = predict(object = mod.ang, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  res1 = predict(object = mod.rad, newdata = z1, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))

  res1 = predict(object = mod.rad, newdata = z2, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp))
  radan = data.frame(var = c('x1', 'x2'), typ = 'Utopia Distance',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol),
                     r.sd = sobol.sd/max(sobol))
  
  import = rbind(delta, cutof, radan)
  import$method = 'Sobol-GP'
  return(import)
}

# if(!file.exists('Importance-Sobol-GP.csv')){
  sobol.import = sobol.GP(nsamp = 1500, mod.dist = mod.dist.adapt, 
           mod.f1 = mod.f1.adapt, mod.f2 = mod.f2.adapt, 
           mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
  write.csv(sobol.import, 'Importance-Sobol-GP.csv', row.names = F)
# }

sobol.import = read.csv('Importance-Sobol-GP.csv')
sobol.import
ggplot(sobol.import) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymin = import - sd, ymax = import + sd)) +
  facet_grid(~typ)
ggplot(sobol.import) +
  geom_col(mapping = aes(x = var, y = r.import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymin = r.import - r.sd, ymax = r.import + r.sd)) +
  facet_grid(~typ)

```


# Existing Method: Support Vector Machines 

SVM methods require existing labels for the points; in this case, the labels are whether or not the point meets the same three selection criteria as tested with the new GP method.

While all 4 kernel types available in the e1071 package are calculated, the linear and sigmoidal kernels consistently gave poor results, and are excluded from the accuracy calculation.

```{r SVM: Functions}
# Create a function to output the plot of all 4 models compared to the real result
SVM.input = function(fine.input, mod.rad, mod.lin, mod.pol, mod.sig, tit){
  # Fine input has the variables x1, x2, and cat
  res = predict(object = mod.lin, newdata = fine.input[, c('x1', 'x2')])
  fine.input$res = res
  g.lin = ggplot(fine.input) +
    geom_point(data = fine.input, mapping = aes(x = x1, y = x2, color = res)) +
    geom_contour(data = fine.input, mapping = aes(x = x1, y = x2, z = cat), breaks = c(0.5)) +
    labs(subtitle = 'Linear', x = expression('x'[1]), y = expression('x'[2])) + 
    guides(color = 'none')
  res = predict(object = mod.rad, newdata = fine.input[,c('x1', 'x2')])
  fine.input$res = res
  g.rad = ggplot() +
    geom_point(data = fine.input, mapping = aes(x = x1, y = x2, color = res)) +
    geom_contour(data = fine.input, mapping = aes(x = x1, y = x2, z = cat), breaks = c(0.5)) +
    labs(subtitle = 'Radial', x = expression('x'[1]), y = expression('x'[2]), title = tit) + guides(color = 'none')
  res = predict(object = mod.pol, newdata = fine.input[,c('x1', 'x2')])
  fine.input$res = res
  g.pol = ggplot() +
    geom_point(data = fine.input, mapping = aes(x = x1, y = x2, color = res)) +
    geom_contour(data = fine.input, mapping = aes(x = x1, y = x2, z = cat), breaks = c(0.5)) +
    labs(subtitle = 'Polynomial', x = expression('x'[1]), y = expression('x'[2])) + guides(color = 'none')
  res = predict(object = mod.sig, newdata = fine.input[,c('x1', 'x2')])
  fine.input$res = res
  g.sig = ggplot() +
    geom_point(data = fine.input, mapping = aes(x = x1, y = x2, color = res)) +
    geom_contour(data = fine.input, mapping = aes(x = x1, y = x2, z = cat), breaks = c(0.5)) +
    labs(subtitle = 'Sigmoid', x = expression('x'[1]), y = expression('x'[2])) + guides(color = 'none')
  return((g.rad + g.lin) / (g.pol + g.sig))
}

SVM.mod.all = function(input.data){
  # Given input data, find all 4 default models and return as a list
  fit.rad = e1071::svm(as.factor(cat) ~ x1*x2, data = input.data[,c('x1', 'x2', 'cat')], 
            scale = FALSE, kernel = "radial", cost = 5)
  fit.lin = e1071::svm(as.factor(cat) ~ x1*x2, data = input.data[,c('x1', 'x2', 'cat')], 
            scale = FALSE, kernel = "linear", cost = 5)
  # Polynomial: increase the kernel degree due to complexity of the boundary; maximum tested that still achieved convergence
  fit.pol = e1071::svm(as.factor(cat) ~ x1*x2, data = input.data[,c('x1', 'x2', 'cat')], 
            scale = FALSE, kernel = "polynomial", cost = 5, degree = 3.5)
  # Increase the coefficient
  fit.sig = e1071::svm(as.factor(cat) ~ x1*x2, data = input.data[,c('x1', 'x2', 'cat')], 
            scale = FALSE, kernel = "sigmoid", cost = 5)
  return(list(rad = fit.rad, lin = fit.lin, pol = fit.pol, sig = fit.sig))
}

# For the radial and polynomial kernels, calculate the marginals and Shapley values for comparison
marginal = function(mod){
  x.rng = seq(from = 0, to = 5, length.out = 50)
  svm.margin = data.frame()
  nsamp = 1500
  for(x in x.rng){
    # x1 marginal
    temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = 0, max = 5))
    res = as.numeric(predict(object = mod$pol, newdata = temp.frame))-1
    svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', prob = sum(res)/length(res), method = 'SVM-pol'))
    res = as.numeric(predict(object = mod$rad, newdata = temp.frame))-1
    svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x1', prob = sum(res)/length(res), method = 'SVM-rad'))
    # x1 marginal
    temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = 0, max = 5))
    res = as.numeric(predict(object = mod$pol, newdata = temp.frame))-1
    svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', prob = sum(res)/length(res), method = 'SVM-pol'))
    res = as.numeric(predict(object = mod$rad, newdata = temp.frame))-1
    svm.margin = rbind(svm.margin, data.frame(x = x, var = 'x2', prob = sum(res)/length(res), method = 'SVM-rad'))
  }
  
  svm.margin$psd = sqrt(svm.margin$prob*(1 - svm.margin$prob)/nsamp)
  return(svm.margin)
}

SVM.shap = function(mod){
  # Strumbelj et al. (2014) Monte Carlo estimate.
  # Since this is a classification setting, only a difference of 0 or 1 is possible 
  # i.e. a difference of -1 is the same as a difference of 1, as it is simply a misclassification
  nsamp = 1500*50 # Same number of samples as other methods for consistency
  x0 = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                  x2 = runif(n = nsamp, min = 0, max = 5))
  z1 = data.frame(x1 = x0$x1,
                  x2 = runif(n = nsamp, min = 0, max = 5))
  z2 = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                  x2 = x0$x2)
  # Apply functions to all
  res.pol = as.numeric(predict(object = mod$pol, newdata = x0))
  res.rad = as.numeric(predict(object = mod$rad, newdata = x0))
  x0$pol = res.pol; x0$rad = res.rad
  res.pol = as.numeric(predict(object = mod$pol, newdata = z1))
  res.rad = as.numeric(predict(object = mod$rad, newdata = z1))
  z1$pol = res.pol; z1$rad = res.rad
  res.pol = as.numeric(predict(object = mod$pol, newdata = z2))
  res.rad = as.numeric(predict(object = mod$rad, newdata = z2))
  z2$pol = res.pol; z2$rad = res.rad
  # Calculate mean and standard error of the differences for Shapley values
  shap = c(mean((x0$pol - z1$pol)), mean((x0$pol - z2$pol)),
           mean((x0$rad - z1$rad)), mean((x0$rad - z2$rad)))
  r.shap = c(shap[1:2]/max(shap[1:2]), shap[3:4]/max(shap[3:4]))
  # For standard error, calculate standard error of the mean with n = 1500
  # to be consistent with the other standard errors
  pol.x1m = apply(matrix(x0$pol - z1$pol, nrow = 1500), 2, mean)
  rad.x1m = apply(matrix(x0$rad - z1$rad, nrow = 1500), 2, mean)
  pol.x2m = apply(matrix(x0$pol - z2$pol, nrow = 1500), 2, mean)
  rad.x2m = apply(matrix(x0$rad - z2$rad, nrow = 1500), 2, mean)
  
  shap.err = c(sd(pol.x1m), sd(pol.x2m),
               sd(rad.x1m), sd(rad.x2m))
  return(data.frame(import = shap, var = c('x1', 'x2'), 
                    sd = shap.err, r.import = r.shap, 
                    method = c('Shapley-pol', 'Shapley-pol', 
                               'Shapley-rad', 'Shapley-rad')))
}

```

Calculate errors with sample size considerations, compared to the range of random selections

```{r SVM: Sample Size Effects on Error - Functions}
SVM.err = function(fine.input, mod.list){
  # Fine grid has the real result; mod.list is the 4 different SVM kernels
  fine.input$rad = predict(object = mod.list$rad, newdata = fine.input[,c('x1', 'x2')])
  fine.input$lin = predict(object = mod.list$lin, newdata = fine.input[,c('x1', 'x2')])
  fine.input$pol = predict(object = mod.list$pol, newdata = fine.input[,c('x1', 'x2')])
  # fine.input$sig = predict(object = mod.list$sig, newdata = fine.input[,c('x1', 'x2')])
  
  # Error rate
  rad = nrow(dplyr::filter(fine.input, cat == 0, rad == 1)) + 
    nrow(dplyr::filter(fine.input, cat == 1, rad == 0))
  lin = nrow(dplyr::filter(fine.input, cat == 0, lin == 1)) + 
    nrow(dplyr::filter(fine.input, cat == 1, lin == 0))
  pol = nrow(dplyr::filter(fine.input, cat == 0, pol == 1)) + 
    nrow(dplyr::filter(fine.input, cat == 1, pol == 0))
  # sig = nrow(filter(fine.input, cat == 0, sig == 1)) + nrow(filter(fine.input, cat == 1, sig == 0))
  # rate = c(rad, lin, pol, sig)/nrow(fine.input)
  rate = c(rad, lin, pol)/nrow(fine.input)

  # Type I error = P[should reject | accepted] = P[both] P[should reject] / P[accepted]
  typ1 =
    c(nrow(dplyr::filter(fine.input, cat == 0, rad == 1))*
        nrow(dplyr::filter(fine.input, cat == 0))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, rad == 1)),
      nrow(dplyr::filter(fine.input, cat == 0, lin == 1))*
        nrow(dplyr::filter(fine.input, cat == 0))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, lin == 1)),
      nrow(dplyr::filter(fine.input, cat == 0, pol == 1))*
        nrow(dplyr::filter(fine.input, cat == 0))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, pol == 1)))#,
      # nrow(filter(fine.input, cat == 0, sig == 1))*nrow(filter(fine.input, cat == 0))/nrow(fine.input)/
        # nrow(filter(fine.input, sig == 1)) )

  # Type II error = P[should accept | rejected] = P[both] P[should accept] / P[rejected]
  typ2 =
    c(nrow(dplyr::filter(fine.input, cat == 1, rad == 0))*
        nrow(dplyr::filter(fine.input, cat == 1))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, rad == 0)),
      nrow(dplyr::filter(fine.input, cat == 1, lin == 0))*
        nrow(dplyr::filter(fine.input, cat == 1))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, lin == 0)),
      nrow(dplyr::filter(fine.input, cat == 1, pol == 0))*
        nrow(dplyr::filter(fine.input, cat == 1))/nrow(fine.input)/
        nrow(dplyr::filter(fine.input, pol == 0)))#,
      # nrow(filter(fine.input, cat == 1, sig == 0))*nrow(filter(fine.input, cat == 1))/nrow(fine.input)/
        # nrow(filter(fine.input, sig == 0)) )
  # return(data.frame(method = c('SVM-rad', 'SVM-lin', 'SVM-pol', 'SVM-sig'), 
  #   rate, err, typ1, typ1.err, typ2, typ2.err))
  return(data.frame(method = c('SVM-rad', 'SVM-lin', 'SVM-pol'), 
                    rate, typ1, typ2))
}

SVM.err.n = function(fine.input, input.data){
  mod.list = SVM.mod.all(input.data = input.data)
  err = SVM.err(fine.input = fine.input, mod.list = mod.list)
  
  # Additional processing to make it consistent with the other convention
  err.n = data.frame(
    prob = c(err$rate, err$typ1,     err$typ2),
    method = err$method,
    typ = c(rep('Total', nrow(err)), rep('False Positive', nrow(err)), 
            rep('False Negative', nrow(err)))
  )
  # Lower limit of errors - for later to avoid divide by 0 issues
  err.n$prob[err.n$prob < 1e-8] = 1e-8
  return(err.n)
}

SVM.err.nSamp = function(data.dist, data.f, data.rad, data.rando, 
                         nParet, nTest, test.vector){
  # Calculate adaptively sampled and not-adaptively sampled conditions, then
  # compare them to obtain the percentile
  
  # Data processing for classification problem
  delta = data.delta[1:(nParet + nTest),]
  cutof = data.cutof[1:(nParet + nTest),]
  radan = data.radan[1:(nParet + nTest),]
  
  # Classification metric
  delta$cat = 0; cutof$cat = 0; radan$cat = 0
  delta$cat[delta$dist <= 1] = 1
  cutof$cat[cutof$f1.norm <= 1 & cutof$f2.norm <= 1] = 1
  radan$cat[sqrt(radan$f1.norm^2 + radan$f2.norm^2) <= 1 & radan$theta > 20] = 1
  # Test vector and random sample get re-used
  test.vector$cat = 0
  test.vector$cat.delta = 0; test.vector$cat.cutof = 0; test.vector$cat.radan = 0
  test.vector$cat.delta[test.vector$dist <= 1] = 1
  test.vector$cat.cutof[test.vector$f1.norm <= 1 & test.vector$f2.norm <= 1] = 1
  test.vector$cat.radan[sqrt(test.vector$f1.norm^2 + test.vector$f2.norm^2) <= 1 & 
                    test.vector$theta > 20] = 1
  data.rando$cat = 0; 
  data.rando$cat.delta = 0; data.rando$cat.cutof = 0; data.rando$cat.radan = 0
  data.rando$cat.delta[data.rando$dist <= 1] = 1
  data.rando$cat.cutof[data.rando$f1.norm <= 1 & data.rando$f2.norm <= 1] = 1
  data.rando$cat.radan[sqrt(data.rando$f1.norm^2 + data.rando$f2.norm^2) <= 1 & 
                    data.rando$theta > 20] = 1
  
  # Calculate errors - adaptive sampling
  test.vector$cat = test.vector$cat.delta
  err.delta = SVM.err.n(fine.input = test.vector, input.data = delta)
  err.delta$class = 'Pareto Distance'

  test.vector$cat = test.vector$cat.cutof
  err.cutof = SVM.err.n(fine.input = test.vector, input.data = cutof)
  err.cutof$class = 'Threshold Cutoff'
  
  test.vector$cat = test.vector$cat.radan
  err.radan = SVM.err.n(fine.input = test.vector, input.data = radan)
  err.radan$class = 'Utopia Distance'
  err = rbind(err.delta, err.cutof, err.radan)
  # Make it clear the probability is the adaptive sampling probability
  names(err)[1] = 'adapt' 
  
  # Random samples
  res.rando = data.frame()
  for(j in 1:5){ #### Update for MC
    # Collect multiple random datasets to get a sense of the variance
    rando = data.rando[c(1:nParet, 
                         sample(x = (nParet+1):nrow(data.rando), size = nTest)),]
    
    rando$cat = rando$cat.delta; test.vector$cat = test.vector$cat.delta
    err.delta = SVM.err.n(fine.input = test.vector, input.data = rando)
    err.delta$class = 'Pareto Distance'
    
    rando$cat = rando$cat.cutof; test.vector$cat = test.vector$cat.cutof
    err.cutof = SVM.err.n(fine.input = test.vector, input.data = rando)
    err.cutof$class = 'Threshold Cutoff'
    
    rando$cat = rando$cat.radan; test.vector$cat = test.vector$cat.radan
    err.radan = SVM.err.n(fine.input = test.vector, input.data = rando)
    err.radan$class = 'Utopia Distance'
    
    res.rando = rbind(res.rando, err.delta, err.cutof, err.radan)
  }
  
  # Aggregated information: median, 95% CI window, and percentile
  err$unc.min = NaN;  err$unc.med = NaN
  err$unc.max = NaN;  err$ptl = NaN
  # Uncertainty correction based on the variance - in this case want the 95% CI
  for(i in 1:nrow(err)){
    cls = err$class[i]
    tp = err$typ[i]
    mod = err$method[i]
    sub = dplyr::filter(res.rando, class == cls, typ == tp)
    err$unc.min[i] = unname(quantile(sub$prob, 0.025))
    err$unc.max[i] = unname(quantile(sub$prob, 0.975))
    err$unc.med[i] = median(sub$prob)
    err$ptl[i] = nrow(dplyr::filter(sub, prob > err$adapt[i])) / nrow(sub)
  }
  # Further labels
  err$n = nTest
  return(err)
}


```

```{r SVM: Sample Size Effects on Error - Calculations}
if(file.exists('SVM_Error-Nsamp.csv') == FALSE){
  # Loading data
  data.delta = read.csv('../Ex_Quartic/GPar_Accept_Delta1.csv')
  data.cutof = read.csv('../Ex_Quartic/GPar_Accept_Threshold.csv')
  data.radan = read.csv('../Ex_Quartic/GPar_Accept_Radius.csv')
  data.paret = read.csv('../Ex_Quartic/GPar_all_start.csv')
  
  # Using the full random dataset to get its variance
  data.rando = read.csv(file = 'GPar_Random.csv')
  # Number of samples collected adaptively
  nSet = nrow(data.delta) - nrow(data.paret)
  
  # Test "samples"
  test.vec = fine.grid[, names(fine.grid) %in% c('x1', 'x2', 'delta', 'cutof', 'radan')]
  
  # Effect of sampling
  n.cores <- parallel::detectCores() - 1
  my.cluster <- parallel::makeCluster(
    n.cores,
    type = "PSOCK"
    )
  doParallel::registerDoParallel(cl = my.cluster)
  # foreach::getDoParRegistered()
  
  err.n = foreach(i = seq(from = 4, to = nSet, by = 4), .combine = 'rbind') %dopar% {
    SVM.err.nSamp(data.dist = data.delta, data.f = data.cutof, data.rad = data.radan, 
                  data.rando = data.rando, nParet = nrow(data.paret), nTest = i, 
                  test.vector = test.vec)
  }
  parallel::stopCluster(cl = my.cluster)
  
  # Reference point is the no sample case
  err0 = SVM.err.nSamp(data.dist = data.delta, data.f = data.cutof, data.rad = data.radan, 
                data.rando = data.rando, nParet = nrow(data.paret), nTest = 0, 
                test.vector = fine.grid)
  err0$unc.min = err0$adapt; err0$unc.med = err0$adapt
  err0$unc.max = err0$adapt; err0$ptl = NaN
  
  err.n = rbind(err0, err.n)
  
  # Normalized to the starting point for visualization
  update = c('class', 'typ', 'ptl', 'n', 'method')
  err.norm = err.n[, !names(err.n) %in% update]
  err.norm = err.norm / err0$adapt
  names(err.norm) = lapply(names(err.norm), FUN = function(x){paste(x, '.n', sep = "")})
  rm(update)
  err.n = cbind(err.n, err.norm)
  err.n
  write.csv(err.n, 'SVM_Error-Nsamp.csv', row.names = F)
}
```

```{r SVM: Sample Size Effects on Error - Plotting}
err.all = read.csv('SVM_Error-Nsamp.csv')

# Normalized errors for each kernel function
err.n = filter(err.all, method == unique(method)[1])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$method))

err.n = filter(err.all, method == method[2])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$method))

err.n = filter(err.all, method == method[3])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$method))

# Percentile Ranking
err.n = filter(err.all, typ == 'Total')
ggplot() +
  geom_line(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  geom_point(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  facet_grid(method~class, scale = 'free') +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Adaptive Sampling Percentile', color = NULL)

```

```{r SVM: Distance}
data.paret = read.csv('../Ex_Quartic/GPar_all_start.csv')
data.delta = read.csv('../Ex_Quartic/GPar_Accept_Delta1.csv')
data.rando = read.csv(file = 'GPar_Random.csv')
data.rando = data.rando[1:nrow(data.paret), ]

# Define acceptance
data.paret$cat = 0; data.delta$cat = 0; data.rando$cat = 0; fine.grid$cat = 0
data.paret$cat[data.paret$dist <= 1] = 1
data.delta$cat[data.delta$dist <= 1] = 1
data.rando$cat[data.rando$dist <= 1] = 1
fine.grid$cat[fine.grid$dist <= 1] = 1

# Plots
mod.paret = SVM.mod.all(input.data = data.paret)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.paret$rad, mod.lin = mod.paret$lin, 
          mod.pol = mod.paret$pol, mod.sig = mod.paret$sig,
          tit = 'Pareto Distance, Starting Dataset')

mod.adapt = SVM.mod.all(input.data = data.delta)
# Store this model for importance ranking later
mod.adapt.delta = mod.adapt
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.adapt$rad, mod.lin = mod.adapt$lin, 
          mod.pol = mod.adapt$pol, mod.sig = mod.adapt$sig,
          tit = 'Pareto Distance, + Adaptive Sampling')

mod.rando = SVM.mod.all(input.data = data.rando)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.rando$rad, mod.lin = mod.rando$lin, 
          mod.pol = mod.rando$pol, mod.sig = mod.rando$sig,
          tit = 'Pareto Distance, + Random Sampling')

```

Graphically, the selection of the kernel form has a large impact on the quality of the result. 
The polynomial and radial kernels appear to roughly match the shape of the normalized distance = 1 criterion; 
the linear and sigmoidal kernels do not appear to match anything. 
Further tuning of the polynomial degree or coefficient could improve the results, but given the constrained computational budget for sampling, separation into a training and testing set would likely lead to worse results.


```{r SVM: Distance marginals}
svm.margin.paret = marginal(mod = mod.paret); svm.margin.paret$cat = 'start'
svm.margin.adapt = marginal(mod = mod.adapt); svm.margin.adapt$cat = 'adapt'
svm.margin.rando = marginal(mod = mod.rando); svm.margin.rando$cat = 'rando'
svm.margin = rbind(svm.margin.paret, svm.margin.adapt, svm.margin.rando)
rm(svm.margin.paret, svm.margin.adapt, svm.margin.rando)

write.csv(svm.margin, 'Margin_SVM_dist.csv', row.names = F)

Infer.plt = read.csv('../Ex_Quartic/Marginals_delta.csv')
Infer.plt = Infer.plt[,!names(Infer.plt) %in% c('X')]
# names(Infer.plt)
Infer.plt$method = 'GP'

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = svm.margin, mapping = aes(x = x, y = prob, linetype = method, color = cat)) +
  facet_wrap(~var, nrow = 2) + theme_bw() +
  labs(x = 'Input Value', y = 'Conditional Probability', linetype = 'SVM Kernel', color = '') +
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  scale_linetype_discrete(labels = c('SVM-pol' = 'Polynomial', 'rad' = 'SVM-Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Pareto Distance'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Calculate coefficients of determination: easier comparison
coefdet = data.frame(method = c(rep('SVM-pol', 3), rep('SVM-rad', 3), rep('GP', 3)),
           dataset = c('0start', '1adapt', '2rando'),
 coef = c(cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'start')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'adapt')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'rando')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2))

coefdet
write.csv(coefdet, 'Marginals_CoefDet_delta.csv', row.names = F)

ggplot(coefdet) +
  geom_col(mapping = aes(x = dataset, fill = dataset, y = coef))+
  facet_grid(~method) +
  labs(x = '', y = '1-Variable Marginal Coefficient of Determination', subtitle = 'Pareto Distance') +
  scale_x_discrete(breaks = c()) +
  scale_fill_manual(labels = c('0start' = 'Starting Dataset', '1adapt' = '+ Adaptive Sampling', 
                                 '2rando' = '+ Random Sampling'), name = '',
                    values = c('0start' = 'skyblue2', '1adapt' = 'red', '2rando' = 'green')) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))

```

Single variable marginals are visually similar to the expected value, but calculating the coefficient of determination shows that the GP method is more consistent regardless of dataset and it demonstrates the expected improvement with increased sampling near the boundary.
The SVM methods overall cannot capture the boundary very well, leading to classification errors.

```{r SVM: Threshold}
data.paret = read.csv('../Ex_Quartic/GPar_all_start.csv')
data.cutof = read.csv('../Ex_Quartic/GPar_Accept_Threshold.csv')
data.rando = read.csv(file = 'GPar_Random.csv')
data.rando = data.rando[1:nrow(data.paret), ]

# Define acceptance
data.paret$cat = 0; data.cutof$cat = 0; data.rando$cat = 0; fine.grid$cat = 0
data.paret$cat[data.paret$f1.norm <= 1 & data.paret$f2.norm <= 1] = 1
data.cutof$cat[data.cutof$f1.norm <= 1 & data.cutof$f2.norm <= 1] = 1
data.rando$cat[data.rando$f1.norm <= 1 & data.rando$f2.norm <= 1] = 1
fine.grid$cat[fine.grid$f1.norm <= 1 & fine.grid$f2.norm <= 1] = 1

# Plots
mod.paret = SVM.mod.all(input.data = data.paret)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.paret$rad, mod.lin = mod.paret$lin, 
          mod.pol = mod.paret$pol, mod.sig = mod.paret$sig,
          tit = 'Threshold Cutoff, Starting Dataset')

mod.adapt = SVM.mod.all(input.data = data.cutof)
mod.adapt.cutof = mod.adapt
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.adapt$rad, mod.lin = mod.adapt$lin, 
          mod.pol = mod.adapt$pol, mod.sig = mod.adapt$sig,
          tit = 'Threshold Cutoff, + Adaptive Sampling')

mod.rando = SVM.mod.all(input.data = data.rando)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.rando$rad, mod.lin = mod.rando$lin, 
          mod.pol = mod.rando$pol, mod.sig = mod.rando$sig,
          tit = 'Threshold Cutoff, + Random Sampling')

```

```{r SVM Marginalization: Threshold Cutoff}
svm.margin.paret = marginal(mod = mod.paret); svm.margin.paret$cat = 'start'
svm.margin.adapt = marginal(mod = mod.adapt); svm.margin.adapt$cat = 'adapt'
svm.margin.rando = marginal(mod = mod.rando); svm.margin.rando$cat = 'rando'
svm.margin = rbind(svm.margin.paret, svm.margin.adapt, svm.margin.rando)
rm(svm.margin.paret, svm.margin.adapt, svm.margin.rando)

write.csv(svm.margin, 'Margin_SVM_cutof.csv', row.names = F)

Infer.plt = read.csv('../Ex_Quartic/Marginals_cutof.csv')
Infer.plt = Infer.plt[,!names(Infer.plt) %in% c('X')]
# names(Infer.plt)
Infer.plt$method = 'GP'

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = svm.margin, mapping = aes(x = x, y = prob, linetype = method, color = cat)) +
  facet_wrap(~var, nrow = 2) + theme_bw() +
  labs(x = 'Input Value', y = 'Conditional Probability', linetype = 'SVM Kernel', color = '', subtitle = 'Cutoff Threshold') +
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  scale_linetype_discrete(labels = c('SVM-pol' = 'Polynomial', 'rad' = 'SVM-Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', subtitle = 'Cutoff Threshold', color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Calculate coefficients of determination: easier comparison
coefdet = data.frame(method = c(rep('SVM-pol', 3), rep('SVM-rad', 3), rep('GP', 3)),
           dataset = c('0start', '1adapt', '2rando'),
 coef = c(cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'start')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'adapt')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'rando')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2))

coefdet
write.csv(coefdet, 'Marginals_CoefDet_cutof.csv', row.names = F)

ggplot(coefdet) +
  geom_col(mapping = aes(x = dataset, fill = dataset, y = coef))+
  facet_grid(~method) +
  labs(x = '', y = '1-Variable Marginal Coefficient of Determination', subtitle = 'Cutoff Threshold') +
  scale_x_discrete(breaks = c()) +
  scale_fill_manual(labels = c('0start' = 'Starting Dataset', '1adapt' = '+ Adaptive Sampling', 
                                 '2rando' = '+ Random Sampling'), name = '',
                    values = c('0start' = 'skyblue2', '1adapt' = 'red', '2rando' = 'green')) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))

```

```{r SVM: Radius and Priority}
data.paret = read.csv(file = '../Ex_Quartic/GPar_all_start.csv')
data.radan = read.csv('../Ex_Quartic/GPar_Accept_Radius.csv')
data.rando = read.csv(file = 'GPar_Random.csv')
data.rando = data.rando[1:nrow(data.paret), ]

# Define acceptance
data.paret$cat = 0; data.radan$cat = 0; data.rando$cat = 0; fine.grid$cat = 0
data.paret$cat[sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2) <= 1 & data.paret$theta > 20] = 1
data.radan$cat[sqrt(data.radan$f1.norm^2 + data.radan$f2.norm^2) <= 1 & data.radan$theta > 20] = 1
data.rando$cat[sqrt(data.rando$f1.norm^2 + data.rando$f2.norm^2) <= 1 & data.rando$theta > 20] = 1
fine.grid$cat[sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2) <= 1 & fine.grid$ang > 20] = 1

# Plots
mod.paret = SVM.mod.all(input.data = data.paret)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.paret$rad, mod.lin = mod.paret$lin, 
          mod.pol = mod.paret$pol, mod.sig = mod.paret$sig,
          tit = 'Utopia Distance + Priority, Starting Dataset')

mod.adapt = SVM.mod.all(input.data = data.radan)
mod.adapt.radan = mod.adapt
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.adapt$rad, mod.lin = mod.adapt$lin, 
          mod.pol = mod.adapt$pol, mod.sig = mod.adapt$sig,
          tit = 'Utopia Distance + Priority, + Adaptive Sampling')

mod.rando = SVM.mod.all(input.data = data.rando)
SVM.input(fine.input = fine.grid[,c('x1', 'x2', 'cat')], 
          mod.rad = mod.rando$rad, mod.lin = mod.rando$lin, 
          mod.pol = mod.rando$pol, mod.sig = mod.rando$sig,
          tit = 'Utopia Distance + Priority, + Random Sampling')

```

```{r SVM Marginals: Utopia Distance}
svm.margin.paret = marginal(mod = mod.paret); svm.margin.paret$cat = 'start'
svm.margin.adapt = marginal(mod = mod.adapt); svm.margin.adapt$cat = 'adapt'
svm.margin.rando = marginal(mod = mod.rando); svm.margin.rando$cat = 'rando'
svm.margin = rbind(svm.margin.paret, svm.margin.adapt, svm.margin.rando)
rm(svm.margin.paret, svm.margin.adapt, svm.margin.rando)

write.csv(svm.margin, 'Margin_SVM_radan.csv', row.names = F)

Infer.plt = read.csv('../Ex_Quartic/Marginals_radan.csv')
Infer.plt = Infer.plt[,!names(Infer.plt) %in% c('X')]
# names(Infer.plt)
Infer.plt$method = 'GP'

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = svm.margin, mapping = aes(x = x, y = prob, linetype = method, color = cat)) +
  facet_wrap(~var, nrow = 2) + theme_bw() +
  labs(x = 'Input Value', y = 'Conditional Probability', linetype = 'SVM Kernel', color = '', 
       subtitle = 'Utopia Distance + Priority') +
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  scale_linetype_discrete(labels = c('SVM-pol' = 'Polynomial', 'rad' = 'SVM-Radial')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(limits = c(0, 1))

ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 3) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 2) + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
                                  'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
                                  'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(3, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', subtitle = 'Cutoff Threshold', color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# Calculate coefficients of determination: easier comparison
coefdet = data.frame(method = c(rep('SVM-pol', 3), rep('SVM-rad', 3), rep('GP', 3)),
           dataset = c('0start', '1adapt', '2rando'),
 coef = c(cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-pol', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'start')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'adapt')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(svm.margin, method == 'SVM-rad', cat == 'rando')$prob, 
    y = filter(Infer.plt, cat == 'tru')$prob, method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'start')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'adapt')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2,
  cor(x = filter(Infer.plt, cat == 'rando')$prob, 
    y = c(filter(Infer.plt, cat == 'tru', var == 'x1')$prob, filter(Infer.plt, cat == 'tru', var == 'x2')$prob), 
    method = 'pearson')^2))

coefdet
write.csv(coefdet, 'Marginals_CoefDet_radan.csv', row.names = F)

ggplot(coefdet) +
  geom_col(mapping = aes(x = dataset, fill = dataset, y = coef))+
  facet_grid(~method) +
  labs(x = '', y = '1-Variable Marginal Coefficient of Determination', subtitle = 'Utopia Distance + Priority') +
  scale_x_discrete(breaks = c()) +
  scale_fill_manual(labels = c('0start' = 'Starting Dataset', '1adapt' = '+ Adaptive Sampling', 
                                 '2rando' = '+ Random Sampling'), name = '',
                    values = c('0start' = 'skyblue2', '1adapt' = 'red', '2rando' = 'green')) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))

```

Importance Rankings for SVM

```{r Importance Ranking Method Comparison: Calculating Marginal Importance for SVM}
import.marginal = function(svm.margin, typ){
  # Set up output
  import.svm.margin = data.frame()
  # Loop across methods
  for(met in unique(svm.margin$method)){
    sub = filter(svm.margin, cat == 'adapt', var == 'x1', method == met)
    import.svm.margin = rbind(import.svm.margin, data.frame(
      import = diff(range(sub$prob))/sum(sub$psd),
      var = 'x1',
      method = paste('Marginal', substring(met, 4), sep = ''),
      sd = sqrt((max(sub$prob)*(1-max(sub$prob)) + min(sub$prob)*(1-min(sub$prob)) * 
                    diff(range(sub$prob)) + var(sub$psd))/nrow(sub))
      ))
    sub = filter(svm.margin, cat == 'adapt', var == 'x2', method == met)
    import.svm.margin = rbind(import.svm.margin, data.frame(
      import = diff(range(sub$prob))/sum(sub$psd),
      var = 'x2',
      method = paste('Marginal', substring(met, 4), sep = ''),
      sd = sqrt((max(sub$prob)*(1-max(sub$prob)) + min(sub$prob)*(1-min(sub$prob)) * 
                    diff(range(sub$prob)) + var(sub$psd))/nrow(sub))
      ))
  }
  # Type, relative importance
  import.svm.margin$typ = typ
  import.svm.margin$r.import = c(import.svm.margin$import[1:2]/max(import.svm.margin$import[1:2]),
                                 import.svm.margin$import[3:4]/max(import.svm.margin$import[3:4]))
  return(import.svm.margin)
}

svm.import.margin = rbind(import.marginal(svm.margin = read.csv('Margin_SVM_dist.csv'), typ = 'Pareto Distance'), 
      import.marginal(svm.margin = read.csv('Margin_SVM_cutof.csv'), typ = 'Threshold Cutoff'), 
      import.marginal(svm.margin = read.csv('Margin_SVM_radan.csv'), typ = 'Utopia Distance'))
```


```{r SVM: Sobol Importance}
# Sobol method - Total effect index
SVM.sobol = function(mod){
  # Strumbelj et al. (2014) Monte Carlo estimate.
  # Since this is a classification setting, only a difference of 0 or 1 is possible 
  # i.e. a difference of -1 is the same as a difference of 1, as it is simply a misclassification
  nsamp = 1500*50 # Same number of samples as other methods for consistency
  x0 = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                  x2 = runif(n = nsamp, min = 0, max = 5))
  z1 = data.frame(x1 = x0$x1,
                  x2 = runif(n = nsamp, min = 0, max = 5))
  z2 = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                  x2 = x0$x2)
  # Apply functions to all
  res.pol = as.numeric(predict(object = mod$pol, newdata = x0))
  res.rad = as.numeric(predict(object = mod$rad, newdata = x0))
  x0$pol = res.pol; x0$rad = res.rad
  res.pol = as.numeric(predict(object = mod$pol, newdata = z1))
  res.rad = as.numeric(predict(object = mod$rad, newdata = z1))
  z1$pol = res.pol; z1$rad = res.rad
  res.pol = as.numeric(predict(object = mod$pol, newdata = z2))
  res.rad = as.numeric(predict(object = mod$rad, newdata = z2))
  z2$pol = res.pol; z2$rad = res.rad
  # Calculate mean and standard error of the differences
  shap = c(mean((x0$pol - z1$pol)^2/2), mean((x0$pol - z2$pol)^2/2),
           mean((x0$rad - z1$rad)^2/2), mean((x0$rad - z2$rad)^2/2))
  r.shap = c(shap[1:2]/max(shap[1:2]), shap[3:4]/max(shap[3:4]))
  # For standard error, calculate standard error of the mean with n = 1500
  # to be consistent with the other standard errors
  pol.x1m = apply(matrix((x0$pol - z1$pol)^2, nrow = 1500), 2, mean)
  rad.x1m = apply(matrix((x0$rad - z1$rad)^2, nrow = 1500), 2, mean)
  pol.x2m = apply(matrix((x0$pol - z2$pol)^2, nrow = 1500), 2, mean)
  rad.x2m = apply(matrix((x0$rad - z2$rad)^2, nrow = 1500), 2, mean)
  
  shap.err = c(sd(pol.x1m), sd(pol.x2m),
               sd(rad.x1m), sd(rad.x2m))
  # r.err = c(shap.err[1:2]/max(shap[1:2]), shap.err[3:4]/max(shap[3:4]))
  return(data.frame(import = shap, var = c('x1', 'x2'), 
                    sd = shap.err, r.import = r.shap, 
                    method = c('Sobol-pol', 'Sobol-pol', 
                               'Sobol-rad', 'Sobol-rad')))
}

```

```{r SVM: Importance Rankings}
import.rank = read.csv('../Ex_Quartic/Importance.csv')
import.rank = import.rank[, !names(import.rank) %in% c('X')]
import.rank$method = unlist(lapply(import.rank$method, function(x) paste(x, '-GP', sep = '')))
import.rank
# import.rank$method[import.rank$method == 'Shapley'] = 'Shapley-GP'
import.rank.sobol = read.csv('Importance-Sobol-GP.csv')

# Shapley
SVM.shap.delta = SVM.shap(mod = mod.adapt.delta)
SVM.shap.delta$typ = 'Pareto Distance'
SVM.shap.cutof = SVM.shap(mod = mod.adapt.cutof)
SVM.shap.cutof$typ = 'Threshold Cutoff'
SVM.shap.radan = SVM.shap(mod = mod.adapt.radan)
SVM.shap.radan$typ = 'Utopia Distance'

# Sobol
SVM.sobol.delta = SVM.sobol(mod = mod.adapt.delta)
SVM.sobol.delta$typ = 'Pareto Distance'
SVM.sobol.cutof = SVM.sobol(mod = mod.adapt.cutof)
SVM.sobol.cutof$typ = 'Threshold Cutoff'
SVM.sobol.radan = SVM.sobol(mod = mod.adapt.radan)
SVM.sobol.radan$typ = 'Utopia Distance'

ggplot(rbind(import.rank, SVM.shap.delta, SVM.shap.cutof, SVM.shap.radan, svm.import.margin)) +
  geom_col(mapping = aes(x = var, y = abs(r.import), fill = var)) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = 'Relative Importance') +
  guides(fill = 'none') + scale_x_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])))

ggplot(rbind(import.rank, SVM.shap.delta, SVM.shap.cutof, SVM.shap.radan, svm.import.margin)) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = 'Relative Importance') +
  guides(fill = 'none') + scale_x_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])))

# Rows: Selection method = Pareto distance, Objective Cutoff, Utopia Distance + Priority
# Columns: method = Shapley with SVM, Shapley with GP, new method

```


```{r}
# Split up by model method
import.all = rbind(import.rank, import.rank.sobol[, names(import.rank.sobol) %in% names(import.rank)],
                   SVM.shap.delta, SVM.shap.cutof, SVM.shap.radan, svm.import.margin,
                   SVM.sobol.delta, SVM.sobol.cutof, SVM.sobol.radan, SVM.sobol.delta)
# Classification split
split = unlist(lapply(import.all$method, strsplit, '-'))
import.all$method = split[c(TRUE, FALSE)]
import.all$model = split[c(FALSE, TRUE)]

import.all
ggplot(import.all) +
  geom_col(mapping = aes(x = model, y = import, fill = var), position = 'dodge') +
  geom_errorbar(mapping = aes(x = model, ymax = import + sd, ymin = import - sd, group = var), 
                position = 'dodge', width = 0.9) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = 'Model', y = 'Relative Importance (Unitless)') +
  scale_x_discrete(labels = c('GP', 'pol' = 'SVM-pol', 'rad' = 'SVM-rad')) +
  scale_fill_discrete(name = '', labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])))

# Relative importance (normalized): uncertainty is divided by the same value
r.max = import.all$import
for(i in seq(from = 1, to = length(r.max)-1, by = 2)){
  r.max[c(i,i+1)] = max(abs(r.max[c(i,i+1)]))
}
import.all$r.import = abs(import.all$import)/r.max
import.all$r.sd = import.all$sd / r.max
ggplot(import.all) +
  geom_col(mapping = aes(x = model, y = r.import, fill = var), position = 'dodge') +
  geom_errorbar(mapping = aes(x = model, ymax = r.import + r.sd, ymin = r.import - r.sd, group = var), 
                position = 'dodge', width = 0.9) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = 'Model', y = 'Relative Importance (Unitless)') +
  scale_x_discrete(labels = c('GP', 'pol' = 'SVM-pol', 'rad' = 'SVM-rad')) +
  scale_fill_discrete(name = '', labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])))

rm(r.max)

write.csv(import.all, 'ImportRank_Method.csv', row.names = F)
```

It is clear that the importance metric that was developed here provided more consistent results for the global importance than Shapley values. 
The importance among different ML models gives slightly different results, suggesting differences in the models more than problems with the importance metric.

Sobol method for the importance based on the variance contribution shows the opposite result with x2 having greater importance - contrary to what is expected based on the marginals.

# Existing Method: Gaussian Mixtures

Since the GM models are unsupervised, there cannot be control for what the selection criteria are. A superficial analysis will be conducted to check what the ML algorithm is converging to, but the results will be interpreted solely in terms of descriptive statistics.

For the purposes of analysis, three models will be tested based on the input to the ML algorithm:
* (x1, x2, f1, f2): the full set of inputs and outputs - since it has the most data, this will likely be the most robust
* (x1, x2): negative control of just the inputs - it should group the points based on the sampling densities
* (f1, f2): outputs only - this should have the best distinction between good and bad performing groups

Up to 12 categories will be allowed, and any type of Gaussian models will be accepted. This will lead to longer fitting time, but should help with accuracy.

```{r GM: Generating models, results='hide'}
GPar.all = read.csv(file = '../Ex_Quartic/GPar_all_start.csv')

# names(GPar.all)
max.cat = 12
cluster.all = Mclust(data = GPar.all[,c('x1', 'x2', 'f1', 'f2')], G = 2:max.cat)
cluster.in = Mclust(data = GPar.all[,c('x1', 'x2')], G = 2:max.cat)
cluster.out = Mclust(data = GPar.all[,c('f1', 'f2')], G = 2:max.cat)

```

```{r GM: Plotting model outputs}
# summary(cluster, parameters = TRUE)

GPar.all$typ.all = cluster.all$classification
GPar.all$typ.in = cluster.in$classification
GPar.all$typ.out = cluster.out$classification

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.all)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.in)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(x1, x2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist, color = 'delta'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof, color = 'cutof'), breaks = c(0.5)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = rad, color = 'rad'), breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = ang, color = 'ang'), breaks = c(50)) +
  # Pareto frontier
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), 
              level = 0.95, formula = (y~x), method = 'loess') + 
  # Clustering
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, fill = as.factor(typ.out)), size = 2.5, shape = 21) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Acceptance Criteria', fill = 'Group', 
       subtitle = '(f1, f2)') +
  scale_color_manual(values = c('delta' = '#1b9e77', 'cutof' = '#d95f02', 
                                'rad' = '#7570b3', 'ang' = '#e7298a', 
                                'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 
                                'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'rad' = expression('r < 1'),
                                'ang' = expression('F'[1]*'and F'[2]*' Balance'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'rad', 'ang', 'Pareto')) +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

```

The complete input (x1, x2, f1, f2) overcomplicates the system, providing 8 groups. One of these groups is clearly the Pareto frontier, but it does not include any points that are of similar performance. There are points to either side of it suggesting similar performance, but how far they extend is difficult to interpret.

The negative control (x1, x2) gives the expected result of largely grouping based on sampling density. This means the highly sampled Pareto frontier and local minimum are their own groups, and the rest of the space is divided based around the boundary between the highly sampled regions.

The output-only model gives the most useful results, as it groups the Pareto frontier inside of another high-performing group, which also includes the local optimum and the space spanning to it. It roughly lines up with the Pareto distance or threshold criteria; it does not appear to prioritize the angle or utopia distance. Showing only this model at finer resolution as it is the only relevant performing model

```{r}
res = predict(object = cluster.out, newdata = fine.grid[c('f1', 'f2')])
fine.grid$cluster.out = res$classification
ggplot() +
  # Cluster results
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = as.factor(cluster.out))) +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), color = 'black', breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof), color = 'red', breaks = c(0.5)) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(f1, f2)') +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

res = predict(object = cluster.all, newdata = fine.grid[c('x1', 'x2', 'f1', 'f2')])
fine.grid$cluster.all = res$classification
ggplot() +
  # Cluster results
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = as.factor(cluster.all)), size = 4) +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist), color = 'black', breaks = c(1)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = cutof), color = 'red', breaks = c(0.5)) +

  labs(x = expression('x'[1]), y = expression('x'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  theme_classic() + #theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

ggplot() +
  # Cluster results
  geom_point(data = filter(fine.grid, f2 < 50, f1 < 200), mapping = aes(x = f1, y = f2, color = as.factor(cluster.all)), size = 2) +
  labs(x = expression('f'[1]), y = expression('f'[2]), 
       color = 'Group', fill = 'Group', 
       subtitle = '(x1, x2, f1, f2)') +
  theme_classic() +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))

```

The (f1, f2) model appears to give a cluster that is somewhere between a threshold cutoff and the Pareto distance cutoff. 
In contrast, the behavior of the (x1, x2, f1, f2) model provides many more groups, including splitting the Pareto front into about three different categories with no obvious analog to why the boundaries are where they are. 
It appears to roughly fit the same boundaries of Pareto distance or threshold cutoffs, but not very well. 
A rough approximation is that groups (5, 6) are the Pareto front, groups (2, 3, 7) make up the region close to the Pareto front, and (1, 4, 8) are the region far from the front.

The model for (f1, f2) is going to give the same conditional probabilities as the Pareto distance or threshold acceptance criteria based on this similarity in the shape of the boundary. 
The interesting result to interpret is the (x1, x2, f1, f2), particularly when marginalizing to (x1, x2). 
Assuming that calculating (f1, f2) is expensive, the best approximation is that found from the GP models used to find the Pareto front itself. 
These will give (f1, f2) as a bivariate Gaussian distribution, which can be sampled from to estimate the likelihood that it falls into the Pareto front, the region close to it, or the region far from it.
This is achievable with a sequential Monte Carlo: given x1, sample x2 from the range, find the distribution of (f1, f2), sample (f1, f2), and solve the classification into the three groups.

```{r}
f1.mod = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f1')
f2.mod = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f2')

x.rng = seq(from = 0, to = 5, length.out = 50)
nsamp.x = 100; nsamp.f = 100
gm.margin = data.frame()
for(x in x.rng){
  # x1
  # Sample x2
  inframe = data.frame(x1 = x, x2 = runif(n = nsamp.x, min = 0, max = 5))
  classes = c()
  for(n in 1:nrow(inframe)){
    # Estimate distribution for f1, f2
    f1.est = predict(object = f1.mod, newdata = inframe, type = 'UK')
    f2.est = predict(object = f2.mod, newdata = inframe, type = 'UK')
    test.frame = data.frame(x1 = inframe$x1, x2 = inframe$x2,
                            f1 = rnorm(n = nsamp.f, mean = f1.est$mean, sd = f1.est$sd),
                            f2 = rnorm(n = nsamp.f, mean = f2.est$mean, sd = f2.est$sd))
    res = predict(object = cluster.all, newdata = test.frame)
    classes = c(classes, res$classification)
  }
  classes = data.frame(group = classes)
  gm.margin = rbind(gm.margin, data.frame(x = x, var = 'x1', 
           p.pare = nrow(filter(classes, group == 5 | group == 6))/(nsamp.f*nsamp.x), 
           p.near = nrow(filter(classes, group == 2 | group == 3 | group == 7))/(nsamp.f*nsamp.x), 
           p.dist = nrow(filter(classes, group == 1 | group == 4 | group == 8))/(nsamp.f*nsamp.x)))
  # x2
  # Sample x2
  inframe = data.frame(x2 = x, x1 = runif(n = nsamp.x, min = 0, max = 5))
  classes = c()
  for(n in 1:nrow(inframe)){
    # Estimate distribution for f1, f2
    f1.est = predict(object = f1.mod, newdata = inframe, type = 'UK')
    f2.est = predict(object = f2.mod, newdata = inframe, type = 'UK')
    test.frame = data.frame(x1 = inframe$x1, x2 = inframe$x2,
                            f1 = rnorm(n = nsamp.f, mean = f1.est$mean, sd = f1.est$sd),
                            f2 = rnorm(n = nsamp.f, mean = f2.est$mean, sd = f2.est$sd))
    res = predict(object = cluster.all, newdata = test.frame)
    classes = c(classes, res$classification)
  }
  classes = data.frame(group = classes)
  gm.margin = rbind(gm.margin, data.frame(x = x, var = 'x2', 
           p.pare = nrow(filter(classes, group == 5 | group == 6))/(nsamp.f*nsamp.x), 
           p.near = nrow(filter(classes, group == 2 | group == 3 | group == 7))/(nsamp.f*nsamp.x), 
           p.dist = nrow(filter(classes, group == 1 | group == 4 | group == 8))/(nsamp.f*nsamp.x)))
}

write.csv(gm.margin, 'Margin_GM.csv', row.names = F)
```

```{r}
gm.margin = read.csv('Margin_GM.csv')
gm.margin = data.frame(x = rep(gm.margin$x, 3),
                       var = rep(gm.margin$var, 3),
                       prob = c(gm.margin$p.pare, gm.margin$p.near, gm.margin$p.dist),
                       typ = c(rep('Pareto', nrow(gm.margin)), 
                               rep('Near-Pareto', nrow(gm.margin)), 
                               rep('Suboptimal', nrow(gm.margin))))
ggplot(gm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = typ)) +
  facet_wrap(~var, nrow = 2) +
  scale_color_brewer(palette = 'Dark2') + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'Region')

gm.margin = read.csv('Margin_GM.csv')
gm.margin = data.frame(x = rep(gm.margin$x),
                       var = rep(gm.margin$var),
                       prob = gm.margin$p.pare + gm.margin$p.near,
                       typ = 'Optimal')
ggplot(gm.margin) +
  geom_path(mapping = aes(x = x, y = prob, color = typ)) +
  facet_wrap(~var, nrow = 2) +
  scale_color_brewer(palette = 'Dark2') + 
  labs(x = 'Input Value', y = 'Conditional Probability', color = 'Region')

```

The estimated probability of being optimal (Pareto front group or the near-Pareto group) is similar to that calculated through the other methods for the Pareto distance or threshold criteria. However, because of the numerous variables required to generate the model, a larger and more complicated sampling procedure is necessary for accurate estimates.


# Overall:
* The unsupervised ML model (Gaussian mixture) can sort according to points that would be similar to the Pareto front. However, in order to obtain the model, all of the inputs and outputs are used in the training data, meaning to solve the single-variable probabilities, one must perform a time-consuming nested sampling procedure.
* The standard supervised ML model (Support Vector Machines) are flexible to selection criteria because those are part of the user input into generating the model. It is very sensitive to the kernel function that was selected, with the radial and polynomial forms working best for this specific case. When compared to the actual function, these good-fitting kernels appeared to be over-fit to the data and cannot capture the full dynamics of the space. In practice, separate training and testing sets should be used, which may be difficult for more complex objective functions due to the need for more sampling and therefore more function evaluations. The SVM did not substantially improve with adaptive sampling, indicating that the expenditure for collecting these samples may not be necessary.
* The GP fuzzy classifier works best with adaptive sampling, increasing its necessary computational budget, but accounts for the uncertainty in the model explicitly. This helps the accuracy of the single-variable probabilities compared to the SVM.
* More complicated conditions, such as only accepting points that prioritize one objective over the other by a certain margin, are difficult for both supervised learning methods to estimate, but the results from the SVM appear to be worse compared to the actual boundaries.
* The SVM methods give single-variable probabilities that are similar to the expected marginal, but not as close as the GP method.

