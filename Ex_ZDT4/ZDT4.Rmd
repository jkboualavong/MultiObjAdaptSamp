---
title: "Proof of concept: ZDT4 (modified)"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook showcases the method below for finding acceptable suboptimal performance in the fourth test function form Zitzler, Deb, and Thiele, 2000. 
The function is changed slightly to have a lower frequency in its cosine term to make it more reflective of physical systems.
The notebook will first test the algorithm using 3 inputs.

# Algorithm concept:
* I. Setup: find the Pareto frontier of the 2D optimization problem
1. Solve objective functions with a space filling method to obtain an initial set of data.
2. Use GPareto to find the Pareto front.
3. Calculate the normalized distance in the objective space and f1/f2 prioritization of each point to the Pareto front. These two metrics are used in the selection criteria that define acceptable suboptimal performance and the relative importance of each input variable.

* II. Acceptable suboptimal performance (the "near-Pareto set")
4. Establish a selection criteria for acceptable suboptimal performance (eg. within some tolerance of the Pareto front).
5. Using Gaussian processes, create a new objective called the "total uncertainty" function that describes the uncertainty in classifying the performance (acceptable or unacceptable) multiplied by the uncertainty in the actual value of the selection criteria functions.
6. Sample new points iteratively by maximizing the total uncertainty function. Repeat until the computational budget has been expended or the total uncertainty function maximum drops below a specified threshold, often relative to first iteration.

* III. Feature importance of each input variable
7. Calculate the conditional probability that a point will fall within the near-Pareto set given only information about that input variable.
8. Calculate the variance in the probability of the 1-variable conditional probability estimate. This variance represents the contribution due to all of the other variables in question.
9. The feature importance metric is the range of the probability divided by the average of the variance. The greater the range of probabilities, the more the single variable can influence the likelihood, but this is relative to the amount that the other variables contribute.

* IV: Suggested ranges
10. For the most important variable, determine the input values that give the peak response. This is the suggested input range.
11. For the next most important variable, calculate the conditional probability if this variable is known precisely, but the most important is known to be within its suggested range.
12. Repeat step 11 by continuing to constrain the optimal range of the previous variables until all conditional probabilities have been found. This sequential analysis describes the characteristics of the most promising inputs for the near-Pareto set.

For this test function, we will not be calculating the suggested ranges. See the 2-input systems (quartic or quadratic) for examples.

# Code
## 0. Initialization

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
```

## I. Setup: 
Set up the functions

The ZDT4 function has been modified slightly (changing the frequency of the cosine term) to have fewer local optima. This helps speed up calculations, as well as makes it more representative of the physical systems of interest, which are not going to have such rapid oscillatory behavior.

```{r Objective Functions}
ZDT4.mod = function (x) 
{
  if (is.null(dim(x))) {
    x <- matrix(x, nrow = 1)
  }
  n <- ncol(x)
  g <- 1 + 10 * (n - 1) + rowSums((x[, 2:n, drop = FALSE] * 
    0.5 - 0.25)^2 - 10 * cos(0.5 * pi * (x[, 2:n, drop = FALSE] * 
    10 - 5)))
  return(cbind(x[, 1], g * (1 - sqrt(x[, 1]/g))))
}

```

```{r Functions: Normalization}
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))

  return(GPar.data)
}

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  # Check if the angle is the same as a point on the Pareto front
  if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}

```

--
```{r Functions: Iterative Sampling}
# Model construction
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 # covtyp = 'gauss', # Gaussian uncertainty
                 # optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50), # Increase robustness
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}

fill.sample.delta = function(x, model){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res = predict(object = model, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(res$sd*(prob*(1-prob) + 0.25/99))
  # Offset on the uncertainty so that 0 uncertainty (p = 0 or 1) does not give a 0 result; 
  # instead these points give a 1 order of magnitude less weight
}

fill.sample.cutof = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), 
  # and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 2, sd = res.f1$sd) * 
    pnorm(q = 0, mean = res.f2$mean - 2, sd = res.f2$sd)
  
  # Variance based on propagation of errors, assuming independent measures
  sd = (res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2)

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(sd*(prob*(1-prob) + 0.25/99))
}

fill.sample.radan = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 2, sd = res.f1$sd) * (1 - pnorm(q = 0, mean = res.f2$mean - 20, sd = res.f2$sd))
  
  # Variance based on propagation of errors
  sd = (res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2) 

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(sd*(prob*(1-prob) + 0.25/99))
}

fill.sample.update = function(GPar.front.old, GPar.data, GPar.new){
  # Update the dataset and Pareto front with the new datapoint
  
  # Update the Pareto front
  # The new point will only have the inputs and direct outputs
  GPar.front.new = rbind(GPar.front.old[, names(GPar.front.old) %in% names(GPar.new)], GPar.new)
  pos.front = t(nondominated_points(points = t(as.matrix(GPar.front.new[, c('f1', 'f2')]))))
  GPar.front.new = filter(GPar.front.new, f1 %in% pos.front[,1], f2 %in% pos.front[,1])
  
  # If the new point is a new minimum for either f1 or f2, then need to re-calculate all of the normalized values
  if(which.min(c(GPar.new$f1, GPar.front.old$f1)) %in% 1:nrow(GPar.new) | 
     which.min(c(GPar.new$f2, GPar.front.old$f2)) %in% 1:nrow(GPar.new) ){
    # Update front
    f1.f2.update = n.obj(GPar.data = GPar.front.new, GPar.front = GPar.front.new)
    GPar.front.new$f1.norm = f1.f2.update$f1.norm; GPar.front.new$f2.norm = f1.f2.update$f2.norm
    GPar.front.new$theta = atan(GPar.front.new$f2.norm/GPar.front.new$f1.norm)*180/pi*10/9
    # Update dataset
    f1.f2.update = n.obj(GPar.data = GPar.data, GPar.front = GPar.front.new)
    GPar.data$f1.norm = f1.f2.update$f1.norm; GPar.data$f2.norm = f1.f2.update$f2.norm
    GPar.data$theta = atan(GPar.data$f2.norm/GPar.data$f1.norm)*180/pi*10/9
    } else{GPar.front.new = GPar.front.old}
  # Get normalized values for new points
  GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front.new)
  GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
  
  # If any new points are on the Pareto front at all, then update the distance and theta
  new.pareto = nrow(filter(GPar.new, f1 %in% pos.front[,1], f2 %in% pos.front[,1]))
  if(new.pareto > 0){ # Need to update all points in the front
    dist = c()
    for(i in 1:nrow(GPar.data)){
      dist = c(dist, n.dist(f1.norm = GPar.data$f1.norm[i], f2.norm = GPar.data$f2.norm[i], GPar.front = GPar.front.new))
    }
    GPar.data$dist = dist
  } 
  # Need to calculate the new point's distance
  dist = c()
  for(i in 1:nrow(GPar.new)){
    dist = c(dist, n.dist(f1.norm = GPar.new$f1.norm[i], f2.norm = GPar.new$f2.norm[i], GPar.front = GPar.front.new))
  }
  GPar.new$dist = dist
  GPar.new$order = max(GPar.data$order) + 1
  # Add the new point
  GPar.data = rbind(GPar.data[, names(GPar.data) %in% names(GPar.new)], GPar.new)
  
  return(list(front = GPar.front.new, data = GPar.data, new = GPar.new))
}
```

```{r Functions: Marginalization}
marginal.dist = function(mod.dist){
  # Calculate the marginal distributions for x1 and x2 given a model
  resolution = 75; MCsamp = 1500
  x1.rng = c(0, 1); x2.rng = x1.rng; x3.rng = x1.rng
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], 
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], 
                                          prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x1', ncond = 'None'))  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], 
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], 
                                          psd = sd(fill.frame$p.del1),
                                          prob = mean(fill.frame$p.del1),
                                          var = 'x2', ncond = 'None'))
    # x3
    fill.frame = data.frame(x3 = x3.seq[i], 
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
    res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
    Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], 
                                          psd = sd(fill.frame$p.del1),
                                          prob = mean(fill.frame$p.del1),
                                          var = 'x3', ncond = 'None'))
  }
  return(rbind(Infer.x1, Infer.x2, Infer.x3))
}

marginal.cut = function(mod.f1, mod.f2){
  resolution = 75; MCsamp = 1500
  x1.rng = c(0, 1); x2.rng = x1.rng; x3.rng = x1.rng
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], 
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x1', ncond = 'None'))  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], 
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x2', ncond = 'None'))
    # x3
    fill.frame = data.frame(x3 = x3.seq[i], 
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
    res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
    Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x3', ncond = 'None'))  
  }
  return(rbind(Infer.x1, Infer.x2, Infer.x3))
}

marginal.rad = function(mod.rad, mod.ang){
  resolution = 75; MCsamp = 1500
  x1.rng = c(0, 1); x2.rng = x1.rng; x3.rng = x1.rng
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], 
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
      (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], 
                                          prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1), 
                                          var = 'x1', ncond = 'None'))  
  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], 
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                            x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
    res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
      (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], 
                                          prob = mean(fill.frame$p.del1),
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x2', ncond = 'None'))
    
    # x3
    fill.frame = data.frame(x3 = x3.seq[i], 
                            x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                            x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
    res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
      (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
    Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], 
                                          prob = mean(fill.frame$p.del1),
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x3', ncond = 'None'))
  }
  return(rbind(Infer.x1, Infer.x2, Infer.x3))
}

```

1. Solve objective functions with a space filling method to obtain an initial set of data.
The function takes three inputs, defined $x_1$, $x_2$, and $x_3$ to produce two outputs $f_1$ and $f_2$. 
The domain of all $x_i$ is [0,1]. 

The initial search obtains the extremes of the 4 corners, the center points on each edge, the central point, and 3*(number of inputs) random points in each quadrant.


```{r Grid Setup}
# Set up simple grid
sz = 3
x1.rng = seq(from = 0, to = 1, length.out = sz)
x2.rng = seq(from = 0, to = 1, length.out = sz)
x3.rng = seq(from = 0, to = 1, length.out = sz)
des.start = expand.grid(x1.rng, x2.rng, x3.rng)
des.start = des.start[,1:3]
names(des.start) = c('x1', 'x2', 'x3')
# Random points in each cube
npt = 2
bounds = c(0, 0.5, 1)
for(i in 1:2){
  for(j in 1:2){
    for(k in 1:2){
      des.start = rbind(des.start,
                  data.frame(x1 = runif(n = length(names(des.start))*npt, min = bounds[i], max = bounds[i+1]),
                             x2 = runif(n = length(names(des.start))*npt, min = bounds[j], max = bounds[j+1]),
                             x3 = runif(n = length(names(des.start))*npt, min = bounds[k], max = bounds[k+1])))
    }
  }
}

# Solve edge cases
Pareto.budget = min(100, 2*nrow(des.start)) # Want a clearly defined Pareto front, but a max budget of 100
des.start = matrix(c(des.start$x1, des.start$x2, des.start$x3), ncol = 3, nrow = nrow(des.start))
res.start = ZDT4.mod(x = des.start)

```

2. Use GPareto to find the Pareto front.

GPareto uses Gaussian processes to approximate the Pareto frontier, defaulting to the SMS-EGO selection criterion.
The function produces both the Pareto frontier estimate and the set of iterations, which will both be used in subsequent steps.
To simplify computation time, the data are stored in .csv files so these variables do not need to be calculated with each attempt.

```{r GPareto Calculation}
res = easyGParetoptim(fn = ZDT4.mod, budget = Pareto.budget, lower = c(0, 0, 0), upper = c(1, 1, 1), 
                      par = des.start, value = res.start, ncores = 2)
plotGPareto(res)
# Format into dataframe for easier plotting
GPar.front = data.frame(x1 = res$par[,1], x2 = res$par[,2], x3 = res$par[,3], f1 = res$value[,1], f2 = res$value[,2])
GPar.all =  data.frame(x1 = res$history$X[,1], x2 = res$history$X[,2], x3 = res$history$X[,3], f1 = res$history$y[,1], f2 = res$history$y[,2])
GPar.all$order = c(rep(0, nrow(des.start)), seq(from = 1, to = Pareto.budget, by = 1))
rm(res)
write.csv(GPar.all, file = 'GPar_all_data.csv')
write.csv(GPar.front, file = 'GPar_fnt_data.csv')
```

3. Calculate the normalized distance in the objective space and f1/f2 prioritization of each point to the Pareto front. These two metrics are used in the selection criteria that define acceptable suboptimal performance and the relative importance of each input variable.

Normalization is defined such that (0,0) is the utopia point and (1,0) and (0,1) are the single-objective optimizations. 
The prioritization is the angle $\theta$ made between the point in the normalized objective space and the x-axis, i.e. the axis of purely prioritizing objective 2. 
This angle is re-mapped to a percentage from 0 to 100% prioritization of objective 1. 
The distance between each point and the Pareto front is the distance in the normalized space

```{r Map to Normalized Objective Space}
GPar.all = read.csv(file = 'GPar_all_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
Pareto.budget = min(100, 2*nrow(des.start)) # Want a clearly defined Pareto front, but a max budget of 100
# Saved data have an index
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]
GPar.front = GPar.front[, !(names(GPar.front) %in% c("X"))]

# Run normalization
GPar.all = n.obj(GPar.data = GPar.all, GPar.front = GPar.front)
GPar.front = n.obj(GPar.data = GPar.front, GPar.front = GPar.front)
GPar.front = GPar.front[order(GPar.front$f1.norm),]

# Calculate theta according to the angle. Remap from [0, pi/2] to [0, 100] for simplicity
GPar.all$theta = atan(GPar.all$f2.norm/GPar.all$f1.norm)*180/pi*10/9

# Run distance calculations in parallel to speed up
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.all)) %dopar%
  n.dist(f1.norm = GPar.all$f1.norm[row], f2.norm = GPar.all$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.all$dist = unlist(dist)
rm(dist, cl)


# Save the starting data to test multiple types of acceptance criteria
write.csv(x = GPar.all, file = 'GPar_all_start.csv')
write.csv(x = GPar.front, file = 'GPar_fnt_start.csv')

```

Plot the contours of the original functions
```{r Plot Objective Functions}
# Fine grid spacing
sz.fine = 100
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = sz.fine)
# x3.rng = seq(from = 0, to = 1, length.out = 5)
# x3.rng = unique(c(x3.rng, unique(GPar.front$x3)))
x3.rng = 0.5 # Simple slice for visualization
con.fine = expand.grid(x1.rng, x2.rng, x3.rng)
con.fine = con.fine[,1:3]
names(con.fine) = c('x1', 'x2', 'x3')

# Calculate
res = ZDT4.mod(x = as.matrix(con.fine))
con.fine$f1 = res[,1]; con.fine$f2 = res[,2]

ggplot() +
  geom_contour(data = con.fine, mapping = aes(x = x1, y = x2, z = f1, color = 'f1'), bins = 10) +
  geom_contour(data = con.fine, mapping = aes(x = x1, y = x2, z = f2, color = 'f2'), bins = 10) +
  labs(x = expression('x'[1]), y = expression('x'[2])) +
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = "black") +
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = "black", method = 'loess', formula = (y~x)) +
  scale_color_manual(name = 'Objective', values = c('f1' = 'red', 'f2' = 'blue'), 
                     labels = c('f1' = expression('F'[1]), 'f2' = expression('F'[2]))) +
  theme_classic() +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

# Lower resolution but include multiple slices
sz.fine = 50
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = sz.fine)
x3.rng = seq(from = 0, to = 1, length.out = 9) # Simple slice for visualization
con.fine = expand.grid(x1.rng, x2.rng, x3.rng)
con.fine = con.fine[,1:3]
names(con.fine) = c('x1', 'x2', 'x3')

# Calculate
res = ZDT4.mod(x = as.matrix(con.fine))
con.fine$f1 = res[,1]; con.fine$f2 = res[,2]

ggplot() +
  geom_point(data = con.fine, 
                      mapping = aes(x = x1, y = x2, color = f2)) +
  labs(x = expression('x'[1]), y = expression('x'[2]*' or x'[3]), color = expression('F'[2])) +
  theme_classic() + facet_wrap(~x3) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

ggplot() +
  geom_point(data = con.fine, 
                      mapping = aes(x = x1, y = x2, color = f1)) +
  labs(x = expression('x'[1]), y = expression('x'[2]*' or x'[3]), color = expression('F'[1])) +
  theme_classic() + facet_wrap(~x3) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

rm(res, con.fine, x1.rng, x2.rng, x3.rng)
```

Three different selection criteria are to be tested: (1) normalized distance to the Pareto frontier, (2) independent threshold values of the two objective functions, (3) distance to the utopia point and relative prioritization. 

While the region of interest is often small, no physical system is of interest if it is not viable at all.
This means that at least 5% of the total hypervolume domain should meet the acceptance criteria.
For the ZDT4 function, this requires initial testing to decide the bounds for what the thresholds for acceptance.

This 5% cutoff will be assessed by Monte Carlo sampling, combined with a coarse 3x3 resolution grid to ensure all regions are sampled.

```{r Identifying cutoff criteria}
# Set up the grid
bounds = seq(from = 0, to = 1, length.out = 4)
# Collect an equal number of points from each grid section
nsamp = 5e2
test.samp = data.frame()
for(i in 1:3){
  for(j in 1:3){
    for(k in 1:3){
      next.grid = data.frame(x1 = runif(n = nsamp, min = bounds[i], max = bounds[i+1]),
                             x2 = runif(n = nsamp, min = bounds[j], max = bounds[j+1]),
                             x3 = runif(n = nsamp, min = bounds[k], max = bounds[k+1]))
      test.samp = rbind(test.samp, next.grid)
    }
  }
}

# Calculate the function evaluations
res = ZDT4.mod(x = as.matrix(test.samp[,c('x1', 'x2', 'x3')]))
test.samp$f1 = res[,1]; test.samp$f2 = res[,2]
# Normalized variables
test.samp = n.obj(GPar.data = test.samp, GPar.front = GPar.front)

# Calculate theta according to the angle. Remap from [0, pi/2] to [0, 100] for simplicity
test.samp$theta = atan(test.samp$f2.norm/test.samp$f1.norm)*180/pi*10/9

# Distance to the Pareto front
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(test.samp)) %dopar%
  n.dist(f1.norm = test.samp$f1.norm[row], f2.norm = test.samp$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
test.samp$dist = unlist(dist)
rm(dist, cl)

# Fraction meeting dist < specific value
dist.check = data.frame(x = seq(from = 0, to = 4, length.out = 20),
                        delta = 0, cutof = 0, radan = 0)
for(i in 1:nrow(dist.check)){
  bound = dist.check$x[i]
  dist.check$delta[i] = nrow(filter(test.samp, dist <= bound))/nrow(test.samp)
  dist.check$cutof[i] = nrow(filter(test.samp, f1.norm <= bound, f2.norm <= bound))/nrow(test.samp)
  dist.check$radan[i] = nrow(filter(test.samp, sqrt(f1.norm^2 + f2.norm^2) <= bound & theta > 20))/nrow(test.samp)
  
}
sz = 2.5
dist.check
ggplot(dist.check) +
  geom_point(mapping = aes(x = x, y = delta, color = 'delta', shape = 'delta'), size = sz) +
  geom_point(mapping = aes(x = x, y = cutof, color = 'cutof', shape = 'cutof'), size = sz) +
  geom_point(mapping = aes(x = x, y = radan, color = 'radan', shape = 'radan'), size = sz) +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'blue', 'radan' = 'green3'),
                     labels = c('delta' = 'P.Dist', 'cutof' = 'Cutoff', 'radan' = 'U.Dist'), 
                     name = '') +
  scale_shape_manual(values = c('delta' = 1, 'cutof' = 2, 'radan' = 3),
                     labels = c('delta' = 'P.Dist', 'cutof' = 'Cutoff', 'radan' = 'U.Dist'),
                     name = '') +
  labs(x = 'Cutoff Value', y = 'Accepted Fraction')

rm(bounds, nsamp, test.samp, sz, dist.check, next.grid, res)

```

Based on this Monte Carlo estimate, the cutoff value should be approximately 2 for all 3 criteria to guarantee at least 5% of the total hypervolume space is accepted.

Determine the expected 1-variable marginals with the above criteria for comparison to the surrogate functions later.
The code is set so it only runs once due to the large dataset size.

```{r Expected Marginals}
# Approximate the true result by with a grid. Use the same grid size as the resolution for the marginalization measures
if(file.exists('ExpectedMarginal_delta.csv') == FALSE){
grid.sz = 75
fine.grid = expand.grid(x1 = seq(from = 0, to = 1, length.out = grid.sz), 
                        x2 = seq(from = 0, to = 1, length.out = grid.sz),
                        x3 = seq(from = 0, to = 1, length.out = grid.sz))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Calculate the actual results
res = ZDT4.mod(x = as.matrix(fine.grid))
fine.grid$f1 = res[,1]; fine.grid$f2 = res[,2]

# Obtain the Pareto front
pos.front = t(nondominated_points(points = t(as.matrix(fine.grid[, c('f1', 'f2')]))))
front = filter(fine.grid, f1 %in% pos.front[,1], f2 %in% pos.front[,2])

fine.grid = n.obj(GPar.data = fine.grid, GPar.front = front)
front = n.obj(GPar.data = front, GPar.front = front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(fine.grid)) %dopar%
  n.dist(f1.norm = fine.grid$f1.norm[row], f2.norm = fine.grid$f2.norm[row], GPar.front = front)
stopCluster(cl)
fine.grid$dist = unlist(dist)
fine.grid$ang = atan(fine.grid$f2.norm/fine.grid$f1.norm)*180/pi*10/9
fine.grid$rad = sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2)

# Calculate the marginals
# Integrate
delta.tru = data.frame()
cutof.tru = data.frame()
radan.tru = data.frame()
for(x in unique(fine.grid$x1)){
  # Only the single value of x1 or x2
  sub1 = filter(fine.grid, x1 == x)
  sub2 = filter(fine.grid, x2 == x)
  sub3 = filter(fine.grid, x3 == x)
  
  # Distance criteria: Pareto distance less than 1
  p1 = nrow(filter(sub1, dist <= 1)) / nrow(sub1)
  p2 = nrow(filter(sub2, dist <= 1)) / nrow(sub2)
  p3 = nrow(filter(sub3, dist <= 1)) / nrow(sub3)
  p = c(p1, p2, p3)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  delta.tru = rbind(delta.tru, data.frame(x = x, prob = p, psd = psd, var = c('x1', 'x2', 'x3')))
  
  # Cutoff criteria: normalized value less than 1 for both f1 and f2
  p1 = nrow(filter(sub1, f1.norm <= 1, f2.norm <= 1)) / nrow(sub1)
  p2 = nrow(filter(sub2, f1.norm <= 1, f2.norm <= 1)) / nrow(sub2)
  p3 = nrow(filter(sub3, f1.norm <= 1, f2.norm <= 1)) / nrow(sub3)
  p = c(p1, p2, p3)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  cutof.tru = rbind(cutof.tru, data.frame(x = x, prob = p, psd = psd, var = c('x1', 'x2', 'x3')))
  
  # Radius-angle criteria: utopia distance less than 1, angle above 20%
  p1 = nrow(filter(sub1, rad <= 1, ang >= 20)) / nrow(sub1)
  p2 = nrow(filter(sub2, rad <= 1, ang >= 20)) / nrow(sub2)
  p3 = nrow(filter(sub3, rad <= 1, ang >= 20)) / nrow(sub2)
  p = c(p1, p2, p3)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  radan.tru = rbind(radan.tru, data.frame(x = x, prob = p, psd = psd, var = c('x1', 'x2', 'x3')))
  
}
rm(sub1, sub2, p1, p2, p3, p, psd, fine.grid, pos.front, front, grid.sz)

# Store for later
write.csv(delta.tru, 'ExpectedMarginal_delta.csv')
write.csv(cutof.tru, 'ExpectedMarginal_cutof.csv')
write.csv(radan.tru, 'ExpectedMarginal_radan.csv')
}
```


Plot the full dataset in the objective space
```{r Visualize GPareto and Normalization, fig.width=6, fig.height=6}
gplt.GPar = ggplot() +
  geom_hline(yintercept = 0, linetype = 'dotted', color = 'black') + geom_vline(xintercept = 0, linetype = 'dotted', color = 'black') +
  geom_line(data = GPar.front, mapping = aes(x = f1, y = f2), color = "black") +
  geom_point(data = GPar.all, mapping = aes(x = f1, y = f2, color = (order > 0), 
                                            shape = (dist == 0)), size = 2) +
  labs(x = "Function 1", y = "Function 2", color = "") +  
  scale_color_discrete(labels = c('Starting Data', 'Pareto Search'), breaks = c(FALSE, TRUE)) +
  scale_shape_discrete(labels = c('Pareto frontier'), breaks = c(TRUE)) +
  theme_classic() + theme(legend.position = c(0.85, 0.85), legend.background = element_rect(fill = 'white'), 
                          legend.title = element_blank())

gplt.Norm = ggplot() +
  geom_hline(yintercept = 0, linetype = 'dotted', color = 'black') + geom_vline(xintercept = 0, linetype = 'dotted', color = 'black') +
  geom_line(data = GPar.front, mapping = aes(x = f1.norm, y = f2.norm), color = "black") +
  geom_point(data = GPar.all, mapping = aes(x = f1.norm, y = f2.norm, color = (order > 0),
                                            shape = (dist == 0)), size = 2) +
  labs(x = "Normalized Function 1", y = "Normalized Function 2", color = "") +  
  scale_color_discrete(labels = c('Starting Data', 'Pareto Search'), breaks = c(FALSE, TRUE)) +
  scale_shape_discrete(labels = c('Pareto frontier'), breaks = c(TRUE)) +
  guides(color = FALSE, shape = FALSE) +
  theme_classic() + theme(legend.position = c(0.85, 0.85), legend.background = element_rect(fill = 'white'),
                          legend.title = element_blank())

gplt.GPar / gplt.Norm
rm(gplt.GPar, gplt.Norm)
```

## II. Acceptable suboptimal performance (the "near-Pareto set")
4. Establish a selection criteria for acceptable suboptimal performance (eg. within some tolerance of the Pareto front).

For this example, defines $\delta$ as the distance between the point and the point on the Pareto frontier with the same value of theta. Accept only points with $\delta < 2$.

```{r Load data: Accept Distance less than 2}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: check if the first name is x1
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

```

5. Using Gaussian processes, create a new objective called the "total uncertainty" function that describes the uncertainty in classifying the performance (acceptable or unacceptable) multiplied by the uncertainty in the actual value of the selection criteria functions.

The sample utility function prioritizes points that are likely to describe the boundary (normalized distance equal to 1) with the greatest model uncertainty at the time. 
When the point is sampled, the uncertainty in the local vicinity effectively decreases to 0. 
Points on the boundary have intermediate acceptance probabilities, ie. close to 0.5, while points with the greatest model uncertainty have the greatest standard error. 
Therefore, the utility function can be estimated as the product of the standard error, the probability of being accepted, and the probability of being rejected.

Visualize the first iteration to show that it actually samples the point that is most useful.
```{r Iteration 1: Distance less than 1, warning=FALSE, message=FALSE}
# Search for the first point
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.delta(x, model = mod.dist)},
             lower = c(0, 0, 0), upper = c(1, 1, 1),
             popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
res = ZDT4.mod(x = as.matrix(GPar.new))
GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
# Add point to the samples
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.front = res$front
GPar.all = res$data
rm(res)
# Fitness value for iteration
start.fit = max(GA.pred@fitness)

# Store the initial mod.dist for later
mod.dist.init = mod.dist

```


```{r}
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist.init, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.dist.init, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```

The slices clearly show preliminary estimates of the multimodality with respect to $x_2$ and $x_3$.

6. Sample new points iteratively by maximizing the total uncertainty function. Repeat until the computational budget has been expended or the total uncertainty function maximum drops below a specified threshold, often relative to first iteration.

A typical stop criteria is based on the hypervolume fraction that is uncertain, i.e. an acceptance probability between 0.25 and 0.75. 
However, as the number of input variables increases, the number of sampled points required for an accurate Monte Carlo estimate increases. 
Instead, a single point-based stopping criteria is more useful; in this case, the metric is the maximum sample utility in the optimization function. 
When the value drops by 2 orders of magnitude, then the next point is not likely to provide much improvement.

The computational budget is (number of inputs)*10
```{r Iteration loop: Accept distance less than 1, message=FALSE, warning=FALSE}
budget = 30

next.fit = start.fit
while(next.fit > start.fit*0.01 & max(GPar.all$order) < budget+Pareto.budget){
  # Create new Kriging model
  mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')
  
  # Run the optimization to find the best point
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.delta(x, model = mod.dist)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]

  # Fill in the remaining calculations: normalized outputs, distance, theta, order
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.front = res$front
  GPar.all = res$data
  rm(res)
  
  # The cutoff point for the loop is 20 additional points or when the 
  # fitness value is less than 1% of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

# Save the data for later
write.csv(GPar.all, 'GPar_Accept_Delta1.csv')

rm(point.next)
```

```{r}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Delta1.csv')
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')

lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.dist, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)

```
## III. Feature importance of each input variable
7. Calculate the conditional probability that a point will fall within the near-Pareto set given only information about that input variable.

The conditional probability is calculated by marginalization of the GP probabilities. From the GP, the value of P[accept | $x_1, x_2, x_3$] can be calculated, and the relationship between P[accept | $x_1, x_2, x_3$] and  P[accept | $x_1$] is the integral:

P[accept | $x_1$] = integral( P[accept | $x_1, x_2, x_3$]  P[$x_2$] P[$x_3$] $dx_3$ $dx_2$ )

In the absence of data to inform the distribution of $x_2$ or $x_3$, this distribution is set as a uniform distribution on the range used in the optimization.

8. Calculate the variance in the probability of the 1-variable conditional probability estimate. This variance represents the contribution due to all of the other variables in question.
9. The feature importance metric is the range of the probability divided by the average of the variance. The greater the range of probabilities, the more the single variable can influence the likelihood, but this is relative to the amount that the other variables contribute.

```{r Feature Importance: 1-Variable Marginalization}
# Load data: compare before/after refinement and with a random sampling for the refinement
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Delta1.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 1),
                        x2 = runif(n = nsamp, min = 0, max = 1),
                        x3 = runif(n = nsamp, min = 0, max = 1))
res = ZDT4.mod(x = as.matrix(GPar.rando))
GPar.rando$f1 = res[,1]; GPar.rando$f2 = res[,2]
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Construct models for each dataset
mod.dist.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')
mod.dist.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')
mod.dist.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 1500 samples each.
Infer.dist.start = marginal.dist(mod.dist.start)
Infer.dist.adapt = marginal.dist(mod.dist.adapt)
Infer.dist.rando = marginal.dist(mod.dist.rando)

# Compare to true marginal
delta.tru = read.csv('ExpectedMarginal_delta.csv')

ggplot() +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.dist.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.dist.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.dist.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~., scales = 'free_y') +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability',
                                  'start' = 'Before Sampling',
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination.
expect1 = filter(delta.tru, var == 'x1')$prob
expect2 = filter(delta.tru, var == 'x2')$prob
expect3 = filter(delta.tru, var == 'x3')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.dist.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.dist.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2),
           R2.x3 = c(cor(x = filter(Infer.dist.start, var == 'x3')$prob, y = expect3, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x3')$prob, y = expect3, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x3')$prob, y = expect3, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.dist.adapt, var == 'x1')
Infer.x2 = filter(Infer.dist.adapt, var == 'x2')
Infer.x3 = filter(Infer.dist.adapt, var == 'x3')
Import.delta = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd),
                                     diff(range(Infer.x3$prob))*nrow(Infer.x3)/sum(Infer.x3$psd)),
                          var = c('x1', 'x2', 'x3'),
                          typ = 'delta')
# Uncertainty in the calculation based on the Monte Carlo error
Import.delta$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)),
                    max(Infer.x3$prob)*(1-max(Infer.x3$prob)) + min(Infer.x3$prob)*(1-min(Infer.x3$prob)) * 
                      diff(range(Infer.x3$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd), var(Infer.x3$psd))/nrow(Infer.x1) )
  
ggplot(Import.delta) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymin = import - sd, ymax = import + sd), width = 0.5) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Pareto Distance Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4', '#fdc086'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3);
delta.tru$cat = 'tru';          Infer.dist.start$cat = 'start'
Infer.dist.adapt$cat = 'adapt'; Infer.dist.rando$cat = 'rando'
write.csv(rbind(delta.tru[, names(delta.tru) %in% names(Infer.dist.start)],
      Infer.dist.start[, names(Infer.dist.start) %in% names(delta.tru)],
      Infer.dist.adapt[, names(Infer.dist.adapt) %in% names(delta.tru)],
      Infer.dist.rando[, names(Infer.dist.rando) %in% names(delta.tru)]), 'Marginals_delta_tru.csv')

```

While the ranking proceeds $x_3 > x_2 > x_1$, $x_3 ~ x_2$, which is consistent with expectation since both variables behave identically in the function.
The slight difference between $x_2$ and $x_3$ appears to be a product sample biasing and asymmetry in sampling with respect to these variables.
Since there was no a priori constraint applied to these variables, the proximity in their importance and marginals is promising, as this is purely a product of the quality of the surrogates.

## Alternative Acceptance Criteria
For the purpose of illustration, the above method will be repeated with two other acceptance criteria:
* Objective function values below a specific threshold.
* Within a specific normalized distance of the utopia point and a specified prioritization of the two objective functions.

### Threshold cutoff
For simplicity, the cutoff values are the normalized values of 1 for both objectives. This is criteria, the acceptable points are those that fall within the normalized box defined by the utopia point (0,0) and the pseudo-nadir point (2,2).

Load the original dataset (after solving the Pareto front).

```{r Threshold: Load original data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: check if the first name is x1
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

```

```{r Threshold: First iteration, message=FALSE, warning=FALSE}
# Search for the first point
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.cutof(x, model.f1 = mod.f1, model.f2 = mod.f2)},
             lower = c(0, 0, 0), upper = c(1, 1, 1),
             popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
res = ZDT4.mod(x = as.matrix(GPar.new))
GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
# Add point to the samples
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.front = res$front
GPar.all = res$data
rm(res)
# Fitness value for iteration
start.fit = max(GA.pred@fitness)

# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.f1.init = mod.f1
mod.f2.init = mod.f2
```

Visualize the first loop again to show that the set of the acceptable points is different due to the different criteria.

```{r Threshold: First Iteration}
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.f1.init, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.f1.init, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```

```{r Threshold: Iterations, message=FALSE, warning=FALSE}
budget = 30
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Create a Kriging model for the distance
  mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
  mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.cutof(x, model.f1 = mod.f1, model.f2 = mod.f2)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]

  # Add point to the samples
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.front = res$front
  GPar.all = res$data
  rm(res)

  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

write.csv(GPar.all, 'GPar_Accept_Threshold.csv')

rm(res)
```

```{r Treshold: Visualize Iterations}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Threshold.csv')
# Create a Kriging model for the distance
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.f1, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.f1, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```

As indicated by this acceptance criteria, the boundary of the set it well defined with just the Pareto frontier, so additional iterations are not as necessary. Continuing with the procedure to show the feature importance and acceptance probability procedures.

```{r Threshold: Marginalization}
# Load data: compare before/after refinement and with a random sampling for the refinement
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Threshold.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 1),
                        x2 = runif(n = nsamp, min = 0, max = 1),
                        x3 = runif(n = nsamp, min = 0, max = 1))
res = ZDT4.mod(x = as.matrix(GPar.rando))
GPar.rando$f1 = res[,1]; GPar.rando$f2 = res[,2]
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Use the final GP model with the full dataset
mod.f1.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

mod.f1.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

mod.f1.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 1500 samples each.
Infer.cut.start = marginal.cut(mod.f1.start, mod.f2.start)
Infer.cut.adapt = marginal.cut(mod.f1.adapt, mod.f2.adapt)
Infer.cut.rando = marginal.cut(mod.f1.rando, mod.f2.rando)

# Compare to true marginal
cutof.tru = read.csv('ExpectedMarginal_cutof.csv')

ggplot() +
  geom_path(data = cutof.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = cutof.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.cut.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.cut.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.cut.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~., scales = 'free_y') +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = 'Threshold Cutoff',
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability',
                                  'start' = 'Before Sampling',
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination.
expect1 = filter(cutof.tru, var == 'x1')$prob
expect2 = filter(cutof.tru, var == 'x2')$prob
expect3 = filter(cutof.tru, var == 'x3')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.cut.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.cut.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.cut.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.cut.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.cut.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.cut.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2),
           R2.x3 = c(cor(x = filter(Infer.cut.start, var == 'x3')$prob, y = expect3, method = 'pearson')^2, 
             cor(x = filter(Infer.cut.adapt, var == 'x3')$prob, y = expect3, method = 'pearson')^2,
             cor(x = filter(Infer.cut.rando, var == 'x3')$prob, y = expect3, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.cut.adapt, var == 'x1')
Infer.x2 = filter(Infer.cut.adapt, var == 'x2')
Infer.x3 = filter(Infer.cut.adapt, var == 'x3')
Import.cutof = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd),
                                     diff(range(Infer.x3$prob))*nrow(Infer.x3)/sum(Infer.x3$psd)),
                          var = c('x1', 'x2', 'x3'),
                          typ = 'cutof')
# Uncertainty in the calculation based on the Monte Carlo error
Import.cutof$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)),
                    max(Infer.x3$prob)*(1-max(Infer.x3$prob)) + min(Infer.x3$prob)*(1-min(Infer.x3$prob)) * 
                      diff(range(Infer.x3$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd), var(Infer.x3$psd))/nrow(Infer.x1) )
  
ggplot(Import.cutof) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymin = import - sd, ymax = import + sd), width = 0.5) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Threshold Cutoff Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4', '#fdc086'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3);
cutof.tru$cat = 'tru';          Infer.cut.start$cat = 'start'
Infer.cut.adapt$cat = 'adapt'; Infer.cut.rando$cat = 'rando'
write.csv(rbind(cutof.tru[, names(cutof.tru) %in% names(Infer.cut.start)],
      Infer.cut.start[, names(Infer.cut.start) %in% names(cutof.tru)],
      Infer.cut.adapt[, names(Infer.cut.adapt) %in% names(cutof.tru)],
      Infer.cut.rando[, names(Infer.cut.rando) %in% names(cutof.tru)]), 'Marginals_cutof_tru.csv')

```

As with the previous criteria (which is very similar in its region of acceptance), $x_2 ~ x_3$ in importance and in their marginals.
The marginals are robust to discontinuities and multiple peaks.

### Utopia Distance and Prioritization
Since the utopia point is set to (0,0), the distance to the utopia point is simple to calculate from the normalized coordinates. Acceptable points are those that prioritize the first objective by at least 80%, which can be determined by the angle theta.

Load the original dataset.

```{r Utopia Distance: Load Data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: the names are:
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}
```

```{r Utopia Distance: First Iteration, message=FALSE, warning=FALSE}
# Search for the first point
GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.radan(x, model.f1 = mod.rad, model.f2 = mod.ang)},
             lower = c(0, 0, 0), upper = c(1, 1, 1),
             popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
res = ZDT4.mod(x = as.matrix(GPar.new))
GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
# Add point to the samples
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.front = res$front
GPar.all = res$data
rm(res)
GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.rad.init = mod.rad
mod.ang.init = mod.ang

# Grid of the relevant region to visualize
lower = c(0, 0, 0); upper = c(1, 1, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2, seq(from = lower[2], to = upper[2], length.out = 25)),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(GPar.new$x3, seq(from = lower[3], to = upper[3], length.out = 25)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Apply Kriging functions
res = predict(object = mod.rad.init, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang.init, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
               mapping = aes(x = x1, y = x2, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info*(prob)*(1-prob), color = info*(prob)*(1-info)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3),
               mapping = aes(x = x1, y = x3, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[3]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info*(prob)*(1-prob), color = info*(prob)*(1-prob)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", 
       shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

# Sometimes the grid scan finds a point that is slightly better than the perceived optimum due to the convergence criteria. In that event, replace the starting point for the cutoff criteria for iteration
start.fit = max(start.fit, max(fine.grid$inf*fine.grid$prob*(1-fine.grid$prob)))

rm(g1, g2, g3, fine.grid)
```

```{r Utopia Distance: Repeated Iterations, message=FALSE, warning=FALSE}
budget = 30
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Create a Kriging model for the distance
  mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
  mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.radan(x, model.f1 = mod.rad, model.f2 = mod.ang)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]

  # Add point to the samples
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.front = res$front
  GPar.all = res$data
  rm(res)
  GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

write.csv(GPar.all, 'GPar_Accept_Radius.csv')

rm(res, cl, point.next)
```

```{r Utopia Distance: Plot after iterations}
GPar.all = read.csv(file = 'GPar_Accept_Radius.csv')
# Plot results on top of the most recent Kriging model
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

# Grid of the relevant region to visualize
lower = c(0, 0, 0); upper = c(1, 1, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2, seq(from = lower[2], to = upper[2], length.out = 25)),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(GPar.new$x3, seq(from = lower[3], to = upper[3], length.out = 25)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Apply Kriging functions
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
               mapping = aes(x = x1, y = x2, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info*(prob)*(1-prob), color = info*(prob)*(1-info)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3),
               mapping = aes(x = x1, y = x3, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[3]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info*(prob)*(1-prob), color = info*(prob)*(1-prob)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", 
       shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

# x3 slices
sz.fine = 100
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = sz.fine)
x3.rng = seq(from = 0, to = 1, length.out = 9) # Simple slice for visualization
fine.grid = expand.grid(x1.rng, x2.rng, x3.rng)
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Calculate
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd
# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = prob)) +
  labs(x = expression('x'[1]), y = expression('x'[2]), color = expression('P[accept]')) +
  theme_classic() + facet_wrap(~x3) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

# x2 slices
sz.fine = 100
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x3.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = 9) # Simple slice for visualization
fine.grid = expand.grid(x1.rng, x2.rng, x3.rng)
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Calculate
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd
# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x3, color = prob)) +
  labs(x = expression('x'[1]), y = expression('x'[3]), color = expression('P[accept]')) +
  theme_classic() + facet_wrap(~x2) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

rm(g1, g2, g3, fine.grid, res)
```

```{r Utopia distance: Marginalization}
# Load data: compare before/after refinement and with a random sampling for the refinement
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Threshold.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 1),
                        x2 = runif(n = nsamp, min = 0, max = 1),
                        x3 = runif(n = nsamp, min = 0, max = 1))
res = ZDT4.mod(x = as.matrix(GPar.rando))
GPar.rando$f1 = res[,1]; GPar.rando$f2 = res[,2]
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Add the $rad calculations
GPar.start$rad = sqrt(GPar.start$f1.norm^2 + GPar.start$f2.norm^2)
GPar.adapt$rad = sqrt(GPar.adapt$f1.norm^2 + GPar.adapt$f2.norm^2)
GPar.rando$rad = sqrt(GPar.rando$f1.norm^2 + GPar.rando$f2.norm^2)

# Use the final GP model with the full dataset
mod.rad.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

mod.rad.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

mod.rad.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 1500 samples each.
Infer.radan.start = marginal.rad(mod.rad.start, mod.ang.start)
Infer.radan.adapt = marginal.rad(mod.rad.adapt, mod.ang.adapt)
Infer.radan.rando = marginal.rad(mod.rad.rando, mod.ang.rando)

# Compare to true marginal
radan.tru = read.csv('ExpectedMarginal_radan.csv')

ggplot() +
  geom_path(data = radan.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = radan.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.radan.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.radan.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.radan.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~., scales = 'free_y') +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = 'Utopia Distance + Priority',
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability',
                                  'start' = 'Before Sampling',
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination.
expect1 = filter(radan.tru, var == 'x1')$prob
expect2 = filter(radan.tru, var == 'x2')$prob
expect3 = filter(radan.tru, var == 'x3')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.radan.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.radan.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.radan.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.radan.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.radan.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.radan.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2),
           R2.x3 = c(cor(x = filter(Infer.radan.start, var == 'x3')$prob, y = expect3, method = 'pearson')^2, 
             cor(x = filter(Infer.radan.adapt, var == 'x3')$prob, y = expect3, method = 'pearson')^2,
             cor(x = filter(Infer.radan.rando, var == 'x3')$prob, y = expect3, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.radan.adapt, var == 'x1')
Infer.x2 = filter(Infer.radan.adapt, var == 'x2')
Infer.x3 = filter(Infer.radan.adapt, var == 'x3')
Import.radan = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd),
                                     diff(range(Infer.x3$prob))*nrow(Infer.x3)/sum(Infer.x3$psd)),
                          var = c('x1', 'x2', 'x3'),
                          typ = 'radan')
# Uncertainty in the calculation based on the Monte Carlo error
Import.radan$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)),
                    max(Infer.x3$prob)*(1-max(Infer.x3$prob)) + min(Infer.x3$prob)*(1-min(Infer.x3$prob)) * 
                      diff(range(Infer.x3$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd), var(Infer.x3$psd))/nrow(Infer.x1) )
  
ggplot(Import.radan) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymin = import - sd, ymax = import + sd), width = 0.5) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Threshold radanf Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4', '#fdc086'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3);
radan.tru$cat = 'tru';          Infer.radan.start$cat = 'start'
Infer.radan.adapt$cat = 'adapt'; Infer.radan.rando$cat = 'rando'
write.csv(rbind(radan.tru[, names(radan.tru) %in% names(Infer.radan.start)],
      Infer.radan.start[, names(Infer.radan.start) %in% names(radan.tru)],
      Infer.radan.adapt[, names(Infer.radan.adapt) %in% names(radan.tru)],
      Infer.radan.rando[, names(Infer.radan.rando) %in% names(radan.tru)]), 'Marginals_radan_tru.csv')
```

## Comparison of selection criteria by their marginals

```{r}
Margin.del = read.csv(file = 'Marginals_delta_tru.csv')
Margin.cut = read.csv(file = 'Marginals_cutof_tru.csv')
Margin.rad = read.csv(file = 'Marginals_radan_tru.csv')

Margin.del$criteria = 'Pareto Distance'
Margin.cut$criteria = 'Objective Cutoff'
Margin.rad$criteria = 'Utopia Distance'

Margin = rbind(Margin.del, Margin.cut, Margin.rad)

ggplot() +
  geom_path(data = filter(Margin, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Margin, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Margin, cat != 'tru'), mapping = aes(x = x, y = prob, color = cat)) +
  facet_grid(var~criteria, scales = 'free_y') + 
  labs(x = 'Input Value', y = 'Probability of Acceptance', subtitle = 'Candidate Screening', 
       color = 'Partial\nConditions') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + 
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), 
                     labels = c('0.0', '0.25', '0.5', '0.75', '1.0')) +
  theme_bw() + theme(panel.spacing = unit(0.7, "lines")) +
  scale_color_manual(labels = c('tru' = 'Expected Probability',
                                  'start' = 'Before Sampling',
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

```

Regardless of the selection criteria, the marginals appear to be close to the expected values, particularly after adaptive sampling. However, the impact seems relatively small, likely because the process of finding the Pareto frontier also samples these good-performance regions because they all contain (most of) the Pareto frontier.
