---
title: "Proof of concept: Polynomial equation"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook showcases the method below for finding acceptable suboptimal performance in a quartic bi-objective function based on the quadratic problem from Marler and Arora 2010.
The quartic function adds the possible complication of a local optimum, which means the near-Pareto set may be discontinuous.
The intention is for this procedure to be applied to optimization problems where the input variables are discrete but unknown, such as selecting appropriate chemical compounds from a large set of options, and guide decisionmaking by indicating which variables are most important and what values it should have to yield optimal performance.

# Algorithm concept:
* I. Setup: find the Pareto frontier of the 2D optimization problem
1. Solve objective functions with a space filling method to obtain an initial set of data.
2. Use GPareto to find the Pareto front.
3. Calculatate the normalized distance in the objective space and f1/f2 prioritization of each point to the Pareto front. These two metrics will be used to establish the acceptable suboptimal performance and the relative importance of each input variable.

* II. Acceptable suboptimal performance (the "near-Pareto set")
4. Establish a selection criteria for acceptable suboptimal performance (eg. within some tolerance of the Pareto front).
5. Using Gaussian processes, create a new objective called the "sample utility" function that describes the probability of meeting the desired distance and objective function criteria, multiplied by the variance.
6. Sample new points iteratively by maximizing the sample utility function. Repeat until the computational budget has been expended or the sample utility function maximum drops below a specified threshold, often relative to first maximum.

* III. Feature importance of each input variable
7. Calculate the conditional probability that a point will fall within the near-Pareto set given only information about that input variable.
8. Calculate the variance in the probability of the 1-variable conditional probability estimate. This variance represents the contribution due to all of the other variables in question.
9. The feature importance metric is the range of the probability divided by the integral of the variance. The greater the range of probabilities, the more the single variable can drive the likelihood, but this should be normalized by the amount that the other variables contribute.

* IV: Suggested ranges
10. For the most important variable, determine the input values that give the peak response. This is the suggested input range.
11. For the next most important variable, calculate the conditional probability if this variable is known precisely, but the most important is known to be within its suggested range.
12. Repeat step 11 by continuing to constrain the optimal range of the previous variables until all conditional probabilities have been found. This sequential analysis describes the characteristics of the most promising inputs for the near-Pareto set.


# Code
## 0. Initialization

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
```

```{r Functions: Objectives}
# Set (x1, x2) on range of [0, 5]

# Objective functions are based on the quadratic functions used previously, but with an additional local minimum
f1 = function(x1, x2){
  return(20*(x1 - 0.75)^2 + 190 + 11.58*x2^4 - 115.85*x2^3 + 383.13*x2^2 - 463.50*x2)
}
# The second objective function is also partially rotated so the local optimum is not perfectly aligned
f2 = function(x1, x2){
  # Remap both variables: rotate 30 degrees counterclockwise
  ang = -pi/24
  n1 = x1*cos(ang) - x2*sin(ang)
  n2 = x1*sin(ang) + x2*cos(ang)
  # return((x1 - 2.5)^2 + 80 + 1.778*x2^4 - 20*x2^3 + 78.573*x2^2 - 124.664*x2)
  return((n1 - 2.5)^2 + 80 + 1.778*n2^4 - 20*n2^3 + 78.573*n2^2 - 124.664*n2)
}


# Using GPareto: Need inputs and outputs as vectors/matrices not as dataframes. Coarse initial design
fun = function(x){
  x1 = x[1]; x2 = x[2]
  return(c(f1(x1, x2), f2(x1, x2)))
}

```

```{r Functions: Normalization}
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))

  return(GPar.data)
}

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  # Check if the angle is the same as a point on the Pareto front
  if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}

```

```{r Functions: Iterative Sampling}
# Model construction
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 # covtyp = 'gauss', # Gaussian uncertainty
                 # optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50), # Increase robustness
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}

fill.sample.delta = function(x, model){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res = predict(object = model, newdata = data.frame(x1 = x[1], x2 = x[2]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(res$sd*(prob*(1-prob) + 0.25/99))
  # Offset on the uncertainty so that 0 uncertainty (p = 0 or 1) does not give a 0 result; 
  # instead these points give a 1 order of magnitude less weight
}

fill.sample.cutoff = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), 
  # and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 1, sd = res.f1$sd) * 
    pnorm(q = 0, mean = res.f2$mean - 1, sd = res.f2$sd)
  
  # Variance based on propagation of errors, assuming independent measures
  sd = (res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2)

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(sd*(prob*(1-prob) + 0.25/99))
}

fill.sample.radan = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 1, sd = res.f1$sd) * (1 - pnorm(q = 0, mean = res.f2$mean - 20, sd = res.f2$sd))
  
  # Variance based on propagation of errors
  sd = (res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2) 

  # Objective result. Adjust the probability weight so the maximum is only 2 orders of magnitude greater than the minimum.
  return(sd*(prob*(1-prob) + 0.25/99))
}

fill.sample.update = function(GPar.front.old, GPar.data, GPar.new){
  # Update the dataset and Pareto front with the new datapoint
  
  # Update the Pareto front
  # The new point will only have the inputs and direct outputs
  GPar.front.new = rbind(GPar.front.old[, names(GPar.front.old) %in% names(GPar.new)], GPar.new)
  pos.front = t(nondominated_points(points = t(as.matrix(GPar.front.new[, c('f1', 'f2')]))))
  GPar.front.new = filter(GPar.front.new, f1 %in% pos.front[,1], f2 %in% pos.front[,1])
  
  # If the new point is a new minimum for either f1 or f2, then need to re-calculate all of the normalized values
  if(which.min(c(GPar.new$f1, GPar.front.old$f1)) %in% 1:nrow(GPar.new) | 
     which.min(c(GPar.new$f2, GPar.front.old$f2)) %in% 1:nrow(GPar.new) ){
    # Update front
    f1.f2.update = n.obj(GPar.data = GPar.front.new, GPar.front = GPar.front.new)
    GPar.front.new$f1.norm = f1.f2.update$f1.norm; GPar.front.new$f2.norm = f1.f2.update$f2.norm
    GPar.front.new$theta = atan(GPar.front.new$f2.norm/GPar.front.new$f1.norm)*180/pi*10/9
    # Update dataset
    f1.f2.update = n.obj(GPar.data = GPar.data, GPar.front = GPar.front.new)
    GPar.data$f1.norm = f1.f2.update$f1.norm; GPar.data$f2.norm = f1.f2.update$f2.norm
    GPar.data$theta = atan(GPar.data$f2.norm/GPar.data$f1.norm)*180/pi*10/9
    } else{GPar.front.new = GPar.front.old}
  # Get normalized values for new points
  GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front.new)
  GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
  
  # If any new points are on the Pareto front at all, then update the distance and theta
  new.pareto = nrow(filter(GPar.new, f1 %in% pos.front[,1], f2 %in% pos.front[,1]))
  if(new.pareto > 0){ # Need to update all points in the front
    dist = c()
    for(i in 1:nrow(GPar.data)){
      dist = c(dist, n.dist(f1.norm = GPar.data$f1.norm[i], f2.norm = GPar.data$f2.norm[i], GPar.front = GPar.front.new))
    }
    GPar.data$dist = dist
  } 
  # Need to calculate the new point's distance
  dist = c()
  for(i in 1:nrow(GPar.new)){
    dist = c(dist, n.dist(f1.norm = GPar.new$f1.norm[i], f2.norm = GPar.new$f2.norm[i], GPar.front = GPar.front.new))
  }
  GPar.new$dist = dist
  GPar.new$order = max(GPar.data$order) + 1
  
  # Add the new point
  GPar.data = rbind(GPar.data[, names(GPar.data) %in% names(GPar.new)], GPar.new)
  
  return(list(front = GPar.front.new, data = GPar.data, new = GPar.new))
}

```

```{r Functions: Marginalization}
marginal.dist = function(mod.dist){
  # Calculate the marginal distributions for x1 and x2 given a model
  resolution = 50; MCsamp = 1500
  x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2)
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
    res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], 
                                          prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          var = 'x1', ncond = 'None'))  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
    res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], 
                                          psd = sd(fill.frame$p.del1),
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          prob = mean(fill.frame$p.del1),
                                          var = 'x2', ncond = 'None'))
  }
  return(rbind(Infer.x1, Infer.x2))
}

marginal.cut = function(mod.f1, mod.f2){
  resolution = 50; MCsamp = 1500
  x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2)
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
    res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          var = 'x1', ncond = 'None'))  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
    res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1),
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          var = 'x2', ncond = 'None'))
  }
  return(rbind(Infer.x1, Infer.x2))
}

marginal.rad = function(mod.rad, mod.ang){
  resolution = 50; MCsamp = 1500
  x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2)
  x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
  x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
  
  Infer.x1 = data.frame(); Infer.x2 = data.frame()
  
  for(i in 1:resolution){
    # x1
    fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
    res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
      (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
    Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], 
                                          prob = mean(fill.frame$p.del1), 
                                          psd = sd(fill.frame$p.del1), 
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          var = 'x1', ncond = 'None'))  
  
    # x2
    fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
    res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
    res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
    fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
      (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
    Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], 
                                          p25 = quantile(x = fill.frame$p.del1, probs = 0.25),
                                          p75 = quantile(x = fill.frame$p.del1, probs = 0.75),
                                          prob = mean(fill.frame$p.del1),
                                          psd = sd(fill.frame$p.del1),
                                          var = 'x2', ncond = 'None'))
  }
  return(rbind(Infer.x1, Infer.x2))
}

```

Since the test function is very easy to calculate, the results can be compared to an estimate by Monte Carlo sampling.
This is calculated ahead of time and stored for later.

```{r Expected results}
# Approximate the true result by with a grid. Use the same grid size as the resolution for the marginalization measures
grid.sz = 50
fine.grid = expand.grid(x1 = seq(from = 0, to = 5, length.out = grid.sz), 
                        x2 = seq(from = 0, to = 5, length.out = grid.sz))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')

# Calculate the actual results
fine.grid$f1 = f1(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid$f2 = f2(x1 = fine.grid$x1, x2 = fine.grid$x2)

# Obtain the Pareto front
pos.front = t(nondominated_points(points = t(as.matrix(fine.grid[, c('f1', 'f2')]))))
front = filter(fine.grid, f1 %in% pos.front[,1], f2 %in% pos.front[,2])

fine.grid = n.obj(GPar.data = fine.grid, GPar.front = front)
front = n.obj(GPar.data = front, GPar.front = front)
fine.grid$dist = NaN
for(row in 1:nrow(fine.grid)){
  fine.grid$dist[row] = n.dist(f1.norm = fine.grid$f1.norm[row], 
                               f2.norm = fine.grid$f2.norm[row], 
                               GPar.front = front)
}
fine.grid$ang = atan(fine.grid$f2.norm/fine.grid$f1.norm)*180/pi*10/9
fine.grid$rad = sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2)

# Calculate the marginals
# Integrate
delta.tru = data.frame()
cutof.tru = data.frame()
radan.tru = data.frame()
for(x in unique(fine.grid$x1)){
  # Only the single value of x1 or x2
  sub1 = filter(fine.grid, x1 == x)
  sub2 = filter(fine.grid, x2 == x)
  
  # Distance criteria: Pareto distance less than 1
  p1 = nrow(filter(sub1, dist <= 1)) / nrow(sub1)
  p2 = nrow(filter(sub2, dist <= 1)) / nrow(sub2)
  p = c(p1, p2)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  delta.tru = rbind(delta.tru, data.frame(x = x, prob = c(p1, p2), psd = psd, var = c('x1', 'x2')))
  
  # Cutoff criteria: normalized value less than 1 for both f1 and f2
  p1 = nrow(filter(sub1, f1.norm <= 1, f2.norm <= 1)) / nrow(sub1)
  p2 = nrow(filter(sub2, f1.norm <= 1, f2.norm <= 1)) / nrow(sub2)
  p = c(p1, p2)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  cutof.tru = rbind(cutof.tru, data.frame(x = x, prob = c(p1, p2), psd = psd, var = c('x1', 'x2')))
  
  # Radius-angle criteria: utopia distance less than 1, angle above 20%
  p1 = nrow(filter(sub1, rad <= 1, ang >= 20)) / nrow(sub1)
  p2 = nrow(filter(sub2, rad <= 1, ang >= 20)) / nrow(sub2)
  p = c(p1, p2)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  radan.tru = rbind(radan.tru, data.frame(x = x, prob = c(p1, p2), psd = psd, var = c('x1', 'x2')))
  
}
rm(sub1, sub2, p1, p2, p, psd, fine.grid, pos.front, front, grid.sz)

# Store for later
write.csv(delta.tru, 'ExpectedMarginal_delta.csv')
write.csv(cutof.tru, 'ExpectedMarginal_cutof.csv')
write.csv(radan.tru, 'ExpectedMarginal_radan.csv')
```


## I. Setup: 
Find the Pareto frontier of the 2D optimization problem

1. Solve objective functions with a space filling method to obtain an initial set of data.
The 2D function takes two inputs, defined $x_1$ and $x_2$, to produce two outputs $f_1$ and $f_2$. 
The domain of both $x_1$ and $x_2$ is [0,5].

The initial search obtains the extremes of the 4 corners, the center points on each edge, the central point, and 3*(number of inputs) random points in each quadrant.

```{r Grid Setup}
# Set up initial design
sz = 3
x1.rng = seq(from = 0, to = 5, length.out = sz)
x2.rng = seq(from = 0, to = 5, length.out = sz)
des.start = expand.grid(x1.rng, x2.rng)
des.start = des.start[,1:2]
names(des.start) = c('x1', 'x2')
npt = 3
des.start = rbind(des.start,
                  data.frame(x1 = runif(n = length(names(des.start))*npt, min = 0, max = 2.5),
                             x2 = runif(n = length(names(des.start))*npt, min = 0, max = 2.5)),
                  data.frame(x1 = runif(n = length(names(des.start))*npt, min = 0, max = 2.5),
                             x2 = runif(n = length(names(des.start))*npt, min = 2.5, max = 5)),
                  data.frame(x1 = runif(n = length(names(des.start))*npt, min = 2.5, max = 5),
                             x2 = runif(n = length(names(des.start))*npt, min = 0, max = 2.5)),
                  data.frame(x1 = runif(n = length(names(des.start))*npt, min = 2.5, max = 5),
                             x2 = runif(n = length(names(des.start))*npt, min = 2.5, max = 5)))
# Solve
des.start$f1 = f1(x1 = des.start$x1, x2 = des.start$x2)
des.start$f2 = f2(x1 = des.start$x1, x2 = des.start$x2)

Pareto.budget = min(100, 2*nrow(des.start)) # Want a clearly defined Pareto front
res.start = matrix(c(des.start$f1, des.start$f2), ncol = 2, nrow = nrow(des.start))
des.start = matrix(c(des.start$x1, des.start$x2), ncol = 2, nrow = nrow(des.start))

```

2. Use GPareto to find the Pareto front.

GPareto uses Gaussian processes to approximate the Pareto frontier, defaulting to the SMS-EGO selection criterion.
The function produces both the Pareto frontier estimate and the set of iterations, which will both be used in subsequent steps.
To simplify computation time, the data are stored in .csv files so these variables do not need to be calculated with each attempt.

```{r GPareto Calculation}
res = easyGParetoptim(fn = fun, budget = Pareto.budget, lower = c(0, 0), upper = c(5, 5), 
                      par = des.start, value = res.start, ncores = 2)
plotGPareto(res)
# Format into dataframe for easier plotting
GPar.front = data.frame(x1 = res$par[,1], x2 = res$par[,2], f1 = res$value[,1], f2 = res$value[,2])
GPar.all =  data.frame(x1 = res$history$X[,1], x2 = res$history$X[,2], f1 = res$history$y[,1], f2 = res$history$y[,2])
GPar.all$order = c(rep(0, nrow(des.start)), seq(from = 1, to = Pareto.budget, by = 1))
rm(res)
write.csv(GPar.all, file = 'GPar_all_data.csv')
write.csv(GPar.front, file = 'GPar_fnt_data.csv')
```

3. Calculatate the normalized distance in the objective space and f1/f2 prioritization of each point to the Pareto front. These two metrics will be used to establish the acceptable suboptimal performance and the relative importance of each input variable.

Normalization is defined such that (0,0) is the utopia point and (1,0) and (0,1) are the single-objective optimizations. The prioitization is the angle $\theta$ made between the point in the normalized objective space and the x-axis, i.e. the axis of purely prioritizing objective 2. This angle is re-mapped to a percentage from 0 to 100% prioritization of objective 2. The distance between each point and the Pareto front is the distance in the normalized space

```{r Map to Normalized Objective Space}
GPar.all = read.csv(file = 'GPar_all_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')

# Saved data have an index
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]
GPar.front = GPar.front[, !(names(GPar.front) %in% c("X"))]
# Run normalization
GPar.all = n.obj(GPar.data = GPar.all, GPar.front = GPar.front)
GPar.front = n.obj(GPar.data = GPar.front, GPar.front = GPar.front)
GPar.front = GPar.front[order(GPar.front$f1.norm),]

# Calculate theta according to the angle. Remap from [0, pi/2] to [0, 100] for simplicity
GPar.all$theta = atan(GPar.all$f2.norm/GPar.all$f1.norm)*180/pi*10/9

# Run distance calculations in parallel to speed up
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.all)) %dopar%
  n.dist(f1.norm = GPar.all$f1.norm[row], f2.norm = GPar.all$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.all$dist = unlist(dist)
rm(dist)


# Save the starting data to test multiple types of acceptance criteria
write.csv(x = GPar.all, file = 'GPar_all_start.csv')
write.csv(x = GPar.front, file = 'GPar_fnt_start.csv')

```

Plot the contours of the original functions
```{r Plot Objective Functions}
# Fine grid spacing
sz.fine = 50
x1.rng = seq(from = 0, to = 5, length.out = sz.fine)
x2.rng = seq(from = 0, to = 5, length.out = sz.fine)
con.fine = expand.grid(x1.rng, x2.rng)
con.fine = con.fine[,1:2]
names(con.fine) = c('x1', 'x2')

# Calculate
con.fine$f1 = f1(x1 = con.fine$x1, x2 = con.fine$x2)
con.fine$f2 = f2(x1 = con.fine$x1, x2 = con.fine$x2)

ggplot() +
  geom_contour(data = con.fine, mapping = aes(x = x1, y = x2, z = log10(f1), color = 'f1'), bins = 10) +
  geom_contour(data = con.fine, mapping = aes(x = x1, y = x2, z = log10(f2), color = 'f2'), bins = 10) +
  labs(x = expression('x'[1]), y = expression('x'[2])) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2), color = "black", alpha = 0.5) +
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = "black") +
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2), color = "black", method = 'loess', formula = (y~x), 
              level = 0.95) +
  scale_color_manual(name = 'Objective', values = c('f1' = 'red', 'f2' = 'blue'), 
                     labels = c('f1' = expression('F'[1]), 'f2' = expression('F'[2]))) +
  theme(legend.position = c(0.9, 0.85), legend.background = element_rect(fill = 'white')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  theme_classic()

```

Plot the full dataset in the objective space
```{r Visualize GPareto and Normalization, fig.width=6, fig.height=8}
gplt.start = ggplot() +
  geom_hline(yintercept = 0, linetype = 'dotted', color = 'black') + geom_vline(xintercept = 0, linetype = 'dotted', color = 'black') +
  geom_point(data = filter(GPar.all, order == 0, f2 < 50), mapping = aes(x = f1, y = f2, color = (order > 0))) +
  labs(x = "Function 1", y = "Function 2", color = "") +  
  scale_color_discrete(labels = c('Starting Data', 'Pareto Search'), breaks = c(FALSE, TRUE)) +
  theme_classic() + theme(legend.position = c(0.85, 0.85), legend.background = element_rect(fill = 'white'), legend.title = element_blank())

gplt.GPar = ggplot() +
  geom_hline(yintercept = 0, linetype = 'dotted', color = 'black') + geom_vline(xintercept = 0, linetype = 'dotted', color = 'black') +
  geom_line(data = GPar.front, mapping = aes(x = f1, y = f2), color = "black") +
  geom_point(data = filter(GPar.all, f2 < 50), mapping = aes(x = f1, y = f2, color = (order > 0))) +
  labs(x = "Function 1", y = "Function 2", color = "") +  
  scale_color_discrete(labels = c('Starting Data', 'Pareto Search'), breaks = c(FALSE, TRUE)) +
  theme_classic() + theme(legend.position = c(0.85, 0.85), legend.background = element_rect(fill = 'white'), legend.title = element_blank())

gplt.Norm = ggplot() +
  geom_hline(yintercept = 0, linetype = 'dotted', color = 'black') + geom_vline(xintercept = 0, linetype = 'dotted', color = 'black') +
  geom_line(data = GPar.front, mapping = aes(x = f1.norm, y = f2.norm), color = "black") +
  geom_point(data = filter(GPar.all, f2 < 50), mapping = aes(x = f1.norm, y = f2.norm, color = (order > 0))) +
  labs(x = "Normalized Function 1", y = "Normalized Function 2", color = "") +  
  scale_color_discrete(labels = c('Starting Data', 'Pareto Search'), breaks = c(FALSE, TRUE)) +
  scale_x_continuous(breaks = c(seq(from = 0, to = 6, by = 2), 1)) +
  scale_y_continuous(breaks = c(seq(from = 0, to = 6, by = 2), 1)) +
  theme_classic() + theme(legend.position = c(0.85, 0.85), legend.background = element_rect(fill = 'white'), legend.title = element_blank())

gplt.start / gplt.GPar / gplt.Norm
rm(gplt.start, gplt.GPar, gplt.Norm)
```

## II. Acceptable suboptimal performance (the "near-Pareto set")
4. Establish a selection criteria for acceptable suboptimal performance (eg. within some tolerance of the Pareto front).

For this example, defines $\delta$ as the distance between the point and the point on the Pareto frontier with the same value of theta. Accept only points with $\delta < 1$.

```{r Load data: Accept Distance less than 1}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: the names are:
nm = c("x1",  "x2", "f1", "f2", "order",  "f1.norm", "f2.norm", "dist", "theta")
GPar.all = GPar.all[,nm]
nm = c("x1",  "x2", "f1", "f2",  "f1.norm", "f2.norm")
GPar.front = GPar.front[,nm]

```

5. Using Gaussian processes, create a new objective called the "sample utility" function that describes the probability of meeting the desired distance and objective function criteria, multiplied by the variance.

The sample utility function prioritizes points that are likely to describe the boundary (normalized distance equal to 1) with the greatest model uncertainty at the time. 
When the point is sampled, this means information gained is maximized. 
Points on the boundary have intermediate acceptance probabilities, ie. close to 0.5, while points with the greatest model uncertainty have the greatest standard error. 
Therefore, the utility function can be estimated as the product of the standard error, the probability of being accepted, and the probability of being rejected.

Visualize the first iteration to show that it actually samples the point that is most useful
```{r Iteration 1: Distance less than 1, message=FALSE, warning=FALSE}
# Search for the first point
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'dist')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.delta(x, model = mod.dist)},
             lower = c(0, 0), upper = c(5, 5),
             popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)

# Update the Pareto front and add the new point
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.all = res$data
GPar.front = res$front

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.dist.init = mod.dist
```

```{r Iteration 1: Plotting}
# Grid of the relevant region to visualize
lower = c(0, 0); upper = c(5,5)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = 50))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist.init, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 1, sd = fine.grid$dist.sd)

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (dist.p1)*(1-dist.p1)-1e-10, color = (dist.p1)*(1-dist.p1)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.p1), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.sd, color = dist.sd), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.sd*(dist.p1)*(1-dist.p1), color = dist.sd*(dist.p1)*(1-dist.p1)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), 
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

```

6. Sample new points iteratively by maximizing the sample utility function. Repeat until the computational budget has been expended or the sample utility function maximum drops below a specified threshold, often relative to first maximum.

The threshold is when the sample utility function's maximum falls below 5% of the sample utility of the first iteration.

```{r Iteration loop: Accept distance less than 1, message=FALSE, warning=FALSE}
budget = 20
next.fit = start.fit
while(next.fit > start.fit*0.05 & max(GPar.all$order) < budget+Pareto.budget){
  # Create new Kriging model
  mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'dist')
  
  # Run the optimization to find the best point
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.delta(x, model = mod.dist)},
               lower = c(0, 0), upper = c(5, 5),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
  GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
  GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)
  
  # Fill in the remaining caluclations: normalized outputs, distance, theta, order
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.all = res$data
  GPar.front = res$front
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

# Save the data for later
write.csv(GPar.all, 'GPar_Accept_Delta1.csv')

```

```{r}
GPar.all = read.csv(file = 'GPar_Accept_Delta1.csv')
# Plot results on top of the most recent Kriging model
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'dist')

lower = c(0, 0); upper = c(5,5)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = 50))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 1, sd = fine.grid$dist.sd)

ncolor = 7; pt.size = 2.5
g4 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (dist.p1)*(1-dist.p1)-1e-10, 
                                                      color = (dist.p1)*(1-dist.p1)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor))+ # Need to expand the range to include the edge cases
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.p1), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty',
       color = 'Collection\nProcess', shape = 'Collection\nProcess') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color = FALSE, shape = FALSE, fill = FALSE)

g5 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.sd, color = dist.sd), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = expression("x"[1]), y = "", color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Information Gain') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + 
  guides(color = FALSE, shape = FALSE, fill = FALSE)
  
g6 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = dist.sd*(dist.p1)*(1-dist.p1), color = dist.sd*(dist.p1)*(1-dist.p1)), 
                      bins = ncolor, alpha = 0.5) +
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = '', y = '', color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Sample Utility', fill = '') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + guides(color=guide_legend(override.aes=list(fill=NA)))

g4 + g5 + g6

```


## III. Feature importance of each input variable

7. Calculate the conditional probability that a point will fall within the near-Pareto set given only information about that input variable.

From the above calculations, the most important variable is x1, as it has a stronger relationship between distance in the objective space and distance in the input space (i.e. a larger perturbation in the input will lead to a less optimal result).

For the purposes of illustration, both P[accept | $x_1$] and P[accept | $x_2$] will be shown to show how the regression is consistent with the probabilities.
The conditional probability is calculated by marginalization of the GP probabilities. From the GP, the value of P[accept | $x_1, x_2$] can be calculated, and the relationship between P[accept | $x_1, x_2$] and  P[accept | $x_1$] is the integral:

P[accept | $x_1$] = integral( P[accept | $x_1, x_2$]  P[$x_2$] $dx_2$ )

In the absence of data to inform the distribution of $x_2$, this distribution is set as a uniform distribution on the range used in the optimization.

8. Calculate the variance in the probability of the 1-variable conditional probability estimate. This variance represents the contribution due to all of the other variables in question.

9. The feature importance metric is the range of the probability divided by the integral of the variance. The greater the range of probabilities, the more the single variable can drive the likelihood, but this should be normalized by the amount that the other variables contribute.

```{r Feature Importance: 1-Variable Marginalization}
# Load data
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Delta1.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                        x2 = runif(n = nsamp, min = 0, max = 5))
GPar.rando$f1 = f1(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
GPar.rando$f2 = f2(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Use the final GP model with the full dataset
mod.dist.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2'), output.name = 'dist')
mod.dist.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2'), output.name = 'dist')
mod.dist.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2'), output.name = 'dist')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 1500 samples each.
Infer.dist.start = marginal.dist(mod.dist.start)
Infer.dist.adapt = marginal.dist(mod.dist.adapt)
Infer.dist.rando = marginal.dist(mod.dist.rando)

delta.tru = read.csv('ExpectedMarginal_delta.csv')

ggplot() +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.dist.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.dist.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.dist.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~.) +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability', 
                                  'start' = 'Before Sampling', 
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination.
expect1 = filter(delta.tru, var == 'x1')$prob
expect2 = filter(delta.tru, var == 'x2')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.dist.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.dist.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.dist.adapt, var == 'x1')
Infer.x2 = filter(Infer.dist.adapt, var == 'x2')
Import.delta = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x1)/sum(Infer.x2$psd)),
                          var = c('x1', 'x2'),
                          typ = 'delta')
# Uncertainty in the calculation based on the Monte Carlo error
Import.delta$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd))/nrow(Infer.x1) )
  
ggplot(Import.delta) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Pareto Distance Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2);
# write.csv(Infer.plt, 'Marginals_delta.csv')
delta.tru$cat = 'tru';          Infer.dist.start$cat = 'start'
Infer.dist.adapt$cat = 'adapt'; Infer.dist.rando$cat = 'rando'
write.csv(rbind(delta.tru[, names(delta.tru) %in% names(Infer.dist.start)],
      Infer.dist.start[, names(Infer.dist.start) %in% names(delta.tru)],
      Infer.dist.adapt[, names(Infer.dist.adapt) %in% names(delta.tru)],
      Infer.dist.rando[, names(Infer.dist.rando) %in% names(delta.tru)]), 'Marginals_delta_tru.csv')

```

While adaptive sampling improves the results compared to without any additional samples, random sampling actually appears to improve the result more for $x_1$.
The expected behavior, where adaptive sampling leads to a better estimate than random sampling, is true for $x_2$.
This is largely due to issues with the estimation of the Pareto front, leading to propagation of errors in the distance and therefore in the resulting models.

For comparison, finding what the expected marginalization behavior would be should it use the same Pareto front as that estimated by the data:

```{r 1-Variable Marginalization: Compare to Expected Result, Estimated Pareto Front}
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
grid.sz = 50
fine.grid = expand.grid(x1 = seq(from = 0, to = 5, length.out = grid.sz), 
                        x2 = seq(from = 0, to = 5, length.out = grid.sz))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')

# Calculate the actual results
fine.grid$f1 = f1(x1 = fine.grid$x1, x2 = fine.grid$x2)
fine.grid$f2 = f2(x1 = fine.grid$x1, x2 = fine.grid$x2)
# Normalize
fine.grid = n.obj(GPar.data = fine.grid, GPar.front = GPar.front)
fine.grid$dist = NaN
for(row in 1:nrow(fine.grid)){
  fine.grid$dist[row] = n.dist(f1.norm = fine.grid$f1.norm[row], 
                               f2.norm = fine.grid$f2.norm[row], 
                               GPar.front = GPar.front)
}
fine.grid$ang = atan(fine.grid$f2.norm/fine.grid$f1.norm)*180/pi*10/9
fine.grid$rad = sqrt(fine.grid$f1.norm^2 + fine.grid$f2.norm^2)

# Calculate the marginals
# Integrate
delta.tru = data.frame()
for(x in unique(fine.grid$x1)){
  # Only the single value of x1 or x2
  sub1 = filter(fine.grid, x1 == x)
  sub2 = filter(fine.grid, x2 == x)
  
  # Distance criteria: Pareto distance less than 1
  p1 = nrow(filter(sub1, dist <= 1)) / nrow(sub1)
  p2 = nrow(filter(sub2, dist <= 1)) / nrow(sub2)
  p = c(p1, p2)
  # Monte carlo error = sqrt(p*(1-p)/samples)
  psd = sqrt(p * (1-p) / nrow(sub1))
  # Add to data
  delta.tru = rbind(delta.tru, data.frame(x = x, prob = c(p1, p2), psd = psd, var = c('x1', 'x2')))

}
rm(sub1, sub2, p1, p2, p, psd, fine.grid, grid.sz)

ggplot() +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = delta.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.dist.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.dist.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.dist.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~.) +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', 
       subtitle = expression('Pareto Distance Criteria; Expected probability based on the estimated Pareto front'),
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability', 
                                  'start' = 'Before Sampling', 
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#e41a1c', 'adapt' = '#377eb8', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination. Expected values are at double the resolution
expect1 = filter(delta.tru, var == 'x1')$prob
expect2 = filter(delta.tru, var == 'x2')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.dist.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.dist.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.dist.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.dist.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2)
           )

delta.tru$cat = 'tru';          Infer.dist.start$cat = 'start'
Infer.dist.adapt$cat = 'adapt'; Infer.dist.rando$cat = 'rando'
write.csv(rbind(delta.tru[, names(delta.tru) %in% names(Infer.dist.start)],
      Infer.dist.start[, names(Infer.dist.start) %in% names(delta.tru)],
      Infer.dist.adapt[, names(Infer.dist.adapt) %in% names(delta.tru)],
      Infer.dist.rando[, names(Infer.dist.rando) %in% names(delta.tru)]), 'Marginals_delta.csv')

```

Recalculating the expected marginalization based on the estimated Pareto front (instead of the accurate Pareto front from fine-resolution grid search) shows that the error and unexpected ordering is largely due to issues with the estimated Pareto front, not due to the sampling method.

## IV: Suggested ranges

10. For the most important variable, determine the input values that give the peak response. This is the suggested input range.
11. For the next most important variable, calculate the conditional probability if this variable is known precisely, but the most important is known to be within its suggested range.
12. Repeat step 11 by continuing to constrain the optimal range of the previous variables until all conditional probabilities have been found. This sequential analysis describes the characteristics of the most promising inputs for the near-Pareto set.

In this case, with only 2 inputs, only one iteration needs to be performed, ie. accept | $x_2, x_1 = x_{1,opt}$

To account for possible uncertainty, the conditional for $x_1$ will be within its optimal region, defined by the values of $x_1$ with at least half of the max probability.

For illustrative purposes, the inverse will also be shown (accept | $x_1, x_2 = x_{2,opt}$). This may be useful is, for instance, $x_2$ is easier to measure.

```{r Partial Conditional Probabilities}
# Remove the percentiles for later
Infer.x1 = Infer.x1[,!names(Infer.x1) %in% c('p75', 'p25', 'psd')]
Infer.x2 = Infer.x2[,!names(Infer.x2) %in% c('p75', 'p25', 'psd')]
# Detrmining the ranges
p.target1 = 0.5*max(Infer.x1$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x1.lims = range(filter(Infer.x1, prob > p.target1)$x)
# Adjust the extremes if they are not already at the extreme
if(x1.lims[1] > min(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[1]))
  sub = Infer.x1[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[1] = res
  rm(mod)
}
if(x1.lims[2] < max(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[2]))
  sub = Infer.x1[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[2] = res
  rm(mod)
}

p.target2 = 0.5*max(Infer.x2$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x2.lims = range(filter(Infer.x2, prob > p.target2)$x)
# Adjust the extremes if they are not already at the extreme
if(x2.lims[1] > min(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[1]))
  sub = Infer.x2[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[1] = res
  rm(mod)
}
if(x2.lims[2] < max(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[2]))
  sub = Infer.x2[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[2] = res
  rm(mod)
}

# Marginalize over the specific region of x1
resolution = 50; MCsamp = 1500
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

Infer.x2.x1 = data.frame(); Infer.x1.x2 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], 
                                              prob = mean(fill.frame$p.del1),
                                              var = 'x2', ncond = 'None'))
  # x1 | x2
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.lims), max = max(x2.lims)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
  Infer.x1.x2 = rbind(Infer.x1.x2, data.frame(x = x1.seq[i],
                                              prob = mean(fill.frame$p.del1),
                                              var = 'x1', ncond = 'None'))
}

# Combine results for visualization
Infer.x1$var = 'x1';    Infer.x1$cond = 'Single Input';          names(Infer.x1)[1] = 'x'
Infer.x1.x2$var = 'x1'; Infer.x1.x2$cond = 'Conditional Input';  names(Infer.x1.x2)[1] = 'x'
Infer.x2$var = 'x2';    Infer.x2$cond = 'Single Input';          names(Infer.x2)[1] = 'x'
Infer.x2.x1$var = 'x2'; Infer.x2.x1$cond = 'Conditional Input';  names(Infer.x2.x1)[1] = 'x'
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x1.x2, Infer.x2.x1);

ggplot(Infer.plt) +
  geom_path(mapping = aes(x = x, y = prob, color = cond)) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# See the improvement due to conditioning
Infer.diff = data.frame(x = rep(Infer.x1$x, 2),
                        var = c(rep('x1 | x2', nrow(Infer.x1)), rep('x2 | x1', nrow(Infer.x1))),
                        prob = c(Infer.x1.x2$prob - Infer.x1$prob, Infer.x2.x1$prob - Infer.x2$prob))
ggplot(Infer.diff) +
  geom_path(mapping = aes(x = x, y = prob, color = var)) +
  labs(x = '', y = 'Increase in Probability of Acceptance', 
       subtitle = expression('Accept if '*delta*' < 1'), color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

```

As expected, conditioning $x_2$ on $x_1$ leads to a larger improvement in the probability of acceptance than the other way around.

## Alternative Acceptance Criteria
For the purpose of illustration, the above method will be repeated with two other acceptance criteria:
* Objective function values below a specific threshold.
* Within a specific normalized distance of the utopia point and a specified prioritization of the two objective functions.

### Threshold cutoff
For simplicity, the cutoff values are the normalized values of 1 for both objectives. This is criteria, the acceptable points are those that fall within the normalized box defined by the utopia point (0,0) and the pseudo-nadir point (1,1).

Load the original dataset.

```{r Threshold: Load original data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: the names are:
nm = c("x1",  "x2", "f1", "f2", "order",  "f1.norm", "f2.norm", "dist", "theta")
GPar.all = GPar.all[,nm]
nm = c("x1",  "x2", "f1", "f2",  "f1.norm", "f2.norm")
GPar.front = GPar.front[,nm]

```

```{r Treshold: First iteration, message=FALSE, warning=FALSE}
# Search for the first point
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f2.norm')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.cutoff(x, model.f1 = mod.f1, model.f2 = mod.f2)},
             lower = c(0, 0), upper = c(5, 5),
             popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)
# Fill in the remaining caluclations: normalized outputs, distance, theta, order
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.all = res$data
GPar.front = res$front

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.f1.init = mod.f1
mod.f2.init = mod.f2
```

Visualize the first loop again to show that the set of the acceptable points is different due to the different criteria.

```{r Threshold: First Iteration}
# Grid of the relevant region to visualize
lower = c(0, 0); upper = c(5,5)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = 50))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')
# Apply Kriging functions
res = predict(object = mod.f1.init, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 1, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 1, sd = fine.grid$f2.sd)
# Reset the ranges for plotting
fine.grid$prob.plot = fine.grid$prob

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (prob.plot)*(1-prob.plot)-1e-10, 
                                                      color = (prob.plot)*(1-prob.plot)-1e-10), alpha = 0.5, bins = ncolor)+
  # Plot the boundary of the uncertain region
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = (prob.plot), 
                                                      color = (prob.plot)), alpha = 0.5, breaks = c(0.25, 0.75))+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = log10(info*(prob)*(1-prob)+1e-10), color = log10(info*(prob)*(1-prob)+1e-10)),
                      bins = ncolor-1, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

```

```{r Threshold: Iterations, message=FALSE, warning=FALSE}
budget = 20
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Create a Kriging model for the distance
  mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f1.norm')
  mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f2.norm')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.cutoff(x, model.f1 = mod.f1, model.f2 = mod.f2)},
               lower = c(0, 0), upper = c(5, 5),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
  GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
  GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)
  
  # Fill in the remaining caluclations: normalized outputs, distance, theta, order
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.all = res$data
  GPar.front = res$front
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}


write.csv(GPar.all, 'GPar_Accept_Threshold.csv')

rm(budget, n.samples)
```

```{r Treshold: Visualize Iterations}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Threshold.csv')
# Create a Kriging model for the distance
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'f2.norm')

# Apply Kriging functions
res = predict(object = mod.f1.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 1, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 1, sd = fine.grid$f2.sd)
# Reset the ranges for plotting
fine.grid$prob.plot = fine.grid$prob

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7; pt.size = 2.5
g4 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (prob.plot)*(1-prob.plot)-1e-10, 
                                                      color = (prob.plot)*(1-prob.plot)-1e-10), alpha = 0.5, bins = ncolor)+
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.plot), alpha = 0.5, breaks = c(0.25, 0.75))+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty',
       color = 'Collection\nProcess', shape = 'Collection\nProcess') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color = FALSE, shape = FALSE, fill = FALSE)

g5 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = expression("x"[1]), y = "", color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Information Gain') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + 
  guides(color = FALSE, shape = FALSE, fill = FALSE)
  
g6 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = log10(info*(prob)*(1-prob)+1e-10), color = log10(info*(prob)*(1-prob)+1e-10)),
                      bins = ncolor-1, alpha = 0.5)+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = '', y = '', color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Sample Utility', fill = '') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + guides(color=guide_legend(override.aes=list(fill=NA)))

g4 + g5 + g6
rm(g1, g2, g3, g4, g5, g6)
```

As indicated by this acceptance criteria, the boundary of the set it well defined with just the Pareto frontier, so additional iterations are not as necessary. Continuing with the procedure to show the feature importance and acceptance probability procedures.

```{r Threshold: Feature Importance}
# Load data
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Threshold.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                        x2 = runif(n = nsamp, min = 0, max = 5))
GPar.rando$f1 = f1(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
GPar.rando$f2 = f2(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Use the final GP model with the full dataset
mod.f1.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f1.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f1.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2'), output.name = 'f1.norm')

mod.f2.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2'), output.name = 'f2.norm')
mod.f2.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2'), output.name = 'f2.norm')
mod.f2.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2'), output.name = 'f2.norm')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 500 samples each.
Infer.cut.start = marginal.cut(mod.f1.start, mod.f2.start)
Infer.cut.adapt = marginal.cut(mod.f1.adapt, mod.f2.adapt)
Infer.cut.rando = marginal.cut(mod.f1.rando, mod.f2.rando)

cutof.tru = read.csv('ExpectedMarginal_cutof.csv')

ggplot() +
  geom_path(data = cutof.tru,       mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = cutof.tru,       mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.cut.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.cut.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  geom_path(data = Infer.cut.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  facet_grid(var~.) +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability', 
                                  'start' = 'Before Sampling', 
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#377eb8', 'adapt' = '#e41a1c', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination. Expected values are at double the resolution
expect1 = filter(cutof.tru, var == 'x1')$prob
expect2 = filter(cutof.tru, var == 'x2')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.cut.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.cut.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.cut.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.cut.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.cut.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.cut.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.cut.adapt, var == 'x1')
Infer.x2 = filter(Infer.cut.adapt, var == 'x2')
Import.cutof = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd)),
                          var = c('x1', 'x2'),
                          typ = 'cutof')
Import.cutof$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd))/nrow(Infer.x1) )

ggplot(Import.cutof) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Cutoff Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2);
# write.csv(Infer.plt, 'Marginals_cutof.csv')
cutof.tru$cat = 'tru';         Infer.cut.start$cat = 'start'
Infer.cut.adapt$cat = 'adapt'; Infer.cut.rando$cat = 'rando'
write.csv(rbind(cutof.tru[, names(cutof.tru) %in% names(Infer.cut.start)],
      Infer.cut.start[, names(Infer.cut.start) %in% names(cutof.tru)],
      Infer.cut.adapt[, names(Infer.cut.adapt) %in% names(cutof.tru)],
      Infer.cut.rando[, names(Infer.cut.rando) %in% names(cutof.tru)]), 'Marginals_cutof.csv')

```

Unlike with the Pareto distance criteria, these cutoff criteria did not have the propagation of error from bad estimates of the Pareto front.
In general, it appears the local maximum that happens when $x_3 = 3$ causes issues for both criteria in their marginals.
Additionally, the feature importance criteria shows that the relative importance is almost nearly balanced, showcasing how the selection criteria can alter the relative importance.

The peak in the single-variable marginalization indicates that $x_1$ is, in fact, more important, as even without knowledge of $x_2$, the probability of acceptance can be as high as 0.6.
This is in contrast the the previous selection criteria, which only showed a weak interaction.

```{r Threshold: Partial Conditional Probabilities}
# Remove the percentiles for later
Infer.x1 = Infer.x1[,!names(Infer.x1) %in% c('p75', 'p25', 'psd')]
Infer.x2 = Infer.x2[,!names(Infer.x2) %in% c('p75', 'p25', 'psd')]

# Determining the ranges
p.target1 = 0.5*max(Infer.x1$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x1.lims = range(filter(Infer.x1, prob > p.target1)$x)
# Adjust the extremes if they are not already at the extreme
if(x1.lims[1] > min(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[1]))
  sub = Infer.x1[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[1] = res
  rm(mod)
}
if(x1.lims[2] < max(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[2]))
  sub = Infer.x1[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[2] = res
  rm(mod)
}

p.target2 = 0.5*max(Infer.x2$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x2.lims = range(filter(Infer.x2, prob > p.target2)$x)
# Adjust the extremes if they are not already at the extreme
if(x2.lims[1] > min(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[1]))
  sub = Infer.x2[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[1] = res
  rm(mod)
}
if(x2.lims[2] < max(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[2]))
  sub = Infer.x2[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[2] = res
  rm(mod)
}

# Marginalize over the specific region of x1
resolution = 50; MCsamp = 1500
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

Infer.x2.x1 = data.frame(); Infer.x1.x2 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd)*pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1),
                                              var = 'x2', ncond = '| x1'))
  # x1 | x2
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.lims), max = max(x2.lims)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd)*pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  Infer.x1.x2 = rbind(Infer.x1.x2, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1),
                                              var = 'x1', ncond = '| x2'))
}

ggplot(rbind(Infer.x1, Infer.x2, Infer.x1.x2, Infer.x2.x1)) +
  geom_path(mapping = aes(x = x, y = prob, color = ncond)) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if f'[1]*'* < 1, f'[2]*'* < 1'), color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()
Infer.diff = data.frame(x = rep(Infer.x1$x, 2),
                        var = c(rep('x1 | x2', nrow(Infer.x1)), rep('x2 | x1', nrow(Infer.x1))),
                        prob = c(Infer.x1.x2$prob - Infer.x1$prob, Infer.x2.x1$prob - Infer.x2$prob))
ggplot(Infer.diff) +
  geom_path(mapping = aes(x = x, y = prob, color = var)) +
  labs(x = '', y = 'Increase in Probability of Acceptance', 
       subtitle = expression('Accept if f'[1]*'* < 1, f'[2]*'* < 1'), 
       color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

```

### Utopia Distance and Prioritization
Since the utopia point is set to (0,0), the distance to the utopia point is simple to calculate from the normalized coordinates. Acceptable points are those that prioritize the first objective by at least 80%, which can be determined by the angle theta.

Load the original dataset.

```{r Utopia Distance: Load Data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: the names are:
nm = c("x1",  "x2", "f1", "f2", "order",  "f1.norm", "f2.norm", "dist", "theta")
GPar.all = GPar.all[,nm]
nm = c("x1",  "x2", "f1", "f2",  "f1.norm", "f2.norm")
GPar.front = GPar.front[,nm]

```

```{r Utopia Distance: First Iteration, message=FALSE, warning=FALSE}
# Search for the first point
GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'theta')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.radan(x, model.f1 = mod.rad, model.f2 = mod.ang)},
             lower = c(0, 0), upper = c(5, 5),
             popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)
# Fill in the remaining calculations: normalized outputs, distance, theta, order
res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
GPar.all = res$data
GPar.front = res$front
GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.rad.init = mod.rad
mod.ang.init = mod.ang

# Grid of the relevant region to visualize
lower = c(0, 0); upper = c(5, 5)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = 50))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')
# Apply Kriging functions
res = predict(object = mod.rad.init, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))
# Reset the ranges for plotting
fine.grid$prob.plot = fine.grid$prob

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (prob.plot)*(1-prob.plot)-1e-10, 
                                                      color = (prob.plot)*(1-prob.plot)-1e-10), alpha = 0.5, bins = ncolor)+
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.plot), alpha = 0.5, breaks = c(0.25, 0.75))+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") + 
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = log10(info*(prob)*(1-prob)+1e-10), color = log10(info*(prob)*(1-prob)+1e-10)),
                      bins = ncolor-1, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +  
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order > 0),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +  
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

```

```{r Utopia Distance: Repeated Iterations, message=FALSE, warning=FALSE}
budget = 20
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Create a Kriging model for the distance
  mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'rad')
  mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'theta')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.radan(x, model.f1 = mod.rad, model.f2 = mod.ang)},
               lower = c(0, 0), upper = c(5, 5),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2])
  GPar.new$f1 = f1(x1 = GPar.new$x1, x2 = GPar.new$x2)
  GPar.new$f2 = f2(x1 = GPar.new$x1, x2 = GPar.new$x2)
  
  # Fill in the remaining caluclations: normalized outputs, distance, theta, order
  res = fill.sample.update(GPar.front.old = GPar.front, GPar.data = GPar.all, GPar.new = GPar.new)
  GPar.all = res$data
  GPar.front = res$front
  GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}


write.csv(GPar.all, 'GPar_Accept_Radius.csv')

# rm(budget)
```

```{r}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Radius.csv')
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2'), output.name = 'theta')

# Apply Kriging functions
res = predict(object = mod.rad.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang.init, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))
# Reset the ranges for plotting
fine.grid$prob.plot = fine.grid$prob

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g4 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = (prob.plot)*(1-prob.plot)-1e-10, 
                                                      color = (prob.plot)*(1-prob.plot)-1e-10), alpha = 0.5, bins = ncolor)+
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.plot), alpha = 0.5, breaks = c(0.25, 0.75))+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'),
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty',
       color = 'Collection\nProcess', shape = 'Collection\nProcess') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color = FALSE, shape = FALSE, fill = FALSE)

g5 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = expression("x"[1]), y = "", color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Information Gain') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-2), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + 
  guides(color = FALSE, shape = FALSE, fill = FALSE)
  
g6 = ggplot() +
  geom_contour_filled(data = fine.grid, mapping = aes(x = x1, y = x2, z = log10(info*(prob)*(1-prob)+1e-10), color = log10(info*(prob)*(1-prob)+1e-10)),
                      bins = ncolor-1, alpha = 0.5)+
  geom_point(data = filter(GPar.all, order == 0, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'initial', color = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > 0, order <= Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Pareto', color = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2]), 
             mapping = aes(x = x1, y = x2, shape = 'Post', color = 'Post'), size = pt.size) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'), labels = c('Initial', 'Pareto', 'Post'), 
                     breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = '', y = '', color = 'Collection\nProcess', shape = 'Collection\nProcess', subtitle = 'Sample Utility', fill = '') +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr") +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + guides(color=guide_legend(override.aes=list(fill=NA)))

g4 + g5 + g6

rm(g1, g2, g3, g4, g5, g6)
```

Fewer points were needed to reach convergence due to a relatively narrow uncertainty band to start.

```{r Utopia distance Feature Importance}
# Load data
GPar.start = read.csv(file = 'GPar_all_start.csv')
GPar.adapt = read.csv(file = 'GPar_Accept_Radius.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Compare to a random sample set
nsamp = nrow(GPar.adapt) - nrow(GPar.start)
GPar.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
                        x2 = runif(n = nsamp, min = 0, max = 5))
GPar.rando$f1 = f1(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
GPar.rando$f2 = f2(x1 = GPar.rando$x1, x2 = GPar.rando$x2)
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.rando = n.obj(GPar.data = GPar.rando, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.rando)) %dopar%
  n.dist(f1.norm = GPar.rando$f1.norm[row], f2.norm = GPar.rando$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.rando$dist = unlist(dist)
GPar.rando$theta = atan(GPar.rando$f2.norm/GPar.rando$f1.norm)*180/pi*10/9
GPar.rando$order = seq(from = max(GPar.start$order) + 1, to = max(GPar.start$order) + nsamp, by = 1)
GPar.rando = rbind(GPar.start[, names(GPar.start) %in% names(GPar.rando)], GPar.rando)

# Explicit radius variable
GPar.start$rad = sqrt(GPar.start$f1.norm^2 + GPar.start$f2.norm)
GPar.adapt$rad = sqrt(GPar.adapt$f1.norm^2 + GPar.adapt$f2.norm)
GPar.rando$rad = sqrt(GPar.rando$f1.norm^2 + GPar.rando$f2.norm)

# Use the final GP model with the full dataset
mod.rad.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2'), output.name = 'rad')
mod.rad.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2'), output.name = 'rad')
mod.rad.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2'), output.name = 'rad')

mod.ang.start = fill.sample.mod(GPar.data = GPar.start, input.name = c('x1', 'x2'), output.name = 'theta')
mod.ang.adapt = fill.sample.mod(GPar.data = GPar.adapt, input.name = c('x1', 'x2'), output.name = 'theta')
mod.ang.rando = fill.sample.mod(GPar.data = GPar.rando, input.name = c('x1', 'x2'), output.name = 'theta')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 500 samples each.
Infer.rad.start = marginal.rad(mod.rad.start, mod.ang.start)
Infer.rad.adapt = marginal.rad(mod.rad.adapt, mod.ang.adapt)
Infer.rad.rando = marginal.rad(mod.rad.rando, mod.ang.rando)

radan.tru = read.csv('ExpectedMarginal_radan.csv')

ggplot() +
  geom_path(data = radan.tru,        mapping = aes(x = x, y = prob+psd, color = 'tru'), linetype = 2) +
  geom_path(data = radan.tru,        mapping = aes(x = x, y = prob-psd, color = 'tru'), linetype = 2) +
  geom_path(data = Infer.rad.start, mapping = aes(x = x, y = prob, color = 'start')) +
  geom_path(data = Infer.rad.rando, mapping = aes(x = x, y = prob, color = 'rando')) +
  geom_path(data = Infer.rad.adapt, mapping = aes(x = x, y = prob, color = 'adapt')) +
  facet_grid(var~.) +
  labs(x = 'Input Variable', y = 'Probability of Acceptance', subtitle = expression('Utopia Distance Criteria'),
       color = 'Dataset') +
  scale_color_manual(labels = c('tru' = 'Expected Probability', 
                                  'start' = 'Before Sampling', 
                                  'adapt' = 'Adaptive Sampling', 'rando' = 'Random Sampling'),
                       values = c('tru' = 'grey40', 'start' = '#377eb8', 'adapt' = '#e41a1c', 'rando' = '#4daf4a'),
                       breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1))))

# Coefficients of determination. Expected values are at double the resolution
expect1 = filter(radan.tru, var == 'x1')$prob
expect2 = filter(radan.tru, var == 'x2')$prob
data.frame(method = c('Before Sampling', 'Adaptive Sampling', 'Random Sampling'),
           R2.x1 = c(cor(x = filter(Infer.rad.start, var == 'x1')$prob, y = expect1, method = 'pearson')^2, 
             cor(x = filter(Infer.rad.adapt, var == 'x1')$prob, y = expect1, method = 'pearson')^2,
             cor(x = filter(Infer.rad.rando, var == 'x1')$prob, y = expect1, method = 'pearson')^2),
           R2.x2 = c(cor(x = filter(Infer.rad.start, var == 'x2')$prob, y = expect2, method = 'pearson')^2, 
             cor(x = filter(Infer.rad.adapt, var == 'x2')$prob, y = expect2, method = 'pearson')^2,
             cor(x = filter(Infer.rad.rando, var == 'x2')$prob, y = expect2, method = 'pearson')^2)
           )


# Relative importance: inverse of the area under the curve, normalized units, 
# adjusted by the range of the probabilities' means
Infer.x1 = filter(Infer.rad.adapt, var == 'x1')
Infer.x2 = filter(Infer.rad.adapt, var == 'x2')
Import.radan = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd)),
                          var = c('x1', 'x2'),
                          typ = 'radan')
Import.radan$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd))/nrow(Infer.x1) )

ggplot(Import.radan) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  labs(x = '', y = 'Relative Importance', fill = 'Variable', 
       subtitle = expression('Utopia Distance Criteria')) +
  scale_y_continuous(breaks = c(), expand = expansion(mult = c(0, .1))) + 
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) + 
  scale_fill_manual(values = c('#7fc97f', '#beaed4'))


# Store the data for importance ranking comparison later
Infer.plt = rbind(Infer.x1, Infer.x2);
# write.csv(Infer.plt, 'Marginals_radan.csv')
radan.tru$cat = 'tru';         Infer.rad.start$cat = 'start'
Infer.rad.rando$cat = 'adapt'; Infer.rad.adapt$cat = 'rando'
write.csv(rbind(radan.tru[, names(radan.tru) %in% names(Infer.rad.start)],
      Infer.rad.start[, names(Infer.rad.start) %in% names(radan.tru)],
      Infer.rad.rando[, names(Infer.rad.rando) %in% names(radan.tru)],
      Infer.rad.adapt[, names(Infer.rad.adapt) %in% names(radan.tru)]), 'Marginals_radan.csv')


```

```{r Utopia distance: Partial Conditional Probabilities}
# Remove the percentiles for later
Infer.x1 = Infer.x1[,!names(Infer.x1) %in% c('p75', 'p25', 'psd')]
Infer.x2 = Infer.x2[,!names(Infer.x2) %in% c('p75', 'p25', 'psd')]

# Determining the ranges
p.target1 = 0.5*max(Infer.x1$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x1.lims = range(filter(Infer.x1, prob > p.target1)$x)
# Adjust the extremes if they are not already at the extreme
if(x1.lims[1] > min(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[1]))
  sub = Infer.x1[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[1] = res
  rm(mod)
}
if(x1.lims[2] < max(Infer.x1$x)){
  pos = which.min(abs(Infer.x1$x - x1.lims[2]))
  sub = Infer.x1[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target1))
  x1.lims[2] = res
  rm(mod)
}

p.target2 = 0.5*max(Infer.x2$prob)
# Since the probability distribution for this function has a single peak, the easiest method is to find the cutoffs for this acceptance.
x2.lims = range(filter(Infer.x2, prob > p.target2)$x)
# Adjust the extremes if they are not already at the extreme
if(x2.lims[1] > min(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[1]))
  sub = Infer.x2[c(pos-1, pos), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[1] = res
  rm(mod)
}
if(x2.lims[2] < max(Infer.x2$x)){
  pos = which.min(abs(Infer.x2$x - x2.lims[2]))
  sub = Infer.x2[c(pos, pos+1), ]
  mod = lm(x ~ prob, data = sub)
  res = predict(object = mod, newdata = data.frame(prob = p.target2))
  x2.lims[2] = res
  rm(mod)
}

# Marginalize over the specific region of x1
resolution = 50; MCsamp = 1500
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

Infer.x2.x1 = data.frame(); Infer.x1.x2 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)))
  res1 = predict(object = mod.rad.adapt, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang.adapt, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', ncond = 1))
  # x1 | x2
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.lims), max = max(x2.lims)))
  res1 = predict(object = mod.rad.adapt, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang.adapt, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x1.x2 = rbind(Infer.x1.x2, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), var = 'x1', ncond = 1))
}

# Combine results for visualization
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x1.x2, Infer.x2.x1);
ggplot(Infer.plt) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(ncond))) +
  facet_wrap(var~., nrow = 2) + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Accept if r < 1, f'[1]*' priority < 80%'), 
       color = '') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# See the improvement due to conditioning
Infer.diff = data.frame(x = rep(Infer.x1$x, 2),
                        var = c(rep('x1', nrow(Infer.x1)), rep('x2', nrow(Infer.x1))),
                        prob = c(Infer.x1.x2$prob - Infer.x1$prob, Infer.x2.x1$prob - Infer.x2$prob))
ggplot(Infer.diff) +
  geom_path(mapping = aes(x = x, y = prob, color = var)) +
  labs(x = '', y = 'Increase in Probability of Acceptance', subtitle = expression('Accept if r < 1, f'[1]*' priority < 80%'), 
       color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

# rm(Infer.diff, Infer.plt)
```

## Comparison of Acceptance Criteria
Comparison of the boundaries to show how changing the acceptance criteria changes the shape of the near-Pareto set

```{r Comparison of Acceptably Optimal Sets}
# Load datasets for obtaining the refined probability functions
data.delta = read.csv('GPar_Accept_Delta1.csv')
data.cutof = read.csv('GPar_Accept_Threshold.csv')
data.radan = read.csv('GPar_Accept_Radius.csv')
# Compare to the estimate of the Pareto frontier
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

##
# Grid of the relevant region to visualize
lower = c(0, 0); upper = c(5,5); grid.sz = 100 # Based on previous samples, no boundary is greater than 3.5
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = grid.sz), 
                        x2 = seq(from = lower[2], to = upper[2], length.out = grid.sz))
fine.grid = fine.grid[,1:2]
names(fine.grid) = c('x1', 'x2')

##
# Normalized distance
mod.dist = fill.sample.mod(GPar.data = data.delta, input.name = c('x1', 'x2'), output.name = 'dist')
res = predict(object = mod.dist, newdata = fine.grid[,c('x1', 'x2')], type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd
fine.grid$prob.delta = pnorm(q = 0, mean = fine.grid$dist.mean - 1, sd = fine.grid$dist.sd)

##
# Threshold cutoff
mod.f1 = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = data.cutof, input.name = c('x1', 'x2'), output.name = 'f2.norm')

# Apply Kriging functions
res = predict(object = mod.f1, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob.cutof = pnorm(q = 0, mean = fine.grid$f1.mean - 1, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 1, sd = fine.grid$f2.sd)

##
# Radius-angle
mod.rad = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = data.radan, input.name = c('x1', 'x2'), output.name = 'theta')

# Apply Kriging functions
res = predict(object = mod.rad, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, x2 = fine.grid$x2), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob.radan = pnorm(q = 0, mean = fine.grid$rad.mean - 1, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

sep = 0
ggplot() +
  # Boundaries: +/- some separation from 0.5
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.delta, color = 'delta'), 
               breaks = c(0.5 + sep, 0.5 - sep)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.cutof, color = 'cutof'), 
               breaks = c(0.5 + sep, 0.5 - sep)) +
  geom_contour(data = fine.grid, mapping = aes(x = x1, y = x2, z = prob.radan, color = 'radan'), 
               breaks = c(0.5 + sep, 0.5 - sep)) +
  # Pareto frontier
  geom_point(data = GPar.front, mapping = aes(x = x1, y = x2), color = 'black') + 
  geom_smooth(data = GPar.front, mapping = aes(x = x1, y = x2, color = 'Pareto'), level = 0.95) + 
  labs(x = expression('x'[1]), y = expression('x'[2]), color = 'Acceptance Criteria') +
  scale_color_manual(values = c('delta' = 'red', 'cutof' = 'blue', 'radan' = 'green', 'Pareto' = 'black'),
                     labels = c('delta' = expression(delta*' < 1'), 'cutof' = expression('F'[1]^'*'*'< 1, F'[2]^'*'*'< 1'), 
                                'radan' = expression('r < 1, '*theta*' > 18'^'o'),
                                'Pareto' = expression('Pareto Front')),
                     breaks = c('delta', 'cutof', 'radan', 'Pareto')) +
  theme_classic() + theme(legend.position = c(0.85, 0.75)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 5)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 5)) +
  guides(colour = guide_legend(override.aes = list(fill = alpha('white', 1))))


```

Comparison of importance rankings

```{r Comparison: Relative Importance}
# Normalize so they all have the same maximum
Import.delta$r.import = Import.delta$import/max(Import.delta$import)
Import.cutof$r.import = Import.cutof$import/max(Import.cutof$import)
Import.radan$r.import = Import.radan$import/max(Import.radan$import)
# Update names for plotting
Import.delta$typ = 'Pareto Distance'
Import.cutof$typ = 'Threshold Cutoff'
Import.radan$typ = 'Utopia Distance'
Import = rbind(Import.delta, Import.cutof, Import.radan)

ggplot(Import) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = import+sd, ymin = import-sd), width = 0.5) +
  facet_wrap(~typ) +
  labs(x = 'Variable', y = 'Relative Importance') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  theme_bw() +
  scale_fill_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])), name = '')

```


## Feature importance method comparison

Using Strumbelj et al. (2014)'s Monte Carlo estimation to calculate the Shapley values.

$$\phi_j = mean(\hat{f}(\bar{x}) - \hat{f}(\bar{z}))$$
where the vector components of $\bar{x}$ and $\bar{z}$ are related as follows:

$$x_i = z_i \forall i \neq j$$
i.e. the importance of variable $j$ is related to the variation that it would cause if it was not controlled.

In this case, $\hat{f}$ gives the probability that the point satisfies the acceptance criteria.

```{r Functions: Shapley values}
# Function evaluations: output the probability
f.delta = function(x, mod.dist){
  fill.frame = data.frame(x1 = x$x1, x2 = x$x2)
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  p.del1 = pnorm(q = 0, mean = res$mean - 1, sd = res$sd)
  return(p.del1)
}
f.cutof = function(x, mod.f1, mod.f2){
  fill.frame = data.frame(x1 = x$x1, x2 = x$x2)
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  return(p.del1)
}
f.radan = function(x, mod.rad, mod.ang){
  fill.frame = data.frame(x1 = x$x1, x2 = x$x2)
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  p.del1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  return(p.del1)
}

# Calculating the Shapley values and their standard error
shap = function(p.x, p.z1, p.z2, nsamp){
  # Standard error = standard error of the mean, each mean is of nsamp samples
  # Convert probabilities into a matrix with nsamp rows
  p.x  = matrix(data = p.x,  nrow = nsamp)
  p.z1 = matrix(data = p.z1, nrow = nsamp)
  p.z2 = matrix(data = p.z2, nrow = nsamp)
  
  # x1 Shapley values
  x1.m = apply(p.x - p.z1, 2, mean)
  x1.shap = mean(x1.m)
  x1.s = sd(x1.m)
  
  # x1 Shapley values
  x2.m = apply(p.x - p.z2, 2, mean)
  x2.shap = mean(x2.m)
  x2.s = sd(x2.m)
  return(c(x1.shap, x2.shap, x1.s, x2.s))
}
```


```{r Shapley Value Calculation}
nsamp = 1500*50 # Same number of points used in the calculation for the marginals
# Create random vectors x and z
x = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
               x2 = runif(n = nsamp, min = 0, max = 5))
z.x1 = data.frame(x1 = runif(n = nsamp, min = 0, max = 5),
               x2 = x$x2)
z.x2 = data.frame(x1 = x$x1,
               x2 = runif(n = nsamp, min = 0, max = 5))

# Shapley values for x1
p.x  = f.delta(x = x,    mod.dist = mod.dist.adapt)
p.z1 = f.delta(x = z.x1, mod.dist = mod.dist.adapt)
p.z2 = f.delta(x = z.x2, mod.dist = mod.dist.adapt)
s = shap(p.x, p.z1, p.z2, nsamp = 1500)
shap.delta = s[1:2]; shap.delta.s = s[3:4]
# shap.delta   = c(mean(p.x - p.z1), mean(p.x - p.z2))
# shap.delta.s = c(sd(p.x - p.z1),   sd(p.x - p.z2))

p.x  = f.cutof(x = x,    mod.f1 = mod.f1.adapt, mod.f2 = mod.f1.adapt)
p.z1 = f.cutof(x = z.x1, mod.f1 = mod.f1.adapt, mod.f2 = mod.f1.adapt)
p.z2 = f.cutof(x = z.x2, mod.f1 = mod.f1.adapt, mod.f2 = mod.f1.adapt)
s = shap(p.x, p.z1, p.z2, nsamp = 1500)
shap.cutof = s[1:2]; shap.cutof.s = s[3:4]
# shap.cutof   = c(mean(p.x - p.z1), mean(p.x - p.z2))
# shap.cutof.s = c(sd(p.x - p.z1),   sd(p.x - p.z2))

p.x  = f.radan(x = x,    mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
p.z1 = f.radan(x = z.x1, mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
p.z2 = f.radan(x = z.x2, mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
s = shap(p.x, p.z1, p.z2, nsamp = 1500)
shap.radan = s[1:2]; shap.radan.s = s[3:4]
# shap.radan   = c(mean(p.x - p.z1), mean(p.x - p.z2))
# shap.radan.s = c(sd(p.x - p.z1),   sd(p.x - p.z2))

Shap = data.frame(import = c(shap.delta, shap.cutof, shap.radan),
                  r.import = c(shap.delta/max(abs(shap.delta)), shap.cutof/max(abs(shap.cutof)),
                               shap.radan/max(abs(shap.radan))),
                  sd = c(shap.delta.s, shap.cutof.s, shap.radan.s),
           var = c('x1', 'x2'),
           typ = Import$typ)

ggplot(Shap) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = import+sd, ymin = import-sd), width = 0.5) +
  facet_wrap(~typ) +
  labs(x = 'Variable', y = 'Shapley Value') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) +
  scale_y_continuous(expand = expansion(mult = c(0.1, 0))) +
  theme_bw() +
  scale_fill_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])), name = '')

(ggplot(Import) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = import+sd, ymin = import-sd), width = 0.5) +
  facet_wrap(~typ) +
  labs(x = 'Variable', y = 'Relative Importance') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  theme_bw() +
  scale_fill_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])), name = '')) /
(ggplot(Shap) +
  geom_col(mapping = aes(x = var, y = (import), fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = import+sd, ymin = import-sd), width = 0.5) +
  facet_wrap(~typ) +
  labs(x = 'Variable', y = 'Shapley Value') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]))) +
  scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) +
  theme_bw() +
  scale_fill_discrete(labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2])), name = '')
)

rm(x, z.x1, z.x2, p.x, p.z1, p.z2)

# Store the importance rankings
Import$method = 'Marginal'
Shap$method = 'Shapley'
write.csv(rbind(Import, Shap), 'Importance.csv')
```

From the Shapley value calculations:
* The variance is very large, despite using the same number of calculations as the marginalizations.
* The Shapley values show that the impact of all variables is negative, indicating that on average, changing either x1 or x2 will lead to a decrease in the probability of being accepted, which does not make practical sense.
* The relative magnitude of $x_1$ and $x_2$ are consistent with the previous importance rankings, which showed that $x_1$ has a larger impact than $x_2$. However, it does not show the fact that the Pareto distance and Threshold cutoff criteria are nearly identical, nor does it capture the fact that the Utopia distance criterion has nearly equal importance weighting.

Overall, the Shapley values are not particularly useful in determining which variable is most important for meeting the selection criterion.



