---
title: "Pattern Recognition Method Comparison: ZDT4"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook takes the data generated by the script in /Ex_ZDT4, which describes the newly developed method, and compares it existing pattern recognition algorithms. 

The classification algorithms are:
* Support Vector Machines: example supervised learning problem
* Gaussian mixture models: example unsupervised learning problem

For ease of calculation, the iterative refinement of the GP models are not included here, nor are the calculations of the marginalized probabilities.
The SVM and GMM methods are tested with (1) the data after finding the Pareto front (before the new adaptive sampling process), (2) the data after adaptive sampling refinement, and (3) the data after the Pareto front search with additional random sampling to see if the new sampling method is useful for these other processes as well.

Comparison among the supervised methods is done by looking at the error rate across the entire space. 
Since the test function is a simple polynomial, the solution of what is acceptable can be found explicitly.
Since the GP method produces a fuzzy classifier, the error rate is the average misclassification probability.
For the SVM method, the error rate is simply the number of incorrectly categorized points divided by the total number of test points.

In addition to the accuracy comparison, a comparison of the importance ranking metric developed for using GP in classification problems will be compared to Shapley values.
As with the accuracy comparison, this will be tested against the data before refinement, after refinement, or after a random sample.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages, message=FALSE, warning=FALSE}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
# Gaussian Mixture Models
library(mclust)
# Support Vector Machines
library(e1071)
```

# Relevant functions

The functions to optimize are a modified version of the ZDT4 function from Zitzler, Deb, and Theile 2000. The frequency of the sinusoid has been modified to reduce the number of local optima for illutrative purposes.

```{r Optimization Functions}
# Input set of (x1, x2) on [0, 1]
ZDT4.mod = function (x) 
{
  if (is.null(dim(x))) {
    x <- matrix(x, nrow = 1)
  }
  n <- ncol(x)
  # Corrected g term
  g <- 1 + 10 * (n - 1) + rowSums((x[, 2:n, drop = FALSE] * 
    0.5 - 0.25)^2 - 2.5 * cos(0.5 * pi * (x[, 2:n, drop = FALSE] * 
    10 - 5)))
  # Adjustment term: parabolic
  f <- rowSums(10*(x[, 2:n, drop = FALSE] - 0.5)^2)
  return(cbind(x[, 1], f+(g * (1 - sqrt(x[, 1]/g))) ))
}

```

Normalization-related functions
```{r Functions part 2}
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))

  return(GPar.data)
}

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  if(f1.norm <= 0 & f2.norm <= 0){
    # If the point dominates the Pareto front in both regards, it's distance is 0
    Par.x = 0; Par.y = 0
  }else if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    # Check if the angle is the same as a point on the Pareto front
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}
```

Gaussian process parameter tuning
```{r}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = DiceKriging::km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 # covtyp = 'gauss', # Gaussian uncertainty
                 # optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50, # Increase robustness
                                max.generations = 400), # Some convergence issues
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}

```

# Comparison of GP-based Boundaries with Different Acceptance Criteria

Comparison of the boundaries to show how changing the acceptance criteria changes the shape of the near-Pareto set. Showcases the robustness to different selection criteria, indicating flexibility in the design objectives.


```{r Datasets, eval=FALSE}
if (!interactive()) {
## Loading
# Load datasets for obtaining the refined probability functions
data.delta = read.csv('../Ex_ZDT4/GPar_Accept_Delta1.csv')
data.cutof = read.csv('../Ex_ZDT4/GPar_Accept_Threshold.csv')
data.radan = read.csv('../Ex_ZDT4/GPar_Accept_Radius.csv')
# Load datasets prior to refinement
data.paret = read.csv('../Ex_ZDT4/GPar_all_start.csv')
data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
# Compare to the estimate of the Pareto frontier
GPar.front = read.csv(file = '../Ex_ZDT4/GPar_fnt_start.csv')

# Include a dataset of random points after finding the pareto front
if(file.exists('GPar_Random.csv') == FALSE){
  nsamp = max(nrow(data.delta), nrow(data.cutof), nrow(data.radan)) - nrow(data.paret)
  nsamp = nsamp*20 # Collect extra for variance testing
  data.rando = data.frame(x1 = runif(n = nsamp, min = 0, max = 1),
                          x2 = runif(n = nsamp, min = 0, max = 1),
                          x3 = runif(n = nsamp, min = 0, max = 1))
  res = ZDT4.mod(x = as.matrix(data.rando))
  data.rando$f1 = res[,1]; data.rando$f2 = res[,2]
  # Fill in the remaining calculations: normalized outputs, distance, theta, order
  data.rando = n.obj(GPar.data = data.rando, GPar.front = GPar.front)
  cl <- makeCluster(3)
  registerDoParallel(cl)
  dist = foreach(row = 1:nrow(data.rando)) %dopar%
    n.dist(f1.norm = data.rando$f1.norm[row], f2.norm = data.rando$f2.norm[row], GPar.front = GPar.front)
  stopCluster(cl)
  data.rando$dist = unlist(dist)
  data.rando$rad = sqrt(data.rando$f1.norm^2 + data.rando$f2.norm^2)
  data.rando$theta = atan(data.rando$f2.norm/data.rando$f1.norm)*180/pi*10/9
  data.rando$order = seq(from = max(data.paret$order) + 1, to = max(data.paret$order) + nsamp, by = 1)
  data.rando = rbind(data.paret[, names(data.paret) %in% names(data.rando)], data.rando)
  
  # Store the random data
  write.csv(data.rando, file = 'GPar_Random.csv', row.names = F)
}
rm(dist)
}
```

Reload datasets so that the fine grid and random sample datasets do not need to be re-calculated
```{r GP: Generate models}
# Load datasets prior to refinement
data.delta = read.csv('../Ex_ZDT4/GPar_Accept_Delta1.csv')
data.cutof = read.csv('../Ex_ZDT4/GPar_Accept_Threshold.csv')
data.radan = read.csv('../Ex_ZDT4/GPar_Accept_Radius.csv')
data.paret = read.csv('../Ex_ZDT4/GPar_all_start.csv')
data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
data.rando = read.csv(file = 'GPar_Random.csv')
data.rando = data.rando[1:nrow(data.delta),] # Extra samples - keep the same for consistency

# Load the estimate of the Pareto frontier
GPar.front = read.csv(file = '../Ex_ZDT4/GPar_fnt_start.csv')

inputs = c('x1', 'x2', 'x3')
##
# Normalized distance
mod.dist.paret = fill.sample.mod(GPar.data = data.paret, input.name = inputs, output.name = 'dist')
mod.dist.adapt = fill.sample.mod(GPar.data = data.delta, input.name = inputs, output.name = 'dist')
mod.dist.rando = fill.sample.mod(GPar.data = data.rando, input.name = inputs, output.name = 'dist')

##
# Threshold cutoff
mod.f1.paret = fill.sample.mod(GPar.data = data.paret, input.name = inputs, output.name = 'f1.norm')
mod.f2.paret = fill.sample.mod(GPar.data = data.paret, input.name = inputs, output.name = 'f2.norm')
mod.f1.adapt = fill.sample.mod(GPar.data = data.cutof, input.name = inputs, output.name = 'f1.norm')
mod.f2.adapt = fill.sample.mod(GPar.data = data.cutof, input.name = inputs, output.name = 'f2.norm')
mod.f1.rando = fill.sample.mod(GPar.data = data.rando, input.name = inputs, output.name = 'f1.norm')
mod.f2.rando = fill.sample.mod(GPar.data = data.rando, input.name = inputs, output.name = 'f2.norm')

##
# Radius-angle
mod.rad.paret = fill.sample.mod(GPar.data = data.paret, input.name = inputs, output.name = 'rad')
mod.ang.paret = fill.sample.mod(GPar.data = data.paret, input.name = inputs, output.name = 'theta')
mod.rad.adapt = fill.sample.mod(GPar.data = data.radan, input.name = inputs, output.name = 'rad')
mod.ang.adapt = fill.sample.mod(GPar.data = data.radan, input.name = inputs, output.name = 'theta')
mod.rad.rando = fill.sample.mod(GPar.data = data.rando, input.name = inputs, output.name = 'rad')
mod.ang.rando = fill.sample.mod(GPar.data = data.rando, input.name = inputs, output.name = 'theta')

```

## Error rate
The error rate in the GP model estimate should account for the built-in uncertainty in the model. 
The probability that the model gives the wrong result given the (x1, x2, x3) coordinates is the probability that it accepts the point when it should reject it, or vice versa.
Since all (x1, x2, x3) are equally likely in this mathematical function, the error rate of the GP model is the average of these probabilities.
There should be an evident decrease in the error rate between the pre- and post-sampling models.

The uncertainty in the error rate is sqrt(p*(1-p)/N) because it is simply a frequency.

Since there are 3 inputs to the function, a fine-spaced grid will require too much memory with little reward to accuracy. A coarse, 4-level grid will be used to ensure some sampling throughout the space, as well as 10^3 randomly sampled points.

```{r GP: Error rate functions}
error.test.samp = function(nlevel, nsamp){
  # Generates the samples for determining the error of the model
  # 3 dimensional input, each with a domain of (0.1)
  
  # Initial grid:
  grid.sample = expand.grid(x1 = seq(from = 0, to = 1, length.out = nlevel),
             x2 = seq(from = 0, to = 1, length.out = nlevel),
             x3 = seq(from = 0, to = 1, length.out = nlevel))
  grid.sample = grid.sample[,1:3]
  names(grid.sample) = c('x1', 'x2', 'x3')
  rand.sample = data.frame(x1 = runif(n = nsamp, min = 0, max = 1),
                           x2 = runif(n = nsamp, min = 0, max = 1),
                           x3 = runif(n = nsamp, min = 0, max = 1))
  return(rbind(grid.sample, rand.sample))
}

# Function to calculate the sample set
error.test.eval = function(nlevel, nsamp){
  err.samp = error.test.samp(nlevel = nlevel, nsamp = nsamp)
  
  # Evaluate the function
  res = ZDT4.mod(x = as.matrix(err.samp))
  err.samp$f1 = res[,1]; err.samp$f2 = res[,2]
  # Normalized evaluation
  err.samp = n.obj(GPar.data = err.samp, GPar.front = GPar.front)
  # Pareto distance
  err.samp$dist = NaN
  for(row in 1:nrow(err.samp)){
    err.samp$dist[row] = n.dist(f1.norm = err.samp$f1.norm[row], 
                                f2.norm = err.samp$f2.norm[row], 
                                GPar.front = GPar.front)
  }
  
  # Utopia radius, f1 prioritization
  err.samp$ang = atan(err.samp$f2.norm/err.samp$f1.norm)*180/pi*10/9
  err.samp$rad = sqrt(err.samp$f1.norm^2 + err.samp$f2.norm^2)
  return(err.samp)
}

# Calculate all errors: minimizes the number of times functions are evaluated
error.test.screen = function(mod.dist, mod.f1, mod.f2, mod.rad, mod.ang, nsamp, nlevel){
  # Obtain sample
  err.samp = error.test.eval(nlevel = nlevel, nsamp = nsamp)
  # Accepted points
  err.samp$delta = 0; err.samp$delta[err.samp$dist <= 0.5] = 1
  err.samp$cutof = 0; 
  err.samp$cutof[err.samp$f1.norm <= 1 & err.samp$f2.norm <= 1] = 1
  err.samp$radan = 0; 
  err.samp$radan[err.samp$rad <= 1 & err.samp$ang > 20] = 1
  
  ## Model estimate: Delta from the pareto front
  res = predict(object = mod.dist, newdata = err.samp[,c('x1', 'x2', 'x3')], type = 'UK')
  # Probability of acceptance
  p.acc = pnorm(q = 0, mean = res$mean - 0.5, sd = res$sd)
  # Probability of incorrect result
  p.err = p.acc
  p.err[err.samp$delta == 1] = 1 - p.err[err.samp$delta == 1]
  # Average error rate
  err.rate = mean(p.err)
  # Type 1 error = P[should reject | accepted] = 
  # P[should reject but accepted] P[should reject] / P[accepted]
  typ1 = sum(p.acc[err.samp$delta == 0])*nrow(dplyr::filter(err.samp, delta == 0))/
    nrow(err.samp)^2/mean(p.acc)
  # Type 2 error = P[should accept | rejected] = 
  # P[should accept but rejected] P[should accept] / P[rejected]
  typ2 = sum(1 - p.acc[err.samp$delta == 1])*nrow(dplyr::filter(err.samp, delta == 1))/
    nrow(err.samp)^2/mean(1 - p.acc)
  # Store
  delta = c(err.rate, typ1, typ2)

  ## Model estimate: normalized cutoff
  res1 = predict(object = mod.f1, newdata = err.samp[,c('x1', 'x2', 'x3')], type = 'UK')
  res2 = predict(object = mod.f2, newdata = err.samp[,c('x1', 'x2', 'x3')], type = 'UK')
  # Probability of acceptance
  p.acc = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd)*
    pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  # Probability of correct result
  p.err = p.acc
  p.err[err.samp$cutof == 1] = 1 - p.err[err.samp$cutof == 1]
  # Average error rate
  err.rate = mean(p.err)
  # Type 1 error
  typ1 = sum(p.acc[err.samp$cutof == 0])*nrow(dplyr::filter(err.samp, cutof == 0))/
    nrow(err.samp)^2/mean(p.acc)
  # Type 2 error
  typ2 = sum(1 - p.acc[err.samp$cutof == 1])*nrow(dplyr::filter(err.samp, cutof == 1))/
    nrow(err.samp)^2/mean(1 - p.acc)
  # Store
  cutof = c(err.rate, typ1, typ2)

  # Model estimate
  res1 = predict(object = mod.rad, newdata = err.samp[,c('x1', 'x2', 'x3')], type = 'UK')
  res2 = predict(object = mod.ang, newdata = err.samp[,c('x1', 'x2', 'x3')], type = 'UK')
  # Probability of acceptance
  p.acc = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd)*
    (1-pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  # Probability of correct result
  p.err = p.acc
  p.err[err.samp$radan == 1] = 1 - p.err[err.samp$radan == 1]
  # Average error rate
  err.rate = mean(p.err)
  # Type 1 error
  typ1 = sum(p.acc[err.samp$radan == 0])*nrow(dplyr::filter(err.samp, radan == 0))/
    nrow(err.samp)^2/mean(p.acc)
  # Type 2 error
  typ2 = sum(1 - p.acc[err.samp$radan == 1])*nrow(dplyr::filter(err.samp, radan == 1))/
    nrow(err.samp)^2/mean(1 - p.acc)
  # Store
  radan = c(err.rate, typ1, typ2)

  return(data.frame(prob = c(delta, cutof, radan), 
                    class = c(rep('Pareto Distance', 3), rep('Threshold Cutoff', 3), 
                              rep('Utopia Distance', 3)),
                    typ = c('Total', 'False Positive', 'False Negative')))
}

error.test.nSamp = function(data.dist, data.f, data.rad, data.rand, 
                            nParet, nTest, nsamp, nlevel){
  # Given the adaptively sampled datasets, generate each model
  inputs = c('x1', 'x2', 'x3')
  delta = data.dist[1:(nParet + nTest),]
  cutof = data.f[1:(nParet + nTest),]
  radan = data.rad[1:(nParet + nTest),]
  mod.dist = fill.sample.mod(GPar.data = delta, input.name = inputs, output.name = 'dist')
  mod.f1   = fill.sample.mod(GPar.data = cutof, input.name = inputs, output.name = 'f1.norm')
  mod.f2   = fill.sample.mod(GPar.data = cutof, input.name = inputs, output.name = 'f2.norm')
  mod.rad  = fill.sample.mod(GPar.data = radan, input.name = inputs, output.name = 'rad')
  mod.ang  = fill.sample.mod(GPar.data = radan, input.name = inputs, output.name = 'theta')
  
  # Calculate the errors
  err = error.test.screen(mod.dist = mod.dist, mod.f1 = mod.f1, 
                    mod.f2 = mod.f2, mod.rad = mod.rad, 
                    mod.ang = mod.ang, nsamp = nsamp, nlevel = nlevel)
  
  # Change variable names - the error is the adaptively sampled error rate
  names(err)[1] = 'adapt'
  
  # Calculate the same for the random sampled datasets, looping around
  res = data.frame()
  for(i in 1:1000){
    # Randomly sampled dataset
    sampSet = c(1:nParet, sample(x = (nParet + 1):nrow(data.rand), size = nTest))
    rand = data.rand[sampSet, ]
    # New models
    mod.dist = fill.sample.mod(GPar.data = rand, input.name = inputs, output.name = 'dist')
    mod.f1   = fill.sample.mod(GPar.data = rand, input.name = inputs, output.name = 'f1.norm')
    mod.f2   = fill.sample.mod(GPar.data = rand, input.name = inputs, output.name = 'f2.norm')
    mod.rad  = fill.sample.mod(GPar.data = rand, input.name = inputs, output.name = 'rad')
    mod.ang  = fill.sample.mod(GPar.data = rand, input.name = inputs, output.name = 'theta')
    
    # Calculate error rates
    res0 = error.test.screen(mod.dist = mod.dist, mod.f1 = mod.f1, 
                  mod.f2 = mod.f2, mod.rad = mod.rad, 
                  mod.ang = mod.ang, nsamp = round(nsamp/10), nlevel = nlevel)
    res = rbind(res, res0)
  }
  
  # Aggregated information: median, 95% CI window, and percentile
  err$unc.min = NaN;   err$unc.med = NaN
  err$unc.max = NaN;   err$ptl = NaN
  # Store appropriate location
  for(i in 1:nrow(err)){
    # Find data of the same selection criteria and error type
    cls = err$class[i]
    tp  = err$typ[i]
    sub = dplyr::filter(res, class == cls, typ == tp)
    # Calculate percentiles
    err$unc.min[i] = unname(quantile(sub$prob, 0.025))
    err$unc.max[i] = unname(quantile(sub$prob, 0.975))
    err$unc.med[i] = median(sub$prob)
    err$ptl[i] = nrow(dplyr::filter(sub, prob > err$adapt[i])) / nrow(sub)
  }
  
  # Additional labeling
  err$n = nTest
  return(err)
}

```

```{r GP: Sample Size Effects on Error - Calculations}
if(!file.exists('ErrorRates-Nsamp.csv')){
  data.delta = read.csv('Ex_ZDT4/GPar_Accept_Delta1.csv')
  data.cutof = read.csv('Ex_ZDT4/GPar_Accept_Threshold.csv')
  data.radan = read.csv('Ex_ZDT4/GPar_Accept_Radius.csv')
  data.paret = read.csv('Ex_ZDT4/GPar_all_start.csv')
  data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
  data.rando = read.csv(file = 'GPar_Random.csv')
  
  # Sampling constants
  nsamp = 1000; nlevel = 5
  nSet = nrow(data.delta) - nrow(data.paret)
  
  # Parallel
  n.cores = parallel::detectCores() - 1
  my.cluster = parallel::makeCluster(n.cores, type = "PSOCK")
  doParallel::registerDoParallel(cl = my.cluster)
  
  err.n = foreach(i = seq(from = 3, to = nSet, by = 3), .combine = 'rbind') %dopar% {
    error.test.nSamp(data.dist = data.delta, data.f = data.cutof, 
                   data.rad = data.radan, data.rand = data.rando, 
                   nParet = nrow(data.paret), nTest = i, nsamp = nsamp, nlevel = nlevel)
  }
  parallel::stopCluster(cl = my.cluster)
  
  # Calculate the zero state
  err0 = error.test.screen(mod.dist = mod.dist.paret, mod.f1 = mod.f1.paret, 
                    mod.f2 = mod.f2.paret, mod.rad = mod.rad.paret, 
                    mod.ang = mod.ang.paret, nsamp = nsamp, nlevel = nlevel)
  err0$prob[err0$prob < 1e-8] = 1e-8 # Lower bound to avoid NaN when normalizing
  # Add the 0 state to the original dataset
  names(err0)[1] = 'adapt'; err0$n = 0
  err0$unc.min = err0$adapt; err0$unc.med = err0$adapt; 
  err0$unc.max = err0$adapt; err0$ptl = NaN
  err.n = rbind(err0, err.n)
  
  # Normalized versions of the variables
  update = c('class', 'typ', 'ptl', 'n')
  err.norm = err.n[, !names(err.n) %in% update]
  err.norm = err.norm / err0$adapt
  names(err.norm) = lapply(names(err.norm), FUN = function(x){paste(x, '.n', sep = "")})
  rm(update)
  
  err.n = cbind(err.n, err.norm)
  
  write.csv(cbind(err.n, err.norm), 'ErrorRates-Nsamp-ZDT.csv', row.names = F)
  rm(err.n, err.norm, err0)
}
```


```{r GP: Sample Size Effects on Error - Plotting}
err.n = read.csv('ErrorRates-Nsamp.csv')

# Absolute errors
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min, rev(err.n$unc.max)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Error Rate', color = NULL)

# Normalized errors
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL)

# Percentiles
ggplot(err.n) +
  geom_line(mapping = aes(x = n, y = ptl*100)) +
  geom_point(mapping = aes(x = n, y = ptl*100)) +
  facet_grid(typ~class) +
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = 'Additional Samples', y = '% of Random Samples worse than Adaptive Sampling', 
       color = NULL)


```

The type I (false positive) error is very high, indicating that there is a high likelihood that the model believes a point to satisfy the criteria when it in reality does not.
However, the overall error rate is still low, which is consistent with the fact that accepting a point is a rare event (approximately 5% of all points), so this is potentially a sampling bias.

Importantly, the adaptive sampling condition leads to more accurate results compared to no sampling (start) and random sampling, supporting the assertion that the sample method reduces the number of necessary samples for an accurate classifier.

## Marginals accuracy

```{r GP Marginals Comparison}
# Single variable conditionals compared to the expected conditionals by integration
Infer.plt = read.csv('../Ex_ZDT4/Marginals_delta_tru.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
            'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
            'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Pareto Distance'), color = '') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + 
  scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

Infer.plt = read.csv('../Ex_ZDT4/Marginals_cutof_tru.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
            'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
            'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Cutoff Thresholds'), color = '') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + 
  scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

Infer.plt = read.csv('../Ex_ZDT4/Marginals_radan_tru.csv')
ggplot() +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob - psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat == 'tru'), 
            mapping = aes(x = x, y = prob + psd, color = cat), linetype = 2) +
  geom_path(data = filter(Infer.plt, cat != 'tru'), 
            mapping = aes(x = x, y = prob, color = cat)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  scale_color_manual(labels = c('tru' = 'Expected Marginal', 'start' = 'Starting Dataset', 
            'adapt' = '+ Adaptive Sampling', 'rando' = '+ Random Sampling'),
                     values = c('tru' = 'black', 'start' = 'skyblue2', 
            'adapt' = 'red', 'rando' = 'green'),
                     breaks = c('tru', 'start', 'adapt', 'rando')) +
  guides(color = guide_legend(override.aes = list(linetype = c(2, 1, 1, 1)))) +
  labs(x = '', y = 'Probability of Acceptance', 
       subtitle = expression('Utopia Distance + Priority'), color = '') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

```

GP Importance ranking

```{r GP: Importance Ranking Functions}
## Importance by marginalization
marginal.import = function(marginal, typ){
  # Split up the marginal into the variables
  Infer.x1 = filter(marginal, var == 'x1')
  Infer.x2 = filter(marginal, var == 'x2')
  Infer.x3 = filter(marginal, var == 'x3')
  
  Import = data.frame(import = c(diff(range(Infer.x1$prob))*nrow(Infer.x1)/sum(Infer.x1$psd), 
                                     diff(range(Infer.x2$prob))*nrow(Infer.x2)/sum(Infer.x2$psd),
                                     diff(range(Infer.x3$prob))*nrow(Infer.x3)/sum(Infer.x3$psd)),
                      var = c('x1', 'x2', 'x3'),
                      typ)
  Import$sd = sqrt(c(max(Infer.x1$prob)*(1-max(Infer.x1$prob)) + min(Infer.x1$prob)*(1-min(Infer.x1$prob)) * 
                      diff(range(Infer.x1$prob)),
                    max(Infer.x2$prob)*(1-max(Infer.x2$prob)) + min(Infer.x2$prob)*(1-min(Infer.x2$prob)) * 
                      diff(range(Infer.x2$prob)),
                    max(Infer.x3$prob)*(1-max(Infer.x3$prob)) + min(Infer.x3$prob)*(1-min(Infer.x3$prob)) * 
                      diff(range(Infer.x3$prob)) )/1500 + 
                  c(var(Infer.x1$psd), var(Infer.x2$psd), var(Infer.x3$psd))/nrow(Infer.x1) )
  # Normalize so they all have the same maximum
  Import$r.import = Import$import/max(Import$import)
  Import$r.sd = Import$sd/max(Import$import)
  return(Import)
}

## Shapley value functions
# Function evaluations: output the probability
# Calculating the Shapley values and their standard error
shap = function(nsamp, mod.dist, mod.f1, mod.f2, mod.rad, mod.ang){
  # Set up the random points
  tot.rand = nsamp*50 # Repeat 50 times to get the standard error
  x0 = data.frame(x1 = runif(n = tot.rand, min = 0, max = 1),
                  x2 = runif(n = tot.rand, min = 0, max = 1),
                  x3 = runif(n = tot.rand, min = 0, max = 1))
  z1 = data.frame(x1 = x0$x1,
                  x2 = runif(n = tot.rand, min = 0, max = 1),
                  x3 = runif(n = tot.rand, min = 0, max = 1))
  z2 = data.frame(x1 = runif(n = tot.rand, min = 0, max = 1),
                  x2 = x0$x2,
                  x3 = runif(n = tot.rand, min = 0, max = 1))
  z3 = data.frame(x1 = runif(n = tot.rand, min = 0, max = 1),
                  x2 = runif(n = tot.rand, min = 0, max = 1),
                  x3 = x0$x3)
  # Shapley values: Pareto distance
  res = predict(object = mod.dist, newdata = x0, type = 'UK')
  p.x0 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  res = predict(object = mod.dist, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  res = predict(object = mod.dist, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  res = predict(object = mod.dist, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  # Matrix form to find the standard error
  p.x0 = matrix(data = p.x0, nrow = nsamp);  p.z1 = matrix(data = p.z1, nrow = nsamp)
  p.z2 = matrix(data = p.z2, nrow = nsamp);  p.z3 = matrix(data = p.z3, nrow = nsamp)
  # Calculate Shapley Values
  x1.m = apply(p.x0 - p.z1, 2, mean)
  x2.m = apply(p.x0 - p.z2, 2, mean)
  x3.m = apply(p.x0 - p.z3, 2, mean)
  x1.shap = mean(x1.m); x1.s = sd(x1.m)
  x2.shap = mean(x2.m); x2.s = sd(x2.m)
  x3.shap = mean(x3.m); x3.s = sd(x3.m)
  shap = data.frame(import = c(x1.shap, x2.shap, x3.shap), 
                    r.import = c(x1.shap, x2.shap, x3.shap)/max(c(x1.shap, x2.shap, x3.shap)),
                    sd = c(x1.s, x2.s, x3.s), r.sd = c(x1.s, x2.s, x3.s)/max(c(x1.shap, x2.shap, x3.shap)),
                    typ = 'Pareto Distance', var = c('x1', 'x2', 'x3'))
  
  # Shapley values: Threshold Cutoff
  res1 = predict(object = mod.f1, newdata = x0, type = 'UK')
  res2 = predict(object = mod.f2, newdata = x0, type = 'UK')
  p.x0 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  res1 = predict(object = mod.f1, newdata = z1, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  res1 = predict(object = mod.f1, newdata = z2, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  res1 = predict(object = mod.f1, newdata = z3, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  
  p.x0 = matrix(data = p.x0, nrow = nsamp);  p.z1 = matrix(data = p.z1, nrow = nsamp)
  p.z2 = matrix(data = p.z2, nrow = nsamp);  p.z3 = matrix(data = p.z3, nrow = nsamp)
  
  x1.m = apply(p.x0 - p.z1, 2, mean)
  x2.m = apply(p.x0 - p.z2, 2, mean)
  x3.m = apply(p.x0 - p.z3, 2, mean)
  x1.shap = mean(x1.m); x1.s = sd(x1.m)
  x2.shap = mean(x2.m); x2.s = sd(x2.m)
  x3.shap = mean(x3.m); x3.s = sd(x3.m)
  shap = rbind(shap, 
               data.frame(import = c(x1.shap, x2.shap, x3.shap), 
                          r.import = c(x1.shap, x2.shap, x3.shap)/max(c(x1.shap, x2.shap, x3.shap)),
                    sd = c(x1.s, x2.s, x3.s), r.sd = c(x1.s, x2.s, x3.s)/max(c(x1.shap, x2.shap, x3.shap)),
                    typ = 'Threshold Cutoff', var = c('x1', 'x2', 'x3')))
  
  # Shapley value: Utopia distance + Priority
  res1 = predict(object = mod.rad, newdata = x0, type = 'UK')
  res2 = predict(object = mod.ang, newdata = x0, type = 'UK')
  p.x0 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  res1 = predict(object = mod.rad, newdata = z1, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  res1 = predict(object = mod.rad, newdata = z2, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  res1 = predict(object = mod.rad, newdata = z3, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  
  p.x0 = matrix(data = p.x0, nrow = nsamp);  p.z1 = matrix(data = p.z1, nrow = nsamp)
  p.z2 = matrix(data = p.z2, nrow = nsamp);  p.z3 = matrix(data = p.z3, nrow = nsamp)
  
  x1.m = apply(p.x0 - p.z1, 2, mean)
  x2.m = apply(p.x0 - p.z2, 2, mean)
  x3.m = apply(p.x0 - p.z3, 2, mean)
  x1.shap = mean(x1.m); x1.s = sd(x1.m)
  x2.shap = mean(x2.m); x2.s = sd(x2.m)
  x3.shap = mean(x3.m); x3.s = sd(x3.m)
  shap = rbind(shap, 
               data.frame(import = c(x1.shap, x2.shap, x3.shap), 
                          r.import = c(x1.shap, x2.shap, x3.shap)/max(c(x1.shap, x2.shap, x3.shap)),
                    sd = c(x1.s, x2.s, x3.s), r.sd = c(x1.s, x2.s, x3.s)/max(c(x1.shap, x2.shap, x3.shap)),
                    typ = 'Utopia Distance', var = c('x1', 'x2', 'x3')))
  
  return(shap)
}

# Sobol method - Total effect index
# Sobol method - Total effect index
sobol = function(nsamp, mod.dist, mod.f1, mod.f2, mod.rad, mod.ang){
  # Set up the random points - 2 sets
  tot.rand = nsamp*50 # Repeat 50 times to get the standard error like Shapley values
  # tot.rand = nsamp
  xA = data.frame(x1 = runif(n = tot.rand, min = 0, max = 1),
                  x2 = runif(n = tot.rand, min = 0, max = 1),
                  x3 = runif(n = tot.rand, min = 0, max = 1))
  xB = data.frame(x1 = runif(n = tot.rand, min = 0, max = 1),
                  x2 = runif(n = tot.rand, min = 0, max = 1),
                  x3 = runif(n = tot.rand, min = 0, max = 1))
  # Single variable exchanges
  z1 = data.frame(x1 = xA$x1, x2 = xB$x2, x3 = xB$x3)
  z2 = data.frame(x1 = xB$x1, x2 = xA$x2, x3 = xB$x3)
  z3 = data.frame(x1 = xB$x1, x2 = xB$x2, x3 = xA$x3)
  
  # Calculate the probabilities
  # Pareto distance
  res = predict(object = mod.dist, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res$mean - 0.5, sd = res$sd)
  
  res = predict(object = mod.dist, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res$mean - 0.5, sd = res$sd)
  
  res = predict(object = mod.dist, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res$mean - 0.5, sd = res$sd)
  
  res = predict(object = mod.dist, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res$mean - 0.5, sd = res$sd)
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol3 = matrix((p.z3 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2), mean(sobol3))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp),
               sd(colSums(sobol3)/nsamp))
  delta = data.frame(var = c('x1', 'x2', 'x3'), typ = 'Pareto Distance',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol), r.sd = sobol.sd/max(sobol))

  # Threshold Cutoff
  res1 = predict(object = mod.f1, newdata = xA, type = 'UK')
  res2 = predict(object = mod.f2, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  
  res1 = predict(object = mod.f1, newdata = z1, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  
  res1 = predict(object = mod.f1, newdata = z2, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  
  res1 = predict(object = mod.f1, newdata = z3, type = 'UK')
  res2 = predict(object = mod.f2, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    pnorm(q = 0, mean = res2$mean - 1, sd = res2$sd)
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol3 = matrix((p.z3 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2), mean(sobol3))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp),
               sd(colSums(sobol3)/nsamp))
  cutof = data.frame(var = c('x1', 'x2', 'x3'), typ = 'Threshold Cutoff',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol), r.sd = sobol.sd/max(sobol))

  # Utopia distance + Priority
  res1 = predict(object = mod.rad, newdata = xA, type = 'UK')
  res2 = predict(object = mod.ang, newdata = xA, type = 'UK')
  p.xA = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  res1 = predict(object = mod.rad, newdata = z1, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z1, type = 'UK')
  p.z1 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))

  res1 = predict(object = mod.rad, newdata = z2, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z2, type = 'UK')
  p.z2 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  res1 = predict(object = mod.rad, newdata = z3, type = 'UK')
  res2 = predict(object = mod.ang, newdata = z3, type = 'UK')
  p.z3 = pnorm(q = 0, mean = res1$mean - 1, sd = res1$sd) * 
    (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  
  sobol1 = matrix((p.z1 - p.xA)^2/2, nrow = nsamp)
  sobol2 = matrix((p.z2 - p.xA)^2/2, nrow = nsamp)
  sobol3 = matrix((p.z3 - p.xA)^2/2, nrow = nsamp)
  sobol = c(mean(sobol1), mean(sobol2), mean(sobol3))
  sobol.sd = c(sd(colSums(sobol1)/nsamp), sd(colSums(sobol2)/nsamp),
               sd(colSums(sobol3)/nsamp))
  radan = data.frame(var = c('x1', 'x2', 'x3'), typ = 'Utopia Distance',
                     import = sobol, sd = sobol.sd,
                     r.import = sobol/max(sobol), r.sd = sobol.sd/max(sobol))
  
  return(rbind(delta, cutof, radan))
}

```

```{r GP: Importance Ranking Calculation}
# Due to the large number of Monte Carlo samples, only performing this calculation
# if it hasn't been done before and storing it as a file
if(!file.exists('GP_ImportanceRank.csv')){
  # Load data; only want the adaptive sampling marginal for the importance ranking
  Infer.delta = read.csv('../Ex_ZDT4/Marginals_delta_tru.csv')
  Infer.delta = filter(Infer.delta, cat == 'adapt')
  Infer.cutof = read.csv('../Ex_ZDT4/Marginals_cutof_tru.csv')
  Infer.cutof = filter(Infer.cutof, cat == 'adapt')
  Infer.radan = read.csv('../Ex_ZDT4/Marginals_radan_tru.csv')
  Infer.radan = filter(Infer.radan, cat == 'adapt')
  
  # GP marginals
  import.delta = marginal.import(marginal = Infer.delta, typ = 'Pareto Distance')
  import.cutof = marginal.import(marginal = Infer.cutof, typ = 'Threshold Cutoff')
  import.radan = marginal.import(marginal = Infer.radan, typ = 'Utopia Distance')
  
  import.GP = rbind(import.delta, import.cutof, import.radan)
  
  import.shap = shap(nsamp = 1500, mod.dist = mod.dist.adapt, 
       mod.f1 = mod.f1.adapt, mod.f2 = mod.f2.adapt, 
       mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
  import.sobol = sobol(nsamp = 1500, mod.dist = mod.dist.adapt, 
       mod.f1 = mod.f1.adapt, mod.f2 = mod.f2.adapt, 
       mod.rad = mod.rad.adapt, mod.ang = mod.ang.adapt)
  
  # Combine datasets
  import.GP$method = 'Marginalization'
  import.shap$method = 'Shapley Value'
  import.sobol$method = 'Sobol Total Effect'
  
  write.csv(rbind(import.GP, import.shap, import.sobol), 'GP_ImportanceRank.csv', row.names = F)
}
```


```{r GP: Importance Ranking Plotting}
import.GP = read.csv('GP_ImportanceRank.csv')

ggplot(import.GP) +
  geom_col(mapping = aes(x = var, y = import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = import + sd, ymin = import - sd)) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(y = 'Importance', x = NULL)

ggplot(import.GP) +
  geom_col(mapping = aes(x = var, y = r.import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = r.import + r.sd, ymin = r.import - r.sd), width = 0.5) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = 'Relative Importance') +
  guides(fill = 'none') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3])))

# Separate scales for each column
g.delta = ggplot(filter(import.GP, typ == 'Pareto Distance')) +
  geom_col(mapping = aes(x = var, y = r.import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = r.import + r.sd, ymin = r.import - r.sd), width = 0.5) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = 'Relative Importance') +
  guides(fill = 'none') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3])))
g.cutof = ggplot(filter(import.GP, typ == 'Threshold Cutoff')) +
  geom_col(mapping = aes(x = var, y = r.import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = r.import + r.sd, ymin = r.import - r.sd), width = 0.5) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = '') +
  guides(fill = 'none') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3])))
g.radan = ggplot(filter(import.GP, typ == 'Utopia Distance')) +
  geom_col(mapping = aes(x = var, y = r.import, fill = var)) +
  geom_errorbar(mapping = aes(x = var, ymax = r.import + r.sd, ymin = r.import - r.sd), width = 0.5) +
  facet_grid(method~typ, scales = 'free_y') +
  labs(x = '', y = '') +
  guides(fill = 'none') +
  scale_x_discrete(labels = c(expression('x'[1]), expression('x'[2]), expression('x'[3])))

g.delta + g.cutof + g.radan
rm(g.delta, g.cutof, g.radan)

```

The Shapley values show very poor reproducibility, indicating they would require many more samples to have comparable variances.

# Existing Method: Support Vector Machines
SVM methods require existing labels for the points; in this case, the labels are whether or not the point meets the same three selection criteria as tested with the new GP method.

```{r SVM: Functions}
SVM.mod.all = function(input.data){
  # Given input data, find all 4 default models and return as a list
  fit.rad = e1071::svm(as.factor(cat) ~ x1*x2*x3, data = input.data[,c('x1', 'x2', 'x3', 'cat')], 
            scale = FALSE, kernel = "radial", cost = 5)
  fit.lin = e1071::svm(as.factor(cat) ~ x1*x2*x3, data = input.data[,c('x1', 'x2', 'x3', 'cat')], 
            scale = FALSE, kernel = "linear", cost = 5)
  fit.pol = e1071::svm(as.factor(cat) ~ x1*x2*x3, data = input.data[,c('x1', 'x2', 'x3', 'cat')], 
            scale = FALSE, kernel = "polynomial", cost = 5, degree = 3.5)
  fit.sig = e1071::svm(as.factor(cat) ~ x1*x2*x3, data = input.data[,c('x1', 'x2', 'x3', 'cat')], 
            scale = FALSE, kernel = "sigmoid", cost = 5)
  return(list(rad = fit.rad, lin = fit.lin, pol = fit.pol, sig = fit.sig))
}

SVM.err = function(nsamp, mod.list, nlevel){
  # Obtain sample of actual results
  err.samp = error.test.eval(nlevel = nlevel, nsamp = nsamp)
  # Accepted points
  err.samp$delta = 0; err.samp$delta[err.samp$dist <= 0.5] = 1
  err.samp$cutof = 0; 
  err.samp$cutof[err.samp$f1.norm <= 1 & err.samp$f2.norm <= 1] = 1
  err.samp$radan = 0; 
  err.samp$radan[err.samp$rad <= 1 & err.samp$ang > 20] = 1
  
  ## Model estimates: delta from Pareto front
  err.samp$rad = as.numeric(predict(object = mod.list$rad.delta, 
                                    newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$lin = as.numeric(predict(object = mod.list$lin.delta, 
                                    newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$pol = as.numeric(predict(object = mod.list$pol.delta, 
                                    newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$sig = as.numeric(predict(object = mod.list$sig.delta, 
                                    newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  # Error rate
  err.rate = c(nrow(dplyr::filter(err.samp, rad == 1 & delta == 0)) +
                 nrow(dplyr::filter(err.samp, rad == 0 & delta == 1)),
               nrow(dplyr::filter(err.samp, lin == 1 & delta == 0)) +
                 nrow(dplyr::filter(err.samp, lin == 0 & delta == 1)),
               nrow(dplyr::filter(err.samp, pol == 1 & delta == 0)) + 
                 nrow(dplyr::filter(err.samp, pol == 0 & delta == 1)),
               nrow(dplyr::filter(err.samp, sig == 1 & delta == 0)) + 
                 nrow(dplyr::filter(err.samp, sig == 0 & delta == 1)))
  err.rate = err.rate/nrow(err.samp)
  # Type 1 error = P[should reject | accepted] = P[should reject but accepted] P[should reject] / P[accepted]
  typ1 = c(nrow(dplyr::filter(err.samp, rad == 1, delta == 0))/
             max(nrow(dplyr::filter(err.samp, rad == 1)), 1),
           nrow(dplyr::filter(err.samp, lin == 1, delta == 0))/
             max(nrow(dplyr::filter(err.samp, lin == 1)), 1),
           nrow(dplyr::filter(err.samp, pol == 1, delta == 0))/
             max(nrow(dplyr::filter(err.samp, pol == 1)), 1),
           nrow(dplyr::filter(err.samp, sig == 1, delta == 0))/
             max(nrow(dplyr::filter(err.samp, sig == 1)), 1) ) * 
    nrow(dplyr::filter(err.samp, delta == 0))/nrow(err.samp)
  # Type 2 error = P[should accept | rejected] = P[should accept but rejected] P[should accept] / P[rejected]
  typ2 = c(nrow(dplyr::filter(err.samp, rad == 0, delta == 1))/
             max(nrow(dplyr::filter(err.samp, rad == 0)), 1),
           nrow(dplyr::filter(err.samp, lin == 0, delta == 1))/
             max(nrow(dplyr::filter(err.samp, lin == 0)), 1),
           nrow(dplyr::filter(err.samp, pol == 0, delta == 1))/
             max(nrow(dplyr::filter(err.samp, pol == 0)), 1),
           nrow(dplyr::filter(err.samp, sig == 0, delta == 1))/
             max(nrow(dplyr::filter(err.samp, sig == 0)), 1) ) * 
    nrow(dplyr::filter(err.samp, delta == 1))/nrow(err.samp)
  delta = c(err.rate, typ1, typ2)

  ## Model estimates: normalized cutoff
  err.samp$rad = as.numeric(predict(object = mod.list$rad.cutof, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$lin = as.numeric(predict(object = mod.list$lin.cutof, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$pol = as.numeric(predict(object = mod.list$pol.cutof, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$sig = as.numeric(predict(object = mod.list$sig.cutof, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  # Error rate
  err.rate = c(nrow(dplyr::filter(err.samp, rad == 1 & cutof == 0)) + 
                 nrow(dplyr::filter(err.samp, rad == 0 & cutof == 1)),
               nrow(dplyr::filter(err.samp, lin == 1 & cutof == 0)) + 
                 nrow(dplyr::filter(err.samp, lin == 0 & cutof == 1)),
               nrow(dplyr::filter(err.samp, pol == 1 & cutof == 0)) + 
                 nrow(dplyr::filter(err.samp, pol == 0 & cutof == 1)),
               nrow(dplyr::filter(err.samp, sig == 1 & cutof == 0)) + 
                 nrow(dplyr::filter(err.samp, sig == 0 & cutof == 1)))
  err.rate = err.rate/nrow(err.samp)
  # Type 1 error = P[should reject | accepted] = P[should reject but accepted] P[should reject] / P[accepted]
  typ1 = c(nrow(dplyr::filter(err.samp, rad == 1, cutof == 0))/
             max(nrow(dplyr::filter(err.samp, rad == 1)), 1),
           nrow(dplyr::filter(err.samp, lin == 1, cutof == 0))/
             max(nrow(dplyr::filter(err.samp, lin == 1)), 1),
           nrow(dplyr::filter(err.samp, pol == 1, cutof == 0))/
             max(nrow(dplyr::filter(err.samp, pol == 1)), 1),
           nrow(dplyr::filter(err.samp, sig == 1, cutof == 0))/
             max(nrow(dplyr::filter(err.samp, sig == 1)), 1) ) * 
    nrow(dplyr::filter(err.samp, cutof == 0))/nrow(err.samp)
  # Type 2 error = P[should accept | rejected] = P[should accept but rejected] P[should accept] / P[rejected]
  typ2 = c(nrow(dplyr::filter(err.samp, rad == 0, cutof == 1))/
             max(nrow(dplyr::filter(err.samp, rad == 0)), 1),
           nrow(dplyr::filter(err.samp, lin == 0, cutof == 1))/
             max(nrow(dplyr::filter(err.samp, lin == 0)), 1),
           nrow(dplyr::filter(err.samp, pol == 0, cutof == 1))/
             max(nrow(dplyr::filter(err.samp, pol == 0)), 1),
           nrow(dplyr::filter(err.samp, sig == 0, cutof == 1))/
             max(nrow(dplyr::filter(err.samp, sig == 0)), 1) ) * 
    nrow(dplyr::filter(err.samp, cutof == 1))/nrow(err.samp)
  cutof = c(err.rate, typ1, typ2)

  ## Model estimates: normalized cutoff
  err.samp$rad = as.numeric(predict(object = mod.list$rad.radan, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$lin = as.numeric(predict(object = mod.list$lin.radan, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$pol = as.numeric(predict(object = mod.list$pol.radan, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  err.samp$sig = as.numeric(predict(object = mod.list$sig.radan, newdata = err.samp[,c('x1', 'x2', 'x3')]))-1
  # Error rate
  err.rate = c(nrow(dplyr::filter(err.samp, rad == 1 & radan == 0)) + 
                 nrow(dplyr::filter(err.samp, rad == 0 & radan == 1)),
               nrow(dplyr::filter(err.samp, lin == 1 & radan == 0)) + 
                 nrow(dplyr::filter(err.samp, lin == 0 & radan == 1)),
               nrow(dplyr::filter(err.samp, pol == 1 & radan == 0)) + 
                 nrow(dplyr::filter(err.samp, pol == 0 & radan == 1)),
               nrow(dplyr::filter(err.samp, sig == 1 & radan == 0)) + 
                 nrow(dplyr::filter(err.samp, sig == 0 & radan == 1)))
  err.rate = err.rate/nrow(err.samp)
  # Type 1 error = P[should reject | accepted] = P[should reject but accepted] P[should reject] / P[accepted]
  # P[accepted] is assumed to be non-zero, so if no points are accepted, then assume at least 1 is
  typ1 = c(nrow(dplyr::filter(err.samp, rad == 1, radan == 0))/
             max(nrow(dplyr::filter(err.samp, rad == 1)), 1),
           nrow(dplyr::filter(err.samp, lin == 1, radan == 0))/
             max(nrow(dplyr::filter(err.samp, lin == 1)), 1),
           nrow(dplyr::filter(err.samp, pol == 1, radan == 0))/
             max(nrow(dplyr::filter(err.samp, pol == 1)), 1),
           nrow(dplyr::filter(err.samp, sig == 1, radan == 0))/
             max(nrow(dplyr::filter(err.samp, sig == 1)), 1) ) * 
    nrow(dplyr::filter(err.samp, radan == 0))/nrow(err.samp)
  # Type 2 error = P[should accept | rejected] = P[should accept but rejected] P[should accept] / P[rejected]
  typ2 = c(nrow(dplyr::filter(err.samp, rad == 0, radan == 1))/
             max(nrow(dplyr::filter(err.samp, rad == 0)), 1),
           nrow(dplyr::filter(err.samp, lin == 0, radan == 1))/
             max(nrow(dplyr::filter(err.samp, lin == 0)), 1),
           nrow(dplyr::filter(err.samp, pol == 0, radan == 1))/
             max(nrow(dplyr::filter(err.samp, pol == 0)), 1),
           nrow(dplyr::filter(err.samp, sig == 0, radan == 1))/
             max(nrow(dplyr::filter(err.samp, sig == 0)), 1) ) * 
    nrow(dplyr::filter(err.samp, radan == 1))/nrow(err.samp)
  radan = c(err.rate, typ1, typ2)

  return(data.frame(prob = c(delta, cutof, radan),
         class = c(rep('Pareto Distance', 12), rep('Threshold Cutoff', 12), 
                   rep('Utopia Distance', 12)),
         typ = c(rep('Total', 4), rep('False Positive', 4), rep('False Negative', 4)),
         mod = c('rad', 'lin', 'pol', 'sig')))
}


SVM.err.nSamp = function(data.dist, data.f, data.rad, data.rand, 
                         nParet, nTest, nsamp, nlevel){
  # Only the correct number of samples - adaptive samples
  data.delta = data.dist[1:(nParet + nTest),]
  data.cutof = data.f[1:(nParet + nTest),]
  data.radan = data.rad[1:(nParet + nTest),]
  
  # Model generation
  mod.adapt.delta = SVM.mod.all(input.data = data.delta)
  mod.adapt.cutof = SVM.mod.all(input.data = data.cutof)
  mod.adapt.radan = SVM.mod.all(input.data = data.radan)
  # Aggregation into a single list
  names(mod.adapt.delta) = paste(names(mod.adapt.delta), '.delta', sep = '')
  names(mod.adapt.cutof) = paste(names(mod.adapt.cutof), '.radan', sep = '')
  names(mod.adapt.radan) = paste(names(mod.adapt.radan), '.cutof', sep = '')
  SVMmod.list.adapt = append(mod.adapt.delta, mod.adapt.cutof)
  SVMmod.list.adapt = append(SVMmod.list.adapt, mod.adapt.radan)
  
  # Calculate classification errors
  err = SVM.err(nsamp = nsamp, mod.list = SVMmod.list.adapt, nlevel = nlevel)
  # Update names - this is the adaptively sampled probability
  names(err)[1] = 'adapt'
  
  # Calculate models with random samples
  err.rando = data.frame()
  for(i in 1:1000){
    # Randomly sampled dataset
    sampSet = c(1:nParet, sample(x = (nParet + 1):nrow(data.rand), size = nTest))
    rand = data.rand[sampSet, ]
    # Classifiers
    rand$cat = 0; rand$cat[rand$dist <= 0.5] = 1
    mod.rando.delta = SVM.mod.all(input.data = rand)
    rand$cat = 0; rand$cat[rand$f1.norm <= 1 & rand$f2.norm <= 1] = 1
    mod.rando.cutof = SVM.mod.all(input.data = rand)
    rand$cat = 0; rand$cat[sqrt(rand$f1.norm^2 + rand$f2.norm^2) <= 1 & rand$theta > 20] = 1
    mod.rando.radan = SVM.mod.all(input.data = rand)
    # Aggregation into a single list
    names(mod.rando.delta) = paste(names(mod.rando.delta), '.delta', sep = '')
    names(mod.rando.cutof) = paste(names(mod.rando.cutof), '.radan', sep = '')
    names(mod.rando.radan) = paste(names(mod.rando.radan), '.cutof', sep = '')
    SVMmod.list.rando = append(mod.rando.delta, mod.rando.cutof)
    SVMmod.list.rando = append(SVMmod.list.rando, mod.rando.radan)
    
    # Calculate errors
    res0 = SVM.err(nsamp = round(nsamp/10), mod.list = SVMmod.list.rando, nlevel = nlevel)
    # Store for later
    err.rando = rbind(err.rando, res0)
  }
  
  # Aggregated information: median, 95% CI window, and percentile
  err$unc.min = NaN;  err$unc.med = NaN
  err$unc.max = NaN;  err$ptl = NaN
  # Store appropriate location
  for(i in 1:nrow(err)){
    # Find data of the same selection criteria and error type
    cls = err$class[i]
    tp  = err$typ[i]
    mdl = err$mod[i]
    sub = dplyr::filter(err.rando, class == cls, typ == tp, mod == mdl)
    # Calculate percentiles
    err$unc.min[i] = unname(quantile(sub$prob, 0.025))
    err$unc.max[i] = unname(quantile(sub$prob, 0.975))
    err$unc.med[i] = median(sub$prob)
    err$ptl[i] = nrow(dplyr::filter(sub, prob > err$adapt[i])) / nrow(sub)
  }
  
  # Additional labeling
  err$n = nTest
  return(err)
}

```

```{r SVM: Sample Size Effects on Error - Calculations}
if(!file.exists('SVM_ErrorRate-Nsamp.csv')){
  data.delta = read.csv('Ex_ZDT4/GPar_Accept_Delta1.csv')
  data.cutof = read.csv('Ex_ZDT4/GPar_Accept_Threshold.csv')
  data.radan = read.csv('Ex_ZDT4/GPar_Accept_Radius.csv')
  data.paret = read.csv('Ex_ZDT4/GPar_all_start.csv')
  data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
  data.rando = read.csv(file = 'GPar_Random.csv')
  
  # Classifier
  data.delta$cat = 0; data.cutof$cat = 0; data.radan$cat = 0
  data.delta$cat[data.delta$dist <= 0.5] = 1
  data.cutof$cat[data.cutof$f1.norm <= 1 & data.cutof$f2.norm <= 1] = 1
  data.radan$cat[sqrt(data.radan$f1.norm^2 + data.radan$f2.norm^2) <= 1 & 
                   data.radan$theta > 20] = 1
  
  # Sampling constants
  nsamp = 1000; nlevel = 5
  nSet = nrow(data.delta) - nrow(data.paret)
  
  # Parallel
  n.cores = parallel::detectCores() - 1
  my.cluster = parallel::makeCluster(n.cores, type = "PSOCK")
  doParallel::registerDoParallel(cl = my.cluster)
  
  err.n = foreach(i = seq(from = 3, to = nSet, by = 3), .combine = 'rbind') %dopar% {
  # i = 1
    SVM.err.nSamp(data.dist = data.delta, data.f = data.cutof, 
                   data.rad = data.radan, data.rand = data.rando, 
                   nParet = nrow(data.paret), nTest = i, nsamp = nsamp, nlevel = nlevel)
  }
  parallel::stopCluster(cl = my.cluster)
  
  ## Calculate the zero state
  # Model generation
  data.paret$cat = 0; data.paret$cat[data.paret$dist <= 0.5] = 1
  mod.paret.delta = SVM.mod.all(input.data = data.paret)
  data.paret$cat = 0
  data.paret$cat[data.paret$f1.norm <= 1 & data.paret$f2.norm <= 1] = 1
  mod.paret.cutof = SVM.mod.all(input.data = data.paret)
  data.paret$cat = 0
  data.paret$cat[sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2) <= 1 & 
                   data.paret$theta > 20] = 1
  mod.paret.radan = SVM.mod.all(input.data = data.paret)
  # Aggregation into a single list
  names(mod.paret.delta) = paste(names(mod.paret.delta), '.delta', sep = '')
  names(mod.paret.cutof) = paste(names(mod.paret.cutof), '.radan', sep = '')
  names(mod.paret.radan) = paste(names(mod.paret.radan), '.cutof', sep = '')
  SVMmod.list.paret = append(mod.paret.delta, mod.paret.cutof)
  SVMmod.list.paret = append(SVMmod.list.paret, mod.paret.radan)
  # Calculate classification errors
  err0 = SVM.err(nsamp = nsamp, mod.list = SVMmod.list.paret, nlevel = nlevel)
  err0$prob[err0$prob < 1e-8] = 1e-8 # Lower bound to avoid NaN when normalizing
  
  # Add the 0 state to the original dataset
  names(err0)[1] = 'adapt'; err0$n = 0
  err0$unc.min = err0$adapt; err0$unc.med = err0$adapt; 
  err0$unc.max = err0$adapt; err0$ptl = NaN
  err.n = rbind(err0, err.n)
  
  # Normalized versions of the variables
  update = c('class', 'typ', 'ptl', 'n', 'mod')
  err.norm = err.n[, !names(err.n) %in% update]
  err.norm = err.norm / err0$adapt
  names(err.norm) = lapply(names(err.norm), FUN = function(x){paste(x, '.n', sep = "")})
  rm(update)
  
  err.n = cbind(err.n, err.norm)
  err.all = err.n
  write.csv(cbind(err.n, err.norm), 'SVM_ErrorRate-Nsamp.csv', row.names = F)
  rm(err.n, err.norm, err0)
}
```

```{r SVM: Sample Size Effects on Error - Plotting}
err.all = read.csv('SVM_ErrorRate-Nsamp.csv')

# Normalized errors for each kernel function
err.n = filter(err.all, mod == unique(mod)[1])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$mod))

err.n = filter(err.all, mod == unique(mod)[2])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$mod))

err.n = filter(err.all, mod == unique(mod)[3])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$mod))

err.n = filter(err.all, mod == unique(mod)[4])
ggplot() +
  geom_polygon(data = data.frame(x = c(err.n$n, rev(err.n$n)),
                                 y = c(err.n$unc.min.n, rev(err.n$unc.max.n)),
                                 typ = c(err.n$typ, rev(err.n$typ)),
                                 class = c(err.n$class, rev(err.n$class))),
               mapping = aes(x = x, y = y, fill = 'Random'), alpha = 0.25) +
  geom_line(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_line(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  geom_point(data = err.n, mapping = aes(x = n, y = unc.med.n, color = 'Random')) +
  geom_point(data = err.n, mapping = aes(x = n, y = adapt.n, color = 'Adaptive')) +
  facet_grid(typ~class, scale = 'free') +
  scale_color_manual(values = c('Random' = 'red', 'Adaptive' = 'blue')) +
  scale_fill_manual(values = c('Random' = 'red')) +
  guides(fill = 'none') +
  labs(x = 'Additional Samples', y = 'Relative Error Rate', color = NULL,
       subtitle = unique(err.n$mod))

# Percentile Ranking
err.n = filter(err.all, typ == 'Total')
ggplot() +
  geom_line(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  geom_point(data = err.n, mapping = aes(x = n, y = ptl*100)) +
  facet_grid(mod~class) +
  guides(fill = 'none') + 
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = 'Additional Samples', y = '% of Random Samples worse than Adaptive Sampling', 
       color = NULL, title = 'Misclassification Errors - SVM Kernels')

```

## Calculating marginals with the SVM

```{r SVM: Model generation}
# Load datasets prior to refinement
data.delta = read.csv('../Ex_ZDT4/GPar_Accept_Delta1.csv')
data.cutof = read.csv('../Ex_ZDT4/GPar_Accept_Threshold.csv')
data.radan = read.csv('../Ex_ZDT4/GPar_Accept_Radius.csv')
data.paret = read.csv('../Ex_ZDT4/GPar_all_start.csv')
data.paret$rad = sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2)
data.rando = read.csv(file = 'GPar_Random.csv')

# Load the estimate of the Pareto frontier
GPar.front = read.csv(file = '../Ex_ZDT4/GPar_fnt_start.csv')
inputs = c('x1', 'x2', 'x3')

# Define acceptance: Pareto distance
data.paret$cat = 0; data.delta$cat = 0; data.rando$cat = 0
data.paret$cat[data.paret$dist <= 0.5] = 1
data.delta$cat[data.delta$dist <= 0.5] = 1
data.rando$cat[data.rando$dist <= 0.5] = 1
SVMmod.delta.paret = SVM.mod.all(input.data = data.paret)
SVMmod.delta.adapt = SVM.mod.all(input.data = data.delta)
SVMmod.delta.rando = SVM.mod.all(input.data = data.rando)

# Define acceptance: cutoff
data.paret$cat = 0; data.cutof$cat = 0; data.rando$cat = 0
data.paret$cat[data.paret$f1.norm <= 1 & data.paret$f2.norm <= 1] = 1
data.cutof$cat[data.cutof$f1.norm <= 1 & data.cutof$f2.norm <= 1] = 1
data.rando$cat[data.rando$f1.norm <= 1 & data.rando$f2.norm <= 1] = 1
SVMmod.cutof.paret = SVM.mod.all(input.data = data.paret)
SVMmod.cutof.adapt = SVM.mod.all(input.data = data.cutof)
SVMmod.cutof.rando = SVM.mod.all(input.data = data.rando)

# Define acceptance: utopia distance and priority
data.paret$cat = 0; data.radan$cat = 0; data.rando$cat = 0
data.paret$cat[sqrt(data.paret$f1.norm^2 + data.paret$f2.norm^2) <= 1 & data.paret$theta > 20] = 1
data.radan$cat[sqrt(data.radan$f1.norm^2 + data.radan$f2.norm^2) <= 1 & data.radan$theta > 20] = 1
data.rando$cat[sqrt(data.rando$f1.norm^2 + data.rando$f2.norm^2) <= 1 & data.rando$theta > 20] = 1
SVMmod.radan.paret = SVM.mod.all(input.data = data.paret)
SVMmod.radan.adapt = SVM.mod.all(input.data = data.radan)
SVMmod.radan.rando = SVM.mod.all(input.data = data.rando)

# Combine each dataset's models to reduce the number of relevant models
names(SVMmod.delta.paret) = paste(names(SVMmod.delta.paret), '.delta', sep = '')
names(SVMmod.radan.paret) = paste(names(SVMmod.radan.paret), '.radan', sep = '')
names(SVMmod.cutof.paret) = paste(names(SVMmod.cutof.paret), '.cutof', sep = '')
SVMmod.list.paret = append(SVMmod.delta.paret, SVMmod.radan.paret)
SVMmod.list.paret = append(SVMmod.list.paret, SVMmod.cutof.paret)

names(SVMmod.delta.adapt) = paste(names(SVMmod.delta.adapt), '.delta', sep = '')
names(SVMmod.radan.adapt) = paste(names(SVMmod.radan.adapt), '.radan', sep = '')
names(SVMmod.cutof.adapt) = paste(names(SVMmod.cutof.adapt), '.cutof', sep = '')
SVMmod.list.adapt = append(SVMmod.delta.adapt, SVMmod.radan.adapt)
SVMmod.list.adapt = append(SVMmod.list.adapt, SVMmod.cutof.adapt)

names(SVMmod.delta.rando) = paste(names(SVMmod.delta.rando), '.delta', sep = '')
names(SVMmod.radan.rando) = paste(names(SVMmod.radan.rando), '.radan', sep = '')
names(SVMmod.cutof.rando) = paste(names(SVMmod.cutof.rando), '.cutof', sep = '')
SVMmod.list.rando = append(SVMmod.delta.rando, SVMmod.radan.rando)
SVMmod.list.rando = append(SVMmod.list.rando, SVMmod.cutof.rando)

```

```{r SVM: Marginal Function}
SVM.marginal.calc = function(mod.list){
  x.rng = seq(from = 0, to = 1, length.out = 75)
  svm.margin = data.frame()
  nsamp = 1500
  for(x in x.rng){
    # x1 marginal
    temp.frame = data.frame(x1 = x, x2 = runif(n = nsamp, min = 0, max = 1),
                            x3 = runif(n = nsamp, min = 0, max = 1))
    for(i in 1:length(mod.list)){
      res = as.numeric(predict(object = mod.list[[i]], newdata = temp.frame[,c('x1', 'x2', 'x3')]))-1
      mod = substr(names(mod.list)[i], 0, 3)
      class = substr(names(mod.list)[i], 5, 9)
      svm.margin = rbind(svm.margin, 
                         data.frame(x = x, var = 'x1', prob = sum(res)/length(res), mod, class))
    }
    # x2 marginal
    temp.frame = data.frame(x2 = x, x1 = runif(n = nsamp, min = 0, max = 1),
                            x3 = runif(n = nsamp, min = 0, max = 1))
    for(i in 1:length(mod.list)){
      res = as.numeric(predict(object = mod.list[[i]], newdata = temp.frame[,c('x1', 'x2', 'x3')]))-1
      mod = substr(names(mod.list)[i], 0, 3)
      class = substr(names(mod.list)[i], 5, 9)
      svm.margin = rbind(svm.margin, 
                         data.frame(x = x, var = 'x2', prob = sum(res)/length(res), mod, class))
    }
    # x3 marginal
    temp.frame = data.frame(x3 = x, x2 = runif(n = nsamp, min = 0, max = 1),
                            x1 = runif(n = nsamp, min = 0, max = 1))
    for(i in 1:length(mod.list)){
      res = as.numeric(predict(object = mod.list[[i]], newdata = temp.frame[,c('x1', 'x2', 'x3')]))-1
      mod = substr(names(mod.list)[i], 0, 3)
      class = substr(names(mod.list)[i], 5, 9)
      svm.margin = rbind(svm.margin, 
                         data.frame(x = x, var = 'x3', prob = sum(res)/length(res), mod, class))
    }
  }
  # Use class names
  svm.margin$class[svm.margin$class == 'delta'] = 'Pareto Distance'
  svm.margin$class[svm.margin$class == 'cutof'] = 'Threshold Cutoff'
  svm.margin$class[svm.margin$class == 'radan'] = 'Utopia Distance'
  
  return(svm.margin)
}


```

```{r SVM Marginals: Calculation}
marginal.paret = SVM.marginal.calc(SVMmod.list.paret)
marginal.adapt = SVM.marginal.calc(SVMmod.list.adapt)
marginal.rando = SVM.marginal.calc(SVMmod.list.rando)

marginal.paret$cat = 'paret'
marginal.adapt$cat = 'adapt'
marginal.rando$cat = 'rando'

SVM.marginal = rbind(marginal.paret, marginal.adapt, marginal.rando)
write.csv(SVM.marginal, 'SVM_marginal.csv', row.names = F)
```

Compare only the cutoff threshold for the data.
Obtain the coefficients of determination compared to the expected marginals.

```{r SVM Marginals vs GP Marginals}
# Load data and convert into a form to concatenate
GP.marginal = read.csv('../Ex_ZDT4/Marginals_cutof_tru.csv')
GP.marginal$mod = 'GP'
GP.marginal$class = 'Threshold Cutoff'
GP.marginal$cat[GP.marginal$cat == 'start'] = 'paret'
SVM.marginal = read.csv('SVM_marginal.csv')
SVM.marginal = filter(SVM.marginal, class == 'Threshold Cutoff')
SVM.marginal$psd = sqrt(SVM.marginal$prob*(1-SVM.marginal$prob)/1500)
SVM.marginal$mod = paste('SVM-', SVM.marginal$mod, sep = '')

# Filter data to only the adaptive sampling condition
marginal.all = rbind(GP.marginal[, names(GP.marginal) %in% names(SVM.marginal)], 
                     SVM.marginal)
marginal.adapt = filter(marginal.all, cat == 'adapt' | cat == 'tru')

sz = 1
ggplot() +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat != 'tru'), mapping = aes(x = x, y = prob, color = mod)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = expression('Cutoff Thresholds'), color = 'Model') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw() +
  scale_color_brewer(type = 'qual', palette = 2)

# Calculating the coefficients of determination
corcoef = data.frame(); 
for(dataset in unique(marginal.all$cat)){
  if(dataset == 'tru'){next}
  for(model in unique(marginal.all$mod)){
    corcoef.agg = data.frame()
    for(xvar in unique(marginal.all$var)){
      true.res = filter(marginal.all, cat == 'tru', var == xvar)
      sub = filter(marginal.all, mod == model, var == xvar, cat == dataset)
      # Some of the probabilities are flat at 0 or 1; to get a coefficient of determination, apply noise
      if(diff(range(sub$prob)) < 0.01){
        adjust = runif(n = nrow(sub), min = 0.999, max = 1.001)*sub$prob + 
          runif(n = nrow(sub), min = -0.001, max = 0.001)
      } else{adjust = sub$prob}
      corcoef = rbind(corcoef,
                      data.frame(var = xvar, model, cat = dataset,
                                 rsq = cor(y = adjust,
                                           x = true.res$prob, method = 'pearson')^2))
      corcoef.agg = rbind(corcoef.agg,
                          data.frame(y = adjust, x = true.res$prob))
    }
    corcoef = rbind(corcoef,
                    data.frame(var = 'agg', model, cat = dataset,
                               rsq = cor(y = corcoef.agg$y,
                                         x = corcoef.agg$x, method = 'pearson')^2))
  }
}


ggplot(filter(corcoef, model %in% c('GP', 'SVM-pol', 'SVM-sig'))) +
  geom_col(mapping = aes(x = factor(cat, levels = c('paret', 'adapt', 'rando')), y = rsq, fill = cat)) +
  facet_grid(var~model) +
  labs(x = 'Model', y = 'Coefficient of Determination', subtitle = 'Marginalization Accuracy: Cutoff Threshold') +
  guides(fill = FALSE) +
  scale_x_discrete(breaks = c('paret', 'adapt', 'rando'),
                   labels = c('paret' = 'Start', 'adapt' = '+Adaptive', 'rando' = '+Random'))

write.csv(corcoef, 'CoefDet_cutof.csv', row.names = F)
```

Repeat the coefficient of determination for the other criteria
```{r}
# Load data and convert into a form to concatenate
GP.marginal = read.csv('../Ex_ZDT4/Marginals_delta_tru.csv')
GP.marginal$mod = 'GP'
GP.marginal$class = 'Pareto Distance'
GP.marginal$cat[GP.marginal$cat == 'start'] = 'paret'
SVM.marginal = read.csv('SVM_marginal.csv')
SVM.marginal = filter(SVM.marginal, class == 'Pareto Distance')
SVM.marginal$psd = sqrt(SVM.marginal$prob*(1-SVM.marginal$prob)/1500)
SVM.marginal$mod = paste('SVM-', SVM.marginal$mod, sep = '')

# Filter data to only the adaptive sampling condition
marginal.all = rbind(GP.marginal[, names(GP.marginal) %in% names(SVM.marginal)], 
                     SVM.marginal)
marginal.adapt = filter(marginal.all, cat == 'adapt' | cat == 'tru')

sz = 1
ggplot() +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat != 'tru'), mapping = aes(x = x, y = prob, color = mod)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = 'Pareto Distance', color = 'Model') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw() +
  scale_color_brewer(type = 'qual', palette = 2)

# Calculating the coefficients of determination
# Calculating the coefficients of determination
corcoef = data.frame(); 
for(dataset in unique(marginal.all$cat)){
  if(dataset == 'tru'){next}
  for(model in unique(marginal.all$mod)){
    corcoef.agg = data.frame()
    for(xvar in unique(marginal.all$var)){
      true.res = filter(marginal.all, cat == 'tru', var == xvar)
      sub = filter(marginal.all, mod == model, var == xvar, cat == dataset)
      # Some of the probabilities are flat at 0 or 1; to get a coefficient of determination, apply noise
      if(diff(range(sub$prob)) < 0.01){
        adjust = runif(n = nrow(sub), min = 0.999, max = 1.001)*sub$prob + 
          runif(n = nrow(sub), min = -0.001, max = 0.001)
      } else{adjust = sub$prob}
      corcoef = rbind(corcoef,
                      data.frame(var = xvar, model, cat = dataset,
                                 rsq = cor(y = adjust,
                                           x = true.res$prob, method = 'pearson')^2))
      corcoef.agg = rbind(corcoef.agg,
                          data.frame(y = adjust, x = true.res$prob))
    }
    corcoef = rbind(corcoef,
                    data.frame(var = 'agg', model, cat = dataset,
                               rsq = cor(y = corcoef.agg$y,
                                         x = corcoef.agg$x, method = 'pearson')^2))
  }
}
ggplot(filter(corcoef, model %in% c('GP', 'SVM-pol', 'SVM-sig'))) +
  geom_col(mapping = aes(x = factor(cat, levels = c('paret', 'adapt', 'rando')), y = rsq, fill = cat)) +
  facet_grid(var~model) +
  labs(x = 'Model', y = 'Coefficient of Determination', subtitle = 'Marginalization Accuracy: Pareto Distance') +
  guides(fill = FALSE) +
  scale_x_discrete(breaks = c('paret', 'adapt', 'rando'),
                   labels = c('paret' = 'Start', 'adapt' = '+Adaptive', 'rando' = '+Random'))

write.csv(corcoef, 'CoefDet_delta.csv', row.names = F)

## Utopia distance
# Load data and convert into a form to concatenate
GP.marginal = read.csv('../Ex_ZDT4/Marginals_radan_tru.csv')
GP.marginal$mod = 'GP'
GP.marginal$class = 'Utopia Distance'
GP.marginal$cat[GP.marginal$cat == 'start'] = 'paret'
SVM.marginal = read.csv('SVM_marginal.csv')
SVM.marginal = filter(SVM.marginal, class == 'Utopia Distance')
SVM.marginal$psd = sqrt(SVM.marginal$prob*(1-SVM.marginal$prob)/1500)
SVM.marginal$mod = paste('SVM-', SVM.marginal$mod, sep = '')

# Filter data to only the adaptive sampling condition
marginal.all = rbind(GP.marginal[, names(GP.marginal) %in% names(SVM.marginal)], 
                     SVM.marginal)
marginal.adapt = filter(marginal.all, cat == 'adapt' | cat == 'tru')

sz = 1
ggplot() +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob - psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat == 'tru'), mapping = aes(x = x, y = prob + psd, color = 'True Result'), 
            linetype = 2, size = sz) +
  geom_path(data = filter(marginal.adapt, cat != 'tru'), mapping = aes(x = x, y = prob, color = mod)) +
  facet_wrap(var~., nrow = 3, scales = 'free_y') + 
  labs(x = '', y = 'Probability of Acceptance', subtitle = 'Utopia Distance + Priority', color = 'Model') +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw() +
  scale_color_brewer(type = 'qual', palette = 2)

# Calculating the coefficients of determination
# Calculating the coefficients of determination
corcoef = data.frame(); 
for(dataset in unique(marginal.all$cat)){
  if(dataset == 'tru'){next}
  for(model in unique(marginal.all$mod)){
    corcoef.agg = data.frame()
    for(xvar in unique(marginal.all$var)){
      true.res = filter(marginal.all, cat == 'tru', var == xvar)
      sub = filter(marginal.all, mod == model, var == xvar, cat == dataset)
      # Some of the probabilities are flat at 0 or 1; to get a coefficient of determination, apply noise
      if(diff(range(sub$prob)) < 0.01){
        adjust = runif(n = nrow(sub), min = 0.999, max = 1.001)*sub$prob + 
          runif(n = nrow(sub), min = -0.001, max = 0.001)
      } else{adjust = sub$prob}
      corcoef = rbind(corcoef,
                      data.frame(var = xvar, model, cat = dataset,
                                 rsq = cor(y = adjust,
                                           x = true.res$prob, method = 'pearson')^2))
      corcoef.agg = rbind(corcoef.agg,
                          data.frame(y = adjust, x = true.res$prob))
    }
    corcoef = rbind(corcoef,
                    data.frame(var = 'agg', model, cat = dataset,
                               rsq = cor(y = corcoef.agg$y,
                                         x = corcoef.agg$x, method = 'pearson')^2))
  }
}
ggplot(filter(corcoef, model %in% c('GP', 'SVM-pol', 'SVM-sig'))) +
  geom_col(mapping = aes(x = factor(cat, levels = c('paret', 'adapt', 'rando')), y = rsq, fill = cat)) +
  facet_grid(var~model) +
  labs(x = 'Model', y = 'Coefficient of Determination', subtitle = 'Marginalization Accuracy: Utopia Distance + Priority') +
  guides(fill = FALSE) +
  scale_x_discrete(breaks = c('paret', 'adapt', 'rando'),
                   labels = c('paret' = 'Start', 'adapt' = '+Adaptive', 'rando' = '+Random'))

write.csv(corcoef, 'CoefDet_radan.csv', row.names = F)
```

The poor accuracy of the SVM marginals reflects the poor classification accuracy overall.

## SVM Importance Measures

Given the poor accuracy of the SVM models (misclassification error > 25%), the importance rankings are not meaningful, so they will not be calculated.

# Existing Method: Gaussian Mixtures

Since the GM models are unsupervised, there cannot be control for what the selection criteria are. A superficial analysis will be conducted to check what the ML algorithm is converging to, but the results will be interpreted solely in terms of descriptive statistics.

For the purposes of analysis, three models will be tested based on the input to the ML algorithm:
* (x1, x2, x3, f1, f2): the full set of inputs and outputs - since it has the most data, this will likely be the most robust
* (x1, x2, x3): negative control of just the inputs - it should group the points based on the sampling densities
* (f1, f2): outputs only - this should have the best distinction between good and bad performing groups

Up to 12 categories will be allowed, and any type of Gaussian models will be accepted. This will lead to longer fitting time, but should help with accuracy.

```{r GM: Generating models, results='hide'}
GPar.all = read.csv(file = '../Ex_ZDT4/GPar_all_start.csv')

# names(GPar.all)
max.cat = 12
cluster.all = Mclust(data = GPar.all[,c('x1', 'x2', 'x3', 'f1', 'f2')], G = 2:max.cat)
cluster.in = Mclust(data = GPar.all[,c('x1', 'x2', 'x3')], G = 2:max.cat)
cluster.out = Mclust(data = GPar.all[,c('f1', 'f2')], G = 2:max.cat)

```

```{r GM: Plotting}
# summary(cluster, parameters = TRUE)
GPar.all$typ.all = cluster.all$classification
GPar.all$typ.in = cluster.in$classification
GPar.all$typ.out = cluster.out$classification
sz = 2.5
ggplot(GPar.all) +
  geom_point(mapping = aes(x = x1, y = x2, color = as.factor(typ.all), 
                           alpha = 1-abs(x3 - 0.5)), size = sz) +
  guides(alpha = FALSE)

ggplot(GPar.all) +
  geom_point(mapping = aes(x = x1, y = x2, color = as.factor(typ.in), 
                           alpha = 1-abs(x3 - 0.5)), size = sz) +
  guides(alpha = FALSE)

ggplot(GPar.all) +
  geom_point(mapping = aes(x = x1, y = x2, color = as.factor(typ.out), 
                           alpha = 1-abs(x3 - 0.5)), size = sz) +
  guides(alpha = FALSE)

```

The complete set of inputs and outputs (x1, x2, x3, f1, f2) produces groups that are defined almost exclusively by f1 and x1, ignoring the Pareto frontier. The negative control of only the inputs based on sample density. The output-only test appears to separate out the Pareto front, but does not include any regions close to it. Overall, this unsupervised method does not work with such a sparse dataset.

# Conclusions:
* The unsupervised ML model (Gaussian mixture) is heavily biased by the dataset, which tended to focus on particular regions due to the adaptive sampling method that was employed. As a result, its classification was not particularly useful.
* The established supervised ML model (Support Vector Machines) are flexible to selection criteria because those are part of the user input into generating the model. It is very sensitive to the kernel function that was selected, with the radial and polynomial forms working best for this specific case. When compared to the actual function, however, the model performs poorly, producing a large number of false positives.
* The GP method requires iterative sampling to produce its most accurate results, and therefore requires more time to compute initially. However, it is the only model that produces relatively accurate classifiers and marginals.
* More complicated conditions, such as only accepting points that prioritize one objective over the other by a certain margin, are difficult for both supervised learning methods to estimate, but the results from the SVM appear to be worse compared to the actual boundaries.

