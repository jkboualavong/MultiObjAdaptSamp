---
title: "Proof of concept: ZDT4.mod"
author: "Jonathan Boualavong"
output: html_notebook
---

<!-- output:    -->
<!--   md_document: -->
<!--     variant: markdown_github -->

# Description
This notebook uses a newly developed Gaussian Process-based method to solve for the near-optimal set of solutions for CO2 capture using proton coupled electron transfers (PCET) to drive pH swings. 
The work assumes that the redox molecule is a quinone, and restricts the search to only combinations of properties that a quinone is likely to have.
After determining the restricted search space, the performance of CO2 capture, defined by minimum energy demand and the CO2 capture flux, will be determined using established thermodynamic and kinetics equations.
The resulting Pareto frontier of this bi-objective problem is solved using the GPareto package, then characterized to define acceptable sub-optimal performance in the likely event that a compound with the exact specifications of any of the Pareto frontier estimates not exist.
The near-optimal set will be solved using the established method (see other mathematical examples for details and description of the algorithm) to describe what criteria the optimal quinone and solution composition should have.

# Code
## Initialization

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear the workspace and define the functions.

```{r Load Packages}
# Setup
rm(list = ls())
# Visualization
library(dplyr)
library(ggplot2)
library(patchwork)
# Parallel processing
library(parallel)
library(doParallel)
# Gaussian processes
library(GPareto)
library(DiceKriging)
library(DiceOptim)
# Optimization
library(GA)
```

## Parameter space
Data for restricting the quinone features is based on results from Huynh et al. J Am Chem Soc. 2016 December 14; 138(49): 15903â€“15910. doi:10.1021/jacs.6b05797.

Remove points that are not likely to be stable within the electrochemcial window of water. For the process:
Q + 2e- + 2H+ <-> QH2
We can roughly assume that the reduction reaction is competing with hydrogen evolution
2H+ + 2e- <-> H2
and the oxidation reaction is competing with oxygen evolution
2H2O + 4OH- <-> O2 + 4e-

Both of these reactions are pH dependent, so the cutoff values are not obvious. 
At the extreme, the reduction potential of hydrogen evolution will be at its highest (most likely to compete with the quinone reduction) at the lowest pH, which would typically be at the pH where carbonic acid makes up > 90% of the CO2 speciation (pH = 5.33).
Similarly, the reduction potential of oxygen evolution will be at its lowest at the highest pH, which would be just above the hydroquinone's buffer region (the higher of its pKas + 1).

Practically, the quinone electrochemistry has much faster kinetics than either gas evolution reaction in the absence of a specific water-splitting catalyst, so a tolerance of about 200 mV will be added to accommodate.

```{r}
# Load data and correlation plot package
quinone.data = read.csv(file = 'HuynhData.csv', skip = 1)

# Filter the data to only those whose reaction would be stable in water
## Hydrogen evolution
h2.evo = -59.2*1e-3 * 5.33
quinone.data = filter(quinone.data, E0.2 > h2.evo)

## Cutoff value for oxygen evolution
o2.evo = 1.229 + -59.2*1e-3*quinone.data$Pka.2+1
quinone.data = filter(quinone.data, E0.1 < o2.evo)

quinone.data

# Correlation plot between the E0 and pka values
cor.mat = matrix(data = rep(0, 16), nrow = 4)
for(i in seq(from = 6, to = 9, by = 1)){
  for(j in seq(from = i, to = 9, by = 1)){
    cor.mat[i-5,j-5] = cor(x = quinone.data[,i], y = quinone.data[,j])
  }
}
cor.mat

g1 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = E0.1, x = E0.2)) +
  labs(subtitle = paste('r = ', round(cor.mat[1,2], 3))) 
g2 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = E0.1, x = Pka.1)) +
  labs(subtitle = paste('r = ', round(cor.mat[1,3], 3)))
g3 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = E0.1, x = Pka.2)) +
  labs(subtitle = paste('r = ', round(cor.mat[1,4], 3)))
g4 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = E0.2, x = Pka.1)) +
  labs(subtitle = paste('r = ', round(cor.mat[2,3], 3)))
g5 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = E0.2, x = Pka.2)) +
  labs(subtitle = paste('r = ', round(cor.mat[2,4], 3)))
g6 = ggplot(quinone.data) +
  geom_point(mapping = aes(y = Pka.1, x = Pka.2)) +
  labs(subtitle = paste('r = ', round(cor.mat[3,4], 3)))

(g1 + g2 + g3) / (plot_spacer() + g4 + g5) / (plot_spacer() + plot_spacer() + g6)
rm(g1, g2, g3, g4, g5, g6)
rm(h2.evo, i, j, o2.evo, cor.mat)
```

While all four variables are strongly co-correlated, this is largely irrelevant because the thermodynamic and kinetic analyses do not rely on the reduction potential besides assuming that gas evolution does not occur.
There is a Bronsted-like relationship between the reduction potential and the reaction rate of the deprotonated hydroquinone nucleophilically attacking the CO2 [Simpson & Durand 1990, doi: 10.1016/0013-4686(90)85012-C], but this is not likely to be the dominant mechanism because the reaction with OH- is approximately 10 to 100 times faster in water.
To determine which pKa will be the free variable and which will be represented by an error term of the linear regression, the relative normalized standard deviations of the errors were compared. 

```{r}
print('pka2 as free variable, pKa1 as the solved variable')
mod1 = lm(Pka.1 ~ Pka.2, data = quinone.data)
sd(mod1$residuals[abs(mod1$residuals) < 10]) / diff(range(quinone.data$Pka.1))
  
print('pka1 as free variable, pKa2 as the solved variable')
mod2 = lm(Pka.2 ~ Pka.1, data = quinone.data)
sd(mod2$residuals[abs(mod2$residuals) < 10]) / diff(range(quinone.data$Pka.2))

x = c(unname(mod1$residuals), unname(mod2$residuals))
var = c(rep('pka1', length(mod1$residuals)), rep('pka2', length(mod1$residuals)))
ggplot(data.frame(x, var)) +
  geom_density(mapping = aes(x = x, color = var)) +
  scale_color_manual(labels = c('pka1' = 'pka1 = f(pka2)', 'pka2' = 'pka2 = f(pka1)'),
                     values = c('pka1' = 'red', 'pka2' = 'blue'), name = '') +
  labs(x = 'Residual', y = 'Probability Density')

# Determine the value for the standard deviation that should be used to encompass 98% of the search space
q1 = quantile(mod2$residuals, probs = c(0.01))
q2 = quantile(mod2$residuals, probs = c(0.99))
# Plot the data and the search space
fit.ln = data.frame(Pka.1 = seq(from = -8.33, to = 13.41, length.out = 3))
fit.ln$Pka.2 = predict(object = mod2, newdata = fit.ln)
ggplot() +
  geom_point(quinone.data, mapping = aes(x = Pka.1, y = Pka.2), color = 'blue', alpha = 0.5) +
  geom_line(fit.ln, mapping = aes(x = Pka.1, y = Pka.2), color = 'black', linetype = 1) +
  geom_line(fit.ln, mapping = aes(x = Pka.1, y = Pka.2+q2), color = 'black', linetype = 2) +
  geom_line(fit.ln, mapping = aes(x = Pka.1, y = Pka.2+q1), color = 'black', linetype = 2) +
  geom_line(fit.ln, mapping = aes(x = Pka.1, y = Pka.1), color = 'red', linetype = 2) +
  geom_line(fit.ln, mapping = aes(x = Pka.1, y = Pka.1+q2), color = 'red', linetype = 2) +
  labs(title = expression('Known Quinone p'*italic(K)*''[a]*'s'),
       y = expression('p'*italic(K)*''['a,2']),
       x = expression('p'*italic(K)*''['a,1']))
q1
q2
rm(fit.ln, x, var)

```

The variance alone show very little difference between the choice, which pKa1 being the free variable having a slightly better response.
This is confirmed by the distribution of the residuals (omitting the single outlier), which shows that having pKa1 as the free variable gives a distribution closer to a normal distribution.
Therefore, the range of pKas is:

pKa1: -8.33 to 13.41
pKa2 = f(pKa1) + error

While the error for pKa2 is nearly Gaussian in shape, the mathematical description works better if it is sampled from a uniform distribution. From the data, 98% of the data is captured within an error of -3.9 to +5.5.

This range of pKas has 2 problems:
* An issue seems to arise when the second pKa is lower than the first, however, despite some actual measurements violating that criteria (red dotted line). 
All points that do violate that restriction are within measurement error with one exception, so that could be the cause.
* The pH of the process is not likely to extend below 3 because the primary acid in question is carbonic acid. This means that pKa values below 2 are effectively the same and do not need to be sampled further. Therefore, the lower bound for pKa1 can be increased to reduce the search space.

The refined search space is:

pKa1: 0 to 13.41
pKa2 = pKa1 + error
error ~ unif(0, 5.5)

## Objective Functions

The objective function for energy demand solves the set of equilibrium equations across the range of charge and gas transfers based on four state variables: 
total dissolved inorganic carbon (DIC), state of charge (xA), partial pressure of CO2 (pCO2), and solution pH.

The minimum energy demand assumes all charge is passed at the Nernst potential, which is updated as charge passes through the system.
The energy demand assumes anti-symmetric operation, ie. quinone reduction at the cathode and hydroquinone oxidation at the anode.
Charge balance of the anolyte and catholyte are assumed to be the result of the background electrolyte travelling across an ion selective membrane.
The gas transfers are assumed to be separate stages from the electrochemical stages, purely for the sake of simplicity. 
This estimate for the minimum energy is a slight overestimate because of over-pressurization of the CO2; coupling the gas transfer steps with the appropriate electrochemical steps decreases the minimum energy by approximately a factor of 2.
The function can accommodate feed CO2 gases of any partial pressure, but this study is interested in 3 applications: coal flue gas (15v% feed to 1.5v% lean), air revitalization (2000 ppm to 1000 ppm), and direct air capture (400 ppm to 250 ppm).

This code is broken down into explicit functions and derived functions.
Explicit functions are a series of sub-functions which solve for one of the four state variables using knowledge of the other three state variables and the solution conditions. 
These functions solve the set of chemical equilibrium, mass, and charge balance equations for the bulk solution.
Derived functions use the information from the explicit functions to determine relevant information for determining the energy demand and CO2 flux.

For the purpose of generalization, these equations are written with the variables 'beta1' and 'beta2' which describe the deprotonated hydroquinone's affinity for CO2, forming an organic carbonate. 
This species is ignored in this particular notebook for the above-stated reason (slow kinetics), and therefore both variables are set to 0.
These variables are included because other compounds have been proposed to capture CO2 primarily through that mechanism, and thus they could also be studied with this script.

```{r PCET Explicit functions}
# Direct explicit functions
# Functions are named with the output variable first, then all relevant inputs
DIC.xA.pCO2.pH.A.k.beta = function(xA, pCO2, pH, A.tot, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Proton concentration
  H = 10^-pH
  
  # Inorganic carbonate
  CO3.free = kH * pCO2 * (H^2 + kc1 * H + kc1 * kc2) / H^2
  
  # Bound carbon
  CO2.bound = A.tot*xA *k1*k2*(beta1*pCO2 + 2*beta2*pCO2^2)/((1 + beta1*pCO2 + beta2*pCO2^2)*k1*k2 + k1*H + H^2)
  
  return(CO3.free + CO2.bound)
}

pH.xA.pCO2.A.k.beta.Na = function(xA, P, At, k1, k2, beta1, beta2, Na){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Polynomial root
  x5 = 1
  x4 = k1 + Na + 2*At*xA 
  x3 = k1*k2 - kw + k1*Na + beta1*k1*k2*P - kc1*kH*P +
      beta2*k1*k2*P^2 + 2*At*k1*xA - 2*At*k2*xA
  x2 = (-k1)*kw + k1*k2*Na - k1*kc1*kH*P - 2*kc1*kc2*kH*P + beta1*k1*k2*Na*P + beta2*k1*k2*Na*P^2 -
      2*At*k1*k2*xA + 2*At*beta1*k1*k2*P*xA + 2*At*beta2*k1*k2*P^2*xA
  x1 = (-k1)*k2*kw - k1*k2*kc1*kH*P - 2*k1*kc1*kc2*kH*P - beta1*k1*k2*kw*P -
      beta1*k1*k2*kc1*kH*P^2 - beta2*k1*k2*kw*P^2 - beta2*k1*k2*kc1*kH*P^3
  x0 = - 2*k1*k2*kc1*kc2*kH*P - 2*beta1*k1*k2*kc1*kc2*kH*P^2 -
     2*beta2*k1*k2*kc1*kc2*kH*P^3
  roots = polyroot(c(x0, x1, x2, x3, x4, x5))
  
  # Only the real and positive roots
  H = roots[abs(Im(roots)) < 1e-8]
  H = Re(H[Re(H) > 0])
  
  # It is possible for multiple roots to satisfy the solution. Typical pH is going to be the one closest to 7-8
  # H = H[which.min(abs(-log10(H) - 7))]

  return(-log10(H[1]))
}

pCO2.xA.pH.A.k.beta.Na = function(xA, pH, At, k1, k2, beta1, beta2, Na, pCO2.prev){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  # Proton concentration
  H = 10^-pH
  
  # Polynomial root
  x3 = (-beta2)*H*k1*k2*kc1*kH - 2*beta2*k1*k2*kc1*kc2*kH
  x2 = beta2*H^3*k1*k2 - beta1*H*k1*k2*kc1*kH - 2*beta1*k1*k2*kc1*kc2*kH -
      beta2*H*k1*k2*kw + beta2*H^2*k1*k2*Na + 2*At*beta2*H^2*k1*k2*xA
  x1 = beta1*H^3*k1*k2 - H^3*kc1*kH - H^2*k1*kc1*kH - H*k1*k2*kc1*kH - 2*H^2*kc1*kc2*kH - 2*H*k1*kc1*kc2*kH -
      2*k1*k2*kc1*kc2*kH - beta1*H*k1*k2*kw + beta1*H^2*k1*k2*Na +
      2*At*beta1*H^2*k1*k2*xA
  x0 = H^5 + H^4*k1 + H^3*k1*k2 - H^3*kw - H^2*k1*kw - H*k1*k2*kw +
      H^4*Na + H^3*k1*Na + H^2*k1*k2*Na + 2*At*H^4*xA + 2*At*H^3*k1*xA -
      2*At*H^3*k2*xA - 2*At*H^2*k1*k2*xA
  roots = polyroot(c(x0, x1, x2, x3))
  
  # Only the real and positive roots
  pCO2 = roots[abs(Im(roots)) < 1e-8]
  pCO2 = Re(pCO2[Re(pCO2) > 0])
  # There are cases of multiepl roots. Find the one that is closest to the previous known value
  pCO2 = pCO2[which.min(abs(log10(pCO2) - log10(pCO2.prev)))]
  
  return(pCO2)
}

pH.DIC.xA.pCO2.A.k.beta = function(DIC, xA, P, At, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  # Polynomial root
  x4 = (DIC - kH*P)
  x3 = (DIC*k1 - k1*kH*P - kc1*kH*P)
  x2 = (DIC*k1*k2 + beta1*DIC*k1*k2*P - k1*k2*kH*P - k1*kc1*kH*P - kc1*kc2*kH*P + beta2*DIC*k1*k2*P^2 - 
    beta1*k1*k2*kH*P^2 - beta2*k1*k2*kH*P^3 - At*beta1*k1*k2*P*xA - 2*At*beta2*k1*k2*P^2*xA)
  x1 = ((-k1)*k2*kc1*kH*P - k1*kc1*kc2*kH*P - beta1*k1*k2*kc1*kH*P^2 - 
        beta2*k1*k2*kc1*kH*P^3)
  x0 = (-k1)*k2*kc1*kc2*kH*P - beta1*k1*k2*kc1*kc2*kH*P^2 - beta2*k1*k2*kc1*kc2*kH*P^3
  roots = polyroot(c(x0, x1, x2, x3, x4))
  
  # Only the real and positive roots
  H = roots[abs(Im(roots)) < 1e-8]
  H = Re(H[Re(H) > 0])
  return(-log10(H))
}

pCO2.DIC.xA.pH.A.k.beta = function(DIC, xA, pH, At, k1, k2, beta1, beta2){
  # Constants: carbonate and water chemistry 
  kH = 3.4e-2; # M/atm
  kc1 = 10^-6.3
  kc2 = 10^-10.3
  kw = 1e-14
  
  H = 10^-pH
  
  # Polynomial root
  x3 = ((-beta2)*H^2*k1*k2*kH - beta2*H*k1*k2*kc1*kH - beta2*k1*k2*kc1*kc2*kH)
  x2 = (beta2*DIC*H^2*k1*k2 - beta1*H^2*k1*k2*kH - beta1*H*k1*k2*kc1*kH - 
        beta1*k1*k2*kc1*kc2*kH - 2*At*beta2*H^2*k1*k2*xA)
  x1 = (beta1*DIC*H^2*k1*k2 - H^4*kH - H^3*k1*kH - H^2*k1*k2*kH - H^3*kc1*kH - 
        H^2*k1*kc1*kH - H*k1*k2*kc1*kH - H^2*kc1*kc2*kH - H*k1*kc1*kc2*kH - 
        k1*k2*kc1*kc2*kH - At*beta1*H^2*k1*k2*xA)
  x0 = DIC*H^4 + DIC*H^3*k1 + DIC*H^2*k1*k2
  roots = polyroot(c(x0, x1, x2, x3))
  
  # Only the real and positive roots
  pCO2 = roots[abs(Im(roots)) < 1e-8]
  pCO2 = Re(pCO2[Re(pCO2) > 0])
  return(pCO2)
}

# There are cases in the process where both pH and pCO2 are unknown. 
# For those cases, both variables can be solved togther, but it leads to coupled nonlinear root finding problems. 
# Initial testing of the equations has found that using an initial guess of pH (such as the pH at the immediately previous state of charge) leads to a good enough estimate of the pH to solve pCO2.
pH.it.guess.DIC.At.k.beta = function(pH.guess, xA.next, DIC, A.tot, k1, k2, beta1, beta2, Na){
  # Iterates to solve the pH and pCO2 at the next electrochemical time step, 
  # given xA and DIC and an initial guess (the pH at the previous time step)
  pCO2.it = c(); pH.it = c(pH.guess)
  pCO2.it = pCO2.DIC.xA.pH.A.k.beta(DIC = DIC, xA = xA.next, pH = pH.it, At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  for(n in 2:74){
    pH.it[n] = pH.xA.pCO2.A.k.beta.Na(xA = xA.next, P = pCO2.it[n - 1], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    pCO2.it[n] = pCO2.DIC.xA.pH.A.k.beta(DIC = DIC, xA = xA.next, pH = pH.it[n], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  } 
  n = 7:5
  pH.it[n] = pH.xA.pCO2.A.k.beta.Na(xA = xA.next, P = pCO2.it[n - 1], At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  # Due to some oscillatory instabilities under specific conditions, take the last 25 and use the value that is closest to the guess
  pH.res = pH.it[50:75]
  pH.res = pH.res[which.min(abs(pH.res) - pH.guess)]
  return(pH.res)
}
```

```{r PCET Derived Functions: Process Conditions}
# Derived functions
# DIC difference: CO2/L*cycle - this is a good first check for the condition to ensure that CO2 is, in fact, captured, represented by a positive value.
DIC.diff = function(Na, A, beta1, beta2, k1, k2, pCO2.in, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  # pCO2.in = 0.1; pCO2.out = 1
  
  # Absorption: low P, high xA
  start.soln = data.frame(p.CO2 = pCO2.in, xA = max(xA.lim))
  start.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = start.soln$xA, P = start.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  start.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = start.soln$xA, pCO2 = start.soln$p.CO2, pH = start.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Desorption: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Calculate the difference
  DIC.diff = start.soln$DIC - stop.soln$DIC
  return(DIC.diff)
}

# Minimum partial pressure of the lean gas
pCO2.lean = function(Na, A, beta1, beta2, k1, k2, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  
  # Calculate the DIC of the outlet after complete desorption: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                       A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  
  # Calculate the pCO2 when fully reduced, holding DIC constant. Due to the need for a previous case, run in ~5 steps
  out.soln = data.frame(DIC = stop.soln$DIC, xA = seq(from = min(xA.lim), to = max(xA.lim), length.out = 5))
  # Loop the pH and pCO2 simultaneously
  loop.pH = pH.it.guess.DIC.At.k.beta(pH.guess = stop.soln$pH[1], xA.next = out.soln$xA[1], DIC = out.soln$DIC[1],
                                      A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  loop.pCO2 = pCO2.xA.pH.A.k.beta.Na(xA = out.soln$xA[1], pH = loop.pH[1],
                                     At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, 
                                     pCO2.prev = stop.soln$p.CO2)
  for(i in 2:5){
    loop.pH[i] = pH.it.guess.DIC.At.k.beta(pH.guess = loop.pH[i-1], xA.next = out.soln$xA[i], DIC = out.soln$DIC[i],
                                        A.tot = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    loop.pCO2[i] = pCO2.xA.pH.A.k.beta.Na(xA = out.soln$xA[i], pH = loop.pH[i],
                                       At = A, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, 
                                       pCO2.prev = loop.pCO2[i-1])
    
  }
  return(loop.pCO2[i])
}

```


Penalty function: logistic weighting function such that for values where 90% of the CO2 is captured, the weight is 1, but when less CO2 is captured, the weight increases based on the relative change on the minimum work of separation.
The hyperparameters L, k, and x0 of the logistic component of the weighting function were tuned by fitting to a variety of minimum lean gas pressures on the range of 0.016 to 2.5 atm and then selected as the modal value.
The logistic function was tuned such that the growth rate steepness captured the range of 0 to 90% capture; when no CO2 could be captured, the weight is applied fully.

A penalty function is not necessary for the flux because insufficient capture would manifest as negative fluxes. It may still show up in the Pareto frontier, but it can easily be filtered out, unlike the cases with the energy demand.

Further tuning of the exact value of L is based on the minimumm energy demand for the specific lean gas minimum.

```{r}
GPar.all = read.csv(file = 'GPar_WeightTuningSet.csv')

# Original weighting function: adjust by the minimum energy demand, simplified
weight.fun = function(pCO2.lean){
  # Maximum weight
  weight.max = log(1/10) / log(pCO2.lean/0.15)
  # Logistic function
  L = weight.max - 0.98
  k = 267; x0 = 0.071
  # data.frame(weight.max)
  return(L/(1 + exp(-k * (pCO2.lean - x0))) + 1)
}

# Calculate weight
GPar.all$weight = weight.fun(GPar.all$pCO2.lean)

ggplot(filter(GPar.all, Energy.kJ.mol*weight < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = weight)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Simplified Weight')
ggplot(filter(GPar.all, Energy.kJ.mol*weight < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Unweighted Energy')
ggplot(filter(GPar.all, Energy.kJ.mol*weight < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol*weight)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Simple Weighted Energy')


```

Simple weighted energy is not enough of a penalty; the minimum energy for systems that do not capture >90% of the CO2 are still showing lower energy demands. 

Using the complete expression for minimum energy demand instead of the simplified form, which makes a number of assumptions about negliglbe terms.

Minimum energy demand:
$$ RT \sum [n_{CO_2} \ ln(y_{CO_2}) +  n_{-CO_2} \ ln(y_{-CO_2})]_{outlet} - [n_{CO_2} \ ln(y_{CO_2}) +  n_{-CO_2} \ ln(y_{-CO_2})]_{inlet}$$

Since the concern is only the ratio for the weighting factor, then can assume the $RT$ term cancels.

The inlet term is constant at 0.15 atm and the reference point is 90% reduction in the partial pressure in the lean gas.
The calculation is based on mass balance, assuming that the volume adjusted to maintain a total pressure of 1 atm.

Assuming 1 total mole of gas is processed at the reference point:
* Inlet: 0.15 mol CO2, 0.85 mol not CO2
* Lean gas: 0.85 mol not CO2, (0.015*0.85)/(1 - 0.015) mol CO2
* Enriched gas: 1 - (0.015*0.85)/(1 - 0.015) mol CO2, 0.1% not CO2

The lean gas and enriched gas will change according to the system with a similar mass balance.

```{r}
weight.fun2 = function(pCO2.lean){
  # Ideal
  n.inlet = c(0.15, 0.85)
  n.lean = c(0.85, 0.015*0.85/(1 - 0.015))
  n.enrich = c(0.001*(1 - 0.015*0.85/(1 - 0.015)), 1 - 0.015*0.85/(1 - 0.015))
  
  E.ideal = sum(n.enrich*log(n.enrich/sum(n.enrich)) + 
                  n.lean*log(n.lean/sum(n.lean)) - 
                  n.inlet*log(n.inlet/sum(n.inlet)))

  # Actual: separate into 3 cases:
  E.tru = rep(x = 0, times = length(pCO2.lean))
  # Third case: if the lean gas pressure is above 1, i.e. it pressurized
  pos3 = (pCO2.lean >= 0.99)
  # Set the lean gas pressure to 0.999, 
  # then multiply the weight by the actual pressure to correct the energy; 
  # since the weight is divided by this energy, this means dividing by the pressure
  set.lean = 0.999
  n.enrich.co2 = (set.lean - 0.15)/(1 - set.lean)
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos3] = (- sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))) / pCO2.lean[pos3]

  # Second case: if the lean gas pressure is between 0.15 and 1, 
  # i.e. the CO2 was moved from the pure gas to the lean gas
  pos2 = (pCO2.lean >= 0.15 & pCO2.lean < 0.99)
  # For the mass balance to work, gas must have moved from the enriched stream to the lean gas
  n.enrich.co2 = (pCO2.lean[pos2] - 0.15)/(1 - pCO2.lean[pos2])
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos2] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # First case: if the lean gas pressure is less than 0.15, i.e. some amount of capture happened
  pos1 = (pCO2.lean < 0.15)
  # Mathematically identical to the lean gas case, just adjusting the lean gas and enriched gas mass balance
  n.lean = pCO2.lean[pos1]*0.85/(1 - pCO2.lean[pos1])
  n.enrich.co2 = 0.15 - n.lean
  n.enrich.gas = 0.001*n.enrich.co2
  
  E.tru[pos1] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # Maximum weight
  weight.max = E.ideal/E.tru

  # Logistic function
  L = weight.max - 0.98
  k = 267; x0 = 0.071
  # data.frame(weight.max)
  return(L/(1 + exp(-k * (pCO2.lean - x0))) + 1)
}

test.pco2 = 10^seq(from = -10, to = 2, by = 0.1)
weight.test = weight.fun2(test.pco2)
class = rep(0, length(test.pco2)); class[test.pco2 >= 0.15] = 1; class[test.pco2 >= 1] = 2
ggplot(filter(data.frame(x = test.pco2, y = weight.test, class) )) +
  geom_point(mapping = aes(x = x, y = y, color = (x < 0.015))) +
  scale_x_log10() + facet_wrap(~class, scales = 'free')
rm(test.pco2, weight.test, class)

# Calculate weight
GPar.all$weight2 = weight.fun2(GPar.all$pCO2.lean)

ggplot(filter(GPar.all, Energy.kJ.mol*weight2 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = weight2)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Complete Weight')
ggplot(filter(GPar.all, Energy.kJ.mol*weight2 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Unweighted Energy')
ggplot(filter(GPar.all, Energy.kJ.mol*weight2 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol*weight2)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Complete Weighted Energy')

```

While better, using the complete energy is not fixed upon this correction in that the minimum energy is still at a value where the flux would be negative. Amplifying an additional multiplicative factor to amplify the weight:

```{r}
weight.fun3 = function(pCO2.lean){
  # Ideal
  n.inlet = c(0.15, 0.85)
  n.lean = c(0.85, 0.015*0.85/(1 - 0.015))
  n.enrich = c(0.001*(1 - 0.015*0.85/(1 - 0.015)), 1 - 0.015*0.85/(1 - 0.015))
  
  E.ideal = sum(n.enrich*log(n.enrich/sum(n.enrich)) + 
                  n.lean*log(n.lean/sum(n.lean)) - 
                  n.inlet*log(n.inlet/sum(n.inlet)))

  # Actual: separate into 3 cases:
  E.tru = rep(x = 0, times = length(pCO2.lean))
  # Third case: if the lean gas pressure is above 1, i.e. it pressurized
  pos3 = (pCO2.lean >= 0.99)
  # Set the lean gas pressure to 0.999, 
  # then multiply the weight by the actual pressure to correct the energy; 
  # since the weight is divided by this energy, this means dividing by the pressure
  set.lean = 0.999
  n.enrich.co2 = (set.lean - 0.15)/(1 - set.lean)
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos3] = (- sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))) / pCO2.lean[pos3]

  # Second case: if the lean gas pressure is between 0.15 and 1, 
  # i.e. the CO2 was moved from the pure gas to the lean gas
  pos2 = (pCO2.lean >= 0.15 & pCO2.lean < 0.99)
  # For the mass balance to work, gas must have moved from the enriched stream to the lean gas
  n.enrich.co2 = (pCO2.lean[pos2] - 0.15)/(1 - pCO2.lean[pos2])
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos2] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # First case: if the lean gas pressure is less than 0.15, i.e. some amount of capture happened
  pos1 = (pCO2.lean < 0.15)
  # Mathematically identical to the lean gas case, just adjusting the lean gas and enriched gas mass balance
  n.lean = pCO2.lean[pos1]*0.85/(1 - pCO2.lean[pos1])
  n.enrich.co2 = 0.15 - n.lean
  n.enrich.gas = 0.001*n.enrich.co2
  
  E.tru[pos1] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # Maximum weight
  weight.max = E.ideal/E.tru

  # Logistic function
  L = weight.max - 0.98
  k = 267; x0 = 0.071
  # data.frame(weight.max)
  return(25*L/(1 + exp(-k * (pCO2.lean - x0))) + 1)
}

test.pco2 = 10^seq(from = -10, to = 2, by = 0.1)
weight.test = weight.fun2(test.pco2)
class = rep(0, length(test.pco2)); class[test.pco2 >= 0.15] = 1; class[test.pco2 >= 1] = 2
ggplot(filter(data.frame(x = test.pco2, y = weight.test, class) )) +
  geom_point(mapping = aes(x = x, y = y, color = (x < 0.015))) +
  scale_x_log10() + facet_wrap(~class, scales = 'free')
rm(test.pco2, weight.test, class)

# Calculate weight
GPar.all$weight3 = weight.fun3(GPar.all$pCO2.lean)

ggplot(filter(GPar.all, Energy.kJ.mol*weight3 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = weight3)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Amplified Weight')
ggplot(filter(GPar.all, Energy.kJ.mol*weight3 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Unweighted Energy')
ggplot(filter(GPar.all, Energy.kJ.mol*weight3 < 100)) +
  geom_point(mapping = aes(x = pCO2.lean, y = Energy.kJ.mol*weight3)) +
  facet_grid(~(pCO2.lean >= 0.015), scales = 'free_x') +
  scale_x_log10() + labs(y = 'Amplified Weighted Energy')

```

Multiplying the logarithmic term by a factor of 25 appears to be sufficiently large to adjust the energies without overshooting the weight.

```{r}
rm(weight.fun, weight.fun2, weight.fun3, GPar.all)
```

Applying the penalty function directly to energy demand

```{r PCET Derived Functions: Energy Demand}
weight.fun = function(pCO2.lean){
  # Ideal
  n.inlet = c(0.15, 0.85)
  n.lean = c(0.85, 0.015*0.85/(1 - 0.015))
  n.enrich = c(0.001*(1 - 0.015*0.85/(1 - 0.015)), 1 - 0.015*0.85/(1 - 0.015))
  
  E.ideal = sum(n.enrich*log(n.enrich/sum(n.enrich)) + 
                  n.lean*log(n.lean/sum(n.lean)) - 
                  n.inlet*log(n.inlet/sum(n.inlet)))

  # Actual: separate into 3 cases:
  E.tru = rep(x = 0, times = length(pCO2.lean))
  # Third case: if the lean gas pressure is above 1, i.e. it pressurized
  pos3 = (pCO2.lean >= 0.99)
  # Set the lean gas pressure to 0.999, 
  # then multiply the weight by the actual pressure to correct the energy; 
  # since the weight is divided by this energy, this means dividing by the pressure
  set.lean = 0.999
  n.enrich.co2 = (set.lean - 0.15)/(1 - set.lean)
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos3] = (- sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))) / pCO2.lean[pos3]

  # Second case: if the lean gas pressure is between 0.15 and 1, 
  # i.e. the CO2 was moved from the pure gas to the lean gas
  pos2 = (pCO2.lean >= 0.15 & pCO2.lean < 0.99)
  # For the mass balance to work, gas must have moved from the enriched stream to the lean gas
  n.enrich.co2 = (pCO2.lean[pos2] - 0.15)/(1 - pCO2.lean[pos2])
  n.lean = n.enrich.co2 + 0.15
  n.enrich.gas = 0.001*n.enrich.co2
  E.tru[pos2] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # First case: if the lean gas pressure is less than 0.15, i.e. some amount of capture happened
  pos1 = (pCO2.lean < 0.15)
  # Mathematically identical to the lean gas case, just adjusting the lean gas and enriched gas mass balance
  n.lean = pCO2.lean[pos1]*0.85/(1 - pCO2.lean[pos1])
  n.enrich.co2 = 0.15 - n.lean
  n.enrich.gas = 0.001*n.enrich.co2
  
  E.tru[pos1] = - sum(n.inlet*log(n.inlet/sum(n.inlet))) +
    0.85*log(0.85/(n.lean + 0.85)) +
    n.enrich.gas*log(n.enrich.gas/(n.enrich.gas + n.enrich.co2)) + 
    n.enrich.co2*log(n.enrich.co2/(n.enrich.gas + n.enrich.co2)) + 
    n.lean*log(n.lean/(n.lean + 0.85))
  
  # Maximum weight
  weight.max = E.ideal/E.tru

  # Logistic function
  L = weight.max - 0.98
  k = 267; x0 = 0.071
  # data.frame(weight.max)
  return(25*L/(1 + exp(-k * (pCO2.lean - x0))) + 1)
}

# Total energy demand - 4-stage process for simplicity
Energy.tot = function(k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out){
  # Constants
  z = 2; R = 8.314; T = 298; F = 96485; resolution = 151;
  # pCO2.in = 0.1; pCO2.out = 1
  xA.lim = c(0.025, 0.975)
  
  # 1 -> 2: Electrochemical oxidation (xA decrease to endpoint), constant DIC
  # Starting solution for initial guess: low P, high xA
  start.soln = data.frame(p.CO2 = pCO2.in, xA = max(xA.lim))
  start.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = start.soln$xA, P = start.soln$p.CO2, 
                                       At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  start.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = start.soln$xA, pCO2 = start.soln$p.CO2, pH = start.soln$pH, 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  # Anode progress
  E.anode = data.frame(DIC = start.soln$DIC, xA = seq(from = start.soln$xA[1], to = min(xA.lim), length.out = resolution))
  # Loop to solve the ieration function
  loop = pH.it.guess.DIC.At.k.beta(pH.guess = start.soln$pH[1], xA.next = E.anode$xA[1], DIC = E.anode$DIC[1], 
                                       A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  for(i in 2:length(E.anode$DIC)){
    loop = c(loop, pH.it.guess.DIC.At.k.beta(pH.guess = loop[i-1], xA.next = E.anode$xA[i], DIC = E.anode$DIC[i], 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na))
  }
  # Some iterations don't converge completely, leading to single points that deviated from the rest of the curve. This is characterized by a single point that is a local maxima or minimum. Endpoints are asusmed to be good
  loop.check.left = loop[1:(resolution-2)] - loop[2:(resolution-1)]
  loop.check.right = loop[2:(resolution-1)] - loop[3:(resolution)]
  # If the signs are different, then it is a local shift
  loop.pos = c(TRUE, (sign(loop.check.left) == sign(loop.check.right)), TRUE)
  for(pos in which(loop.pos == FALSE)){ # Take the average
    loop[pos] = (loop[pos-1] + loop[pos+1])/2
  }
  E.anode$pH = loop; 
  # Loop pCO2 calculation as well, since the pCO2 function relies on the previous point
  loop = pCO2.xA.pH.A.k.beta.Na(xA = E.anode$xA[1], pH = E.anode$pH[1],
                                At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = start.soln$p.CO2)
  for(i in 2:length(E.anode$DIC)){
    loop[i] = pCO2.xA.pH.A.k.beta.Na(xA = E.anode$xA[i], pH = E.anode$pH[i],
                                  At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = loop[i-1])
  }
  E.anode$p.CO2 = loop
  E.anode$q = abs(E.anode$xA - E.anode$xA[1])*A.tot*z*F # Coulombs
  
  # 3 -> 4: Electrochemical reduction (xA increase to endpoint), constant DIC
  # Starting solution for initial guess: high P, low xA
  stop.soln = data.frame(p.CO2 = pCO2.out, xA = min(xA.lim))
  stop.soln$pH = pH.xA.pCO2.A.k.beta.Na(xA = stop.soln$xA, P = stop.soln$p.CO2, 
                                       At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  stop.soln$DIC = DIC.xA.pCO2.pH.A.k.beta(xA = stop.soln$xA, pCO2 = stop.soln$p.CO2, pH = stop.soln$pH, 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2)
  # Cathode progress
  E.cathode = data.frame(DIC = stop.soln$DIC[1], xA = seq(from = stop.soln$xA[1], to = max(xA.lim), length.out = resolution))
  # Loop to solve the ieration function
  loop = pH.it.guess.DIC.At.k.beta(pH.guess = stop.soln$pH[1], xA.next = E.cathode$xA[1], DIC = E.cathode$DIC[1], 
                                       A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
  for(i in 2:length(E.cathode$DIC)){
    loop = c(loop, pH.it.guess.DIC.At.k.beta(pH.guess = loop[i-1], xA.next = E.cathode$xA[i], DIC = E.cathode$DIC[i], 
                                         A.tot = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na))
  }
  # Some iterations don't converge completely, leading to single points that deviated from the rest of the curve. This is characterized by a single point that is a local maxima or minimum. Endpoints are asusmed to be good
  loop.check.left = loop[1:(resolution-2)] - loop[2:(resolution-1)]
  loop.check.right = loop[2:(resolution-1)] - loop[3:(resolution)]
  # If the signs are different, then it is a local shift
  loop.pos = c(TRUE, (sign(loop.check.left) == sign(loop.check.right)), TRUE)
  for(pos in which(loop.pos == FALSE)){ # Take the average
    loop[pos] = (loop[pos-1] + loop[pos+1])/2
  }
  E.cathode$pH = loop;
  # Loop pCO2 calculation as well, since the pCO2 function relies on the previous point
  loop = pCO2.xA.pH.A.k.beta.Na(xA = E.cathode$xA[1], pH = E.cathode$pH[1],
                                At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = stop.soln$p.CO2)
  for(i in 2:length(E.cathode$DIC)){
    loop[i] = pCO2.xA.pH.A.k.beta.Na(xA = E.cathode$xA[i], pH = E.cathode$pH[i],
                                  At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na, pCO2.prev = loop[i-1])
  }
  E.cathode$p.CO2 = loop
  E.cathode$q = abs(E.cathode$xA - E.cathode$xA[1])*A.tot*z*F # Coulombs
  
  # Equilibrium potential: Deviation from standard reduction potential
  E.anode$H = 10^-E.anode$pH
  E.anode$E = R*T/(z*F) * log( (1 - E.anode$xA)/E.anode$xA * 
                                   ((1 + beta1*E.anode$p.CO2 + beta2*E.anode$p.CO2^2)*k1*k2 + k1*E.anode$H + E.anode$H^2)/(k1*k2))
  E.cathode$H = 10^-E.cathode$pH
  E.cathode$E = R*T/(z*F) * log( (1 - E.cathode$xA)/E.cathode$xA * 
                                   ((1 + beta1*E.cathode$p.CO2 + beta2*E.cathode$p.CO2^2)*k1*k2 + k1*E.cathode$H + E.cathode$H^2)/(k1*k2))
  
  # Total energy
  E.cell = data.frame(q = E.anode$q, V = E.anode$E - E.cathode$E)
  # Only the positive energy demand
  E.cell = filter(E.cell, V > 0)
  len = length(E.cell$q)
  if(len == 0){
    E.cell = data.frame(q = rep(x = 0, times = 10), V = rep(x = 0, times = 10))
    len = 10
  }
  # E.anode$typ = "anode"; E.cathode$typ = "cathode"
  Energy.tot.sep = sum(0.5*(E.cell$V[2:len] + E.cell$V[1:(len-1)])*(E.cell$q[2:len] - E.cell$q[1:(len-1)]))
  
  ## Adjust the total energy by multiplying by the penalty function
  # Calculate the lean gas pressure
  # k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out
  p.lean = pCO2.lean(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.out = pCO2.out)
  penalty = weight.fun(pCO2.lean = p.lean)
  
  # Normalize by the total amount of carbon moved, i.e. units of kJ/mol
  DIC.capture = DIC.diff(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.in = pCO2.in, pCO2.out = pCO2.out)
  return(Energy.tot.sep*penalty/DIC.capture*1e-3)
}

```


```{r PCET Derived Functions: CO2 Flux}
# These equations are based on the framework for determining the CO2 flux as presented in Wilcox 2012.
Enhance.factor = function(pH, pCO2.in, A, k1, k2, beta1, beta2, pCO2){
  # Constants: general
  kw = 1e-14 # M^2
  kH = 3.4e-2; # M/atm
  z = 1 # OH- + CO2 = HCO3-
  # Constants: from Wilcox 2012
  Dco2 = 0.5e-5 # cm2/s, assume slowest due to high ionic strength
  kL = 0.1 # Assume fast mass transfer of typical range
  # Constants: average of Pocker 1997, Zeman 2007, Stolaroff 2008, Wilcox 2012
  k.rate = (6.03e3 + 6.745e3 + 8.5e3 + 12.1e3)/4
  # Constants: Lvov2012
  Doh = 5.2e-5 # cm2/s

  # Base concentration = OH + HQ- + 2Q--
  H = 10^-pH
  OH = kw/H
  base = OH + A*(2*k1*k2 + H*k1) / (H^2 + H*k1 + k1*k2*(1 + beta1*pCO2 + beta2*pCO2^2))
  
  # Interface CO2 concentration - assume 90% capture from the inlet
  CO2.int = 0.1*pCO2.in*kH
  
  # Hatta number: reaction rate / mass transfer rate
  Ha = sqrt(Dco2*base*k.rate)/kL
  # Instantaneous enhancement factor
  Ei = 1 + Doh*base / (z*Dco2*CO2.int)
  
  # return(c(Ei, Ha / tanh(Ha), Ha))
  # Check the extreme cases for E to simplify the equations
  if(Ha > 10*Ei){ # Instantaneous
    E = Ei
  } else if(Ha < Ei/2){ # Pseudo-1st order
    E = Ha / tanh(Ha)
  } else if(Ha > 3){ # 1st order
    E = Ha
  } else{ # No simplification - Solve the root that is less than Ei, as Ei is the upper bound
    x.guess = c(0.9, 0.95)*Ei
    for(i in 1:5){ # Newton's method
      y.guess = (Ha*(Ei - x.guess) / (Ei - 1)) / tanh(Ha*(Ei - x.guess) / (Ei - 1)) - x.guess
      slp.fit = (y.guess[1] - y.guess[2]) / (x.guess[1] - x.guess[2])
      E.guess = -y.guess[1]/slp.fit + x.guess[1]
      x.guess = c(0.975, 1.025)*E.guess
    }
    E = E.guess
  }
  #### Need to include order of magnitude for reaction rate with sorbent - use acid anhydride formation rate constants as estimates?
  return(E)
}

# Calculate the average kinetic driving force over the course of absorption (stage 4 -> 1)
kinetic.force = function(k1, k2, beta1, beta2, A.tot, Na, pCO2.in, pCO2.out){
  # Constants
  xA.lim = c(0.025, 0.975)
  kH = 3.4e-2; # M/atm
  # Calculate the pCO2 of the fully reduced species prior to equilibration with the gas
  out.pCO2 = pCO2.lean(Na = Na, A = A.tot, beta1 = beta1, beta2 = beta2, k1 = k1, k2 = k2, pCO2.out = pCO2.out)
  # If the minimum outlet pCO2 is greater than the target capture:
  # if(out.pCO2 > 0.1*pCO2.in | is.na(out.pCO2)){
  if(is.na(out.pCO2)){
    return(0)
  } else{
    # Calculate the pH at the start of desorption
    soln41 = data.frame(xA = max(xA.lim), p.CO2 = out.pCO2)
    # Solve pH with multiple cores
    soln41$pH = pH.xA.pCO2.A.k.beta.Na(xA = soln41$xA, P = soln41$p.CO2, 
                         At = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, Na = Na)
    soln41$E = Enhance.factor(pH = soln41$pH, pCO2.in = pCO2.in, A = A.tot, k1 = k1, k2 = k2, beta1 = beta1, beta2 = beta2, pCO2 = soln41$p.CO2)

    # Calculate the concentration difference between the interface and the bulk. Assuming 90% capture
    soln41$delC = (0.1*pCO2.in - out.pCO2)*kH
    # Flux: delC * kL * E, assume kL = 0.01 cm/s
    # Unit conversion: L to cm3, cm2 to m2
    flux = soln41$delC*soln41$E*0.01 * (1/1e3) * (100^2)
    # return(soln41)
    return(signif(flux, 5)) # Highest driving force at outlet
    # return(soln41)
  }
}

```


To work with GPareto, the two objective functions need to combined, and the input should be a matrix, not a dataframe. I account for this with a wrapper function to simplify the process.

```{r}
PCET.obj.flu = function(inputs){
  # Inputs is a matrix where each row is an instance and each column is a specific variable:
  # From left to right, the columns are:
  # pka1, err.pka2, log10(A.tot), Na/A.tot
  # For the functions, the variables should be:
  # k1, k2, A.tot, Na
  # The use of log units and ratios helps alleviate resolution issues associated with spanning multiple orders of magnitude
  # The use of the error of pKa2 removes the correlation between the two variables
  
  # Storing the proper information.
  if(is.matrix(inputs)){
    dat = data.frame(k1 = 10^-inputs[,1],
                     k2 = 10^-(inputs[,1] + inputs[,2]),
                     A.tot = 10^inputs[,3],
                     Na = inputs[,4]*10^inputs[,3])
  } else{
    # The optimization function sometimes stores as a vector instead of as a matrix if it is just a single point
    dat = data.frame(k1 = 10^-inputs[1],
                     k2 = 10^-(inputs[1] + inputs[2]),
                     A.tot = 10^inputs[3],
                     Na = inputs[4]*10^inputs[3])
  }
  # The following conditions are assumed for PCET from flue gas
  beta1 = 0; beta2 = 0;
  pCO2.in = 0.15; pCO2.out = 1;
  
  # The functions require substantial computation, so each row has to be processed independently
  energy = c(); flux = c()
  for(i in 1:nrow(dat)){
    energy[i] = Energy.tot(k1 = dat$k1[i], k2 = dat$k2[i], 
                           beta1 = 0, beta2 = 0, 
                           A.tot = dat$A.tot[i], Na = dat$Na[i], 
                           pCO2.in = 0.15, pCO2.out = 1)
    flux[i] = kinetic.force(k1 = dat$k1[i], k2 = dat$k2[i], 
                            beta1 = 0, beta2 = 0, 
                            A.tot = dat$A.tot[i], Na = dat$Na[i], 
                            pCO2.in = 0.15, pCO2.out = 1)
  }
  # Obtain the negative of the flux so it is a minimization function for the optimization search
  return(c(energy, -flux)) 
}

```

# Pareto frontier search

Using the GPareto package, using Gaussian Processes to find the Pareto frontier. The initial sampling design is based on the following constraints:

pKa1: 2 to 13.5
pKa2: linearly related to pKa1 with error term on (0, +5.5)
A.tot: 10 mM to 3.1 M (-2 to 0.5 in log10 units)
Na: -1 to 5 times the concentration of A.tot

While pKa1 can feasibly extend down to -8.3, the pH of the system is not going to extend below approximately 3 as a lowest estimate due to the strength of carbonic acid. As a result, pKa values below 2 are effectively identical.
A.tot represents the total concentration of quinone. 
Na represents the concentration of NaOH that was added to the system; a negative value instead represents HCl.
Since the capture process requires basic conditions, negative Na conditions (HCl addition) should be less extreme if they are present at all.
It is assumed that the background electrolyte (NaCl) is at extreme excess and accounts for all transport across the membrane.

The initial design finds all 'corners' of the 4-dimensional hypercube, the center of each face (extreme of 1, midpoints of the rest), and 7*4=28 random points.

```{r Pareto Search Initial Design}
# Initial ranges
pka1.rng = c(2, 13.5)
pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5)
Na.A.rng = c(-1, 5)

# Corners = extreme points
corners = expand.grid(pka1.rng, pka2.rng, logA.rng, Na.A.rng)
corners = corners[,c(1:4)]
names(corners) = c('pka1', 'pka2', 'logA', 'Na.A')
# Faces = extreme of 1 variable; midpoint of the rest
faces = data.frame(pka1 = c(pka1.rng, rep(mean(pka1.rng), 6)),
                   pka2 = c(rep(mean(pka2.rng), 2), pka2.rng, rep(mean(pka2.rng), 4)),
                   logA = c(rep(mean(logA.rng), 4), logA.rng, rep(mean(logA.rng), 2)),
                   Na.A = c(rep(mean(Na.A.rng), 6), Na.A.rng) )
# Random samples: number of variables times 7
nsamp = length(names(corners))*7
samples = data.frame(pka1 = runif(n = nsamp, min = min(pka1.rng), max = max(pka1.rng)),
                     pka2 = runif(n = nsamp, min = min(pka2.rng), max = max(pka2.rng)),
                     logA = runif(n = nsamp, min = min(logA.rng), max = max(logA.rng)),
                     Na.A = runif(n = nsamp, min = min(Na.A.rng), max = max(Na.A.rng)) )

design.start = rbind(faces, corners, samples)
# for(i in 1:nrow(design.start)){
#   PCET.obj.flu(inputs = as.matrix(design.start[i,]))
# }
# Output as a list
result.start = PCET.obj.flu(inputs = as.matrix(design.start))
result.start = matrix(result.start, ncol = 2)
Pareto.budget = 100
rm(corners, samples, faces)

# Store the initial design
design.start$Energy.kJ.mol = result.start[,1]
design.start$Flux.mol.m2s = result.start[,2]
write.csv(design.start, file = 'PCET_StartDesign.csv')

```

```{r Pareto Search: GPareto}
# Load design
design.start = read.csv(file = 'PCET_StartDesign.csv')
result.start = design.start[,c('Energy.kJ.mol', 'Flux.mol.m2s')]
design.start = design.start[,c('pka1', 'pka2', 'logA', 'Na.A')]
Pareto.budget = 200

res = easyGParetoptim(fn = PCET.obj.flu, budget = Pareto.budget, 
                      lower = c(2, 0, -2, -1), upper = c(13.5, 5.5, 0.5, 5), 
                      par = as.matrix(design.start), value = as.matrix(result.start), ncores = 2)
plotGPareto(res)

# Format into dataframe for easier plotting
GPar.front = data.frame(pka1 = res$par[,1], pka2 = res$par[,2], logA = res$par[,3], Na.A = res$par[,4],
                        Energy.kJ.mol = res$value[,1], Flux.mol.m2s = -res$value[,2])
GPar.all =  data.frame(pka1 = res$history$X[,1], pka2 = res$history$X[,2], logA = res$history$X[,3], 
                       Na.A = res$history$X[,4], 
                       Energy.kJ.mol = res$history$y[,1], Flux.mol.m2s = -res$history$y[,2])
GPar.all$order = c(rep(0, nrow(design.start)), seq(from = 1, to = Pareto.budget, by = 1))
# rm(res)
write.csv(GPar.all, file = 'GPar_all_data.csv')
write.csv(GPar.front, file = 'GPar_fnt_data.csv')
```

```{r Visualize the Pareto Front}
# rm(res)
GPar.all = read.csv(file = 'GPar_all_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
ggplot() +
  geom_point(filter(GPar.all, abs(Flux.mol.m2s) < 0.1, Energy.kJ.mol < 150), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s)) +
  geom_line(filter(GPar.front, abs(Flux.mol.m2s) < 1, Energy.kJ.mol < 150), 
            mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  geom_point(filter(GPar.front, abs(Flux.mol.m2s) < 1, Energy.kJ.mol < 150), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  labs(x = 'CO2 Flux (mol/m^2/s)', y = 'Energy Demand (kJ/mol)', subtitle = 'Pareto Frontier')

ggplot() +
  geom_point(filter(GPar.all, Flux.mol.m2s > 0), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s)) +
  geom_line(filter(GPar.front, Flux.mol.m2s > 0), 
            mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  geom_point(filter(GPar.front, Flux.mol.m2s > 0), 
             mapping = aes(y = Energy.kJ.mol, x = Flux.mol.m2s), color = 'cyan') +
  labs(x = 'CO2 Flux (mol/m^2/s)', y = 'Energy Demand (kJ/mol)', subtitle = 'Pareto Frontier')

```

# Requirements for CO2 capture

A substantial fraction of points were not able to satisfy the 90% capture constraint, as noted by their negative CO2 flux. 
The first step is to see if the size of the search space can be reduced for future optimization to only the 90% capture conditions.
This will be done by making a GP model of the kinetic rate, sampling points with a kinetic rate of 0 (signifying 90% capture), then marginalizating the probability that the result has positive flux.

The GP model sometimes does not converge because it is optimized using a genetic algorithm. 
This leads to a model that is unrepresentative. This function checks that the resulting model has adequate variance variability and outputs only models that make sense for the training data.

```{r 90% Capture Refinement Functions}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 covtyp = 'gauss', # Gaussian uncertainty
                 optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50), # Increase robustness
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 1000 points from the search space.
    pt = 1000; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}

fill.sample.obj.nec = function(x, model.flux){
  # Evaluate the Kriging model function at x 
  res.flux = predict(object = model.flux, 
                     newdata = data.frame(pka1 = x[1], 
                                          pka2 = x[2], 
                                          logA = x[3], 
                                          Na.A = x[4]), type = 'UK')

  # Probability distribution fits a Gaussian distribution
  # Interested in the probability that the distribution has a true mean of 0
  # Calculate a t-statistic, randomly sampling 1000 points from the normal distribution
  nsamp = 1000
  rsamp = rnorm(n = nsamp, mean = res.flux$mean, sd = res.flux$sd)
  p = t.test(x = rsamp, y = rep(0, nsamp), alternative = 'two.sided')
  prob = p$p.value
  
  # Variance based on propagation of errors, assuming independent measures
  sd = res.flux$sd

  # Objective result
  return(sd*(prob*(1-prob) + 0.25/9))
}

# Next point search function
fill.sample.nec = function(GPar.data){
  # Model
  mod.flux = fill.sample.mod(GPar.data = GPar.data, input.name = c('pka1', 'pka2', 'logA', 'Na.A'), 
                             output.name = 'Flux.mol.m2s')
  
  # Next point by genetic algorithm
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj.nec(x, model = mod.flux)},
               lower = c(2, 0, -2, -1), upper = c(13.5, 5.5, 0.5, 5),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  point.next = GA.pred@solution[1,]
  GPar.new = data.frame(pka1 = point.next[1],
                        pka2 = point.next[2],
                        logA = point.next[3],
                        Na.A = point.next[4])
  
  # True result for both the energy and kinetics to add this to the dataset
  res = PCET.obj.flu(inputs = point.next)
  GPar.new$Energy.kJ.mol = res[1]
  GPar.new$Flux.mol.m2s  = -res[2] # Flip sign because optimization function minimizes
  GPar.new$order = max(GPar.data$order) + 1

  # Also add the fitness to the dataframe for iteration cutoffs
  GPar.new$fit = max(GA.pred@fitness)
  
  # Return the new point and the fitness
  return(GPar.new)
}

```

Applying the functions with a looping iterative search

```{r 90% Capture Refinement Iterations}
# Load data
GPar.all = read.csv(file = 'GPar_all_data.csv')
# Remove indices
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]

# First iteration to set the baseline of how much improvement there is to find.
newpoint = fill.sample.nec(GPar.data = GPar.all)
start.fit = newpoint$fit; 
current.fit = newpoint$fit; 

# Repeat for a maximum of 200 iterations, or until the fitness drops below 1/1000 of the starting fitness, 
# indicating little further improvement
max.iter = max(GPar.all$order) + 200
while(max(GPar.all$order) < max.iter & current.fit*1e3 > start.fit){
  GPar.all = rbind(GPar.all, newpoint[,names(newpoint) %in% names(GPar.all)])
  newpoint = fill.sample.nec(GPar.data = GPar.all)
  current.fit = newpoint$fit
}

# Store the data
write.csv(GPar.all, file = 'GPar_90Cap_data.csv')


```

```{r 90% Capture Refinement Iteration Visualization}
# Plot the results to show refinement
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
ggplot(filter(GPar.all, Energy.kJ.mol < 100)) +
  geom_point(mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = (order > 200))) +
  facet_wrap(~(Flux.mol.m2s > 0), scales = 'free_x') +
  labs(x = 'CO2 flux at 90% capture (mol/m^2/s)', y = 'Energy demand (kJ/mol C)') +
  scale_color_manual(labels = c('Initial', 'Refinement'), values = c('red', 'blue'), name = '')

# Show on a log scale (separating the positive and negative fluxes) to see how low the magnitude of the flux can be
ggplot(filter(GPar.all, Energy.kJ.mol < 100)) +
  geom_point(mapping = aes(x = abs(Flux.mol.m2s), y = Energy.kJ.mol, color = (order > 200))) +
  facet_wrap(~(Flux.mol.m2s > 0), scales = 'free_x') +
  scale_x_log10() +
  labs(x = 'CO2 flux at 90% capture (mol/m^2/s)', y = 'Energy demand (kJ/mol C)') +
  scale_color_manual(labels = c('Initial', 'Refinement'), values = c('red', 'blue'), name = '')

# Display the characteristics of points with very high flux
g1 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4), 
               mapping = aes(x = pka1, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = pka1, color = 'bad')) +
  labs(x = 'pKa1', y = 'Probability Density') + guides(color = FALSE) + 
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g2 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = pka1 + pka2, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = pka1 + pka2, color = 'bad')) +
  labs(x = 'pKa2', y = 'Probability Density') + guides(color = FALSE) + 
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g3 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = 10^logA, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = 10^logA, color = 'bad')) +
  labs(x = '[Quinone] (M)', y = 'Probability Density') + guides(color = FALSE) +
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
g4 = ggplot() +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s > 1e-4),
               mapping = aes(x = 10^logA*Na.A, color = 'good')) +
  geom_density(filter(GPar.all, Energy.kJ.mol < 100, Flux.mol.m2s < 0), 
               mapping = aes(x = 10^logA*Na.A, color = 'bad')) +
  labs(x = 'Additional Base (M)', y = 'Probability Density') +
  scale_color_manual(values = c('good' = 'blue', 'bad' = 'red'),
                     labels = c('good' = 'High Flux', 'bad' = 'No Capture'),
                     name = '', breaks = c('good', 'bad'))
(g1 + g3) / (g2 + g4)

rm(g1, g2, g3, g4)
```

In addition to refining knowledge of the boundary where the flux is zero (90% capture), I show the distribution of "good" points (above 10^-4 mol/m2/s flux) and "bad" points (no flux).
The distinct differences between the two suggests that high and low flux conditions are very distinct in their pKa and concentration profiles, but the amount of acid or base is not a substantial factor.

After performing the marginalization to determine if the search space can be constrained, the Pareto front will be re-calculated with this new information.

Based on the distributions of the high flux conditions compared to the no capture conditions, it is likely that the result falls near an optimum rather than any of the extremes of the sample region.

For marginalization, assume a uniform distribution. 
While this is unrealistic, it is just to establish a constraint on the bounds rather than to identify real compounds.
Additionally, the interest is in removing the conditions that have a <5% likelihood of capturing >90% CO2, regardless of the other variables.

```{r 90% Capture Marginalization}
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
mod.flux = fill.sample.mod(GPar.data = GPar.all, input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                           output.name = 'Flux.mol.m2s')

# Set up the marginalization
resolution = 50; MCsamp = 2000
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-1, 5)
# Note: for the Na/A ratio, the interest is in the order of magnitude shifts, not the absolute shifts, going down to 10^-7.
# Since it is being tested for both positive and negative values, but they are of similar order of magnitude, half will be dedicated to each side

# Set the ranges
kin.constrain = data.frame(pka1 = seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                           pka2 = seq(from = pka2.rng[1], to = pka2.rng[2], length.out = resolution),
                           logA = seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                           Na.A = c(-10^seq(from = 0, to = -7, length.out = resolution/2),
                                    10^seq(from = -7, to = log10(Na.A.rng[2]), length.out = resolution/2)),
                           p.pka1 = NaN, p.pka2 = NaN, p.logA = NaN, p.Na.A = NaN, # Probability acceptance median
                           l.pka1 = NaN, l.pka2 = NaN, l.logA = NaN, l.Na.A = NaN, # lower bound
                           h.pka1 = NaN, h.pka2 = NaN, h.logA = NaN, h.Na.A = NaN) # upper bound
lower = 0.25; upper = 0.75

for(i in 1:resolution){
  # pka1
  fill.frame = data.frame(pka1 = kin.constrain$pka1[i],
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean - 1e-5, sd = res$sd)
  kin.constrain$p.pka1[i] = mean(fill.frame$p.accept)
  kin.constrain$h.pka1[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.pka1[i] = quantile(fill.frame$p.accept, probs = lower)

  # pka2
  fill.frame = data.frame(pka2 = kin.constrain$pka2[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean - 1e-5, sd = res$sd)
  kin.constrain$p.pka2[i] = mean(fill.frame$p.accept)
  kin.constrain$h.pka2[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.pka2[i] = quantile(fill.frame$p.accept, probs = lower)

  # pka1
  fill.frame = data.frame(logA = kin.constrain$logA[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean - 1e-5, sd = res$sd)
  kin.constrain$p.logA[i] = mean(fill.frame$p.accept)
  kin.constrain$h.logA[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.logA[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # pka2
  fill.frame = data.frame(Na.A = kin.constrain$Na.A[i],
                          pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame - 1e-5, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean, sd = res$sd)
  kin.constrain$p.Na.A[i] = mean(fill.frame$p.accept)
  kin.constrain$h.Na.A[i] = quantile(fill.frame$p.accept, probs = upper)
  kin.constrain$l.Na.A[i] = quantile(fill.frame$p.accept, probs = lower)
}

# Split the acid and base addition
kin.constrain$ph.cond = 'Acidic'
kin.constrain$ph.cond[kin.constrain$Na.A > 0] = 'Basic'

```

```{r}
kin.constrain
g1 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = pka1, y = p.pka1)) +
  geom_line(mapping = aes(x = pka1, y = h.pka1), linetype = 2) +
  geom_line(mapping = aes(x = pka1, y = l.pka1), linetype = 2) +
  labs(x = 'pKa1', y = 'Probability of >90% Capture')
g2 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = pka2, y = p.pka2)) +
  geom_line(mapping = aes(x = pka2, y = h.pka2), linetype = 2) +
  geom_line(mapping = aes(x = pka2, y = l.pka2), linetype = 2) +
  labs(x = 'pKa2 - pKa1', y = 'Probability of >90% Capture')
g3 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = 10^logA, y = p.logA)) +
  geom_line(mapping = aes(x = 10^logA, y = h.logA), linetype = 2) +
  geom_line(mapping = aes(x = 10^logA, y = l.logA), linetype = 2) +
  labs(x = 'Quinone Concentration', y = 'Probability of >90% Capture') +
  scale_x_log10()
g4 = ggplot(kin.constrain) +
  geom_line(mapping = aes(x = abs(Na.A), y = p.Na.A)) +
  geom_line(mapping = aes(x = abs(Na.A), y = h.Na.A), linetype = 2) +
  geom_line(mapping = aes(x = abs(Na.A), y = l.Na.A), linetype = 2) +
  facet_wrap(~ph.cond, scales = 'free_x') +  scale_x_log10() +
  labs(x = 'Na/Quinone Concentration Ratio', y = 'Probability of >90% Capture')

(g1 + g3) / (g2 + g4)
rm(g1, g2, g3, g4)
```

Dotted lines represent the asymmetric 50% confidence interval, i.e. 50% of outcomes where the other variables are randomly selected will fall between those two lines.
None of the partial dependence probability curves indicate an unambiguous region of the search space that can be omitted, it does indicate some interesting features:

* Lower pKa1 values are more likely to capture sufficient carbon, contrary to the belief that high pKas (both) would be most efficient due to a greater difference in H+ upon oxidation or reduction.
* Higher concentrations of quinone have higher variance, and thus are more likely to be unambiguously viable or inviable.
* It is difficult to determine if the uptick from very low quinone concentrations and the downticks from very high acid or base additions are from edge effects or not.
* The system works best if the two pKa values are closer together.

While these cannot be used to remove areas of the search space from sampling, it does give some indication of what to consider when comparing options.

One possible improvement to this presentation is to marginalize against 2 variables instead of just 1. 
It is likely that the combination of both pKas or of both concentrations is what matters, and this may provide more insight into restricting the space further since many inputs are co-correlated.

```{r 90% Capture 2D Marginal}
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
mod.flux = fill.sample.mod(GPar.data = GPar.all, input.name = c('pka1', 'pka2', 'logA', 'Na.A'),
                           output.name = 'Flux.mol.m2s')

# Set up the marginalization
resolution = 35; MCsamp = 1500
pka1.rng = c(2, 13.5); pka2.rng = c(0, 5.5)
logA.rng = c(-2, 0.5); Na.A.rng = c(-1, 5)
lower = 0.25; upper = 0.75

# Set up the grid search
pkaX.grid = expand.grid(seq(from = pka1.rng[1], to = pka1.rng[2], length.out = resolution),
                        seq(from = pka2.rng[1], to = pka2.rng[2], length.out = resolution))
pkaX.grid = pkaX.grid[,c(1:2)]
names(pkaX.grid) = c('pka1', 'pka2')
pkaX.grid$p = NaN; pkaX.grid$l = NaN; pkaX.grid$s = NaN # Probability and the high and low

# Note: for the Na/A ratio, the interest is in the order of magnitude shifts, not the absolute shifts, going down to 10^-7.
# Since it is being tested for both positive and negative values, but they are of similar order of magnitude, half will be dedicated to each side
conc.grid = expand.grid(seq(from = logA.rng[1], to = logA.rng[2], length.out = resolution),
                        c(-10^seq(from = 0, to = -7, length.out = resolution/2-1),
                          10^seq(from = -7, to = log10(Na.A.rng[2]), length.out = resolution/2)) )
conc.grid = conc.grid[,c(1:2)]
names(conc.grid) = c('logA', 'Na.A')
conc.grid$p = NaN; conc.grid$l = NaN; conc.grid$s = NaN # Probability and the high and low

# Loop
for(i in 1:nrow(pkaX.grid)){
  # pKa grid
  fill.frame = data.frame(pka1 = pkaX.grid$pka1[i],
                          pka2 = pkaX.grid$pka2[i],
                          logA = runif(n = MCsamp, min = logA.rng[1], max = logA.rng[2]),
                          Na.A = runif(n = MCsamp, min = Na.A.rng[1], max = Na.A.rng[2]))
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean, sd = res$sd)
  pkaX.grid$p[i] = mean(fill.frame$p.accept)
  pkaX.grid$h[i] = quantile(fill.frame$p.accept, probs = upper)
  pkaX.grid$l[i] = quantile(fill.frame$p.accept, probs = lower)
  
  # Concentration grid
  fill.frame = data.frame(pka1 = runif(n = MCsamp, min = pka1.rng[1], max = pka1.rng[2]),
                          pka2 = runif(n = MCsamp, min = pka2.rng[1], max = pka2.rng[2]),
                          logA = conc.grid$logA[i],
                          Na.A = conc.grid$Na.A[i])
  res = predict(object = mod.flux, newdata = fill.frame, type = 'UK')
  fill.frame$p.accept = pnorm(q = 0, mean = res$mean, sd = res$sd)
  conc.grid$p[i] = mean(fill.frame$p.accept)
  conc.grid$h[i] = quantile(fill.frame$p.accept, probs = upper)
  conc.grid$l[i] = quantile(fill.frame$p.accept, probs = lower)
}

# Split the acid and base addition
conc.grid$ph.cond = 'Acidic'
conc.grid$ph.cond[conc.grid$Na.A > 0] = 'Basic'

```

```{r}
# Input variables: pKa
g1 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = p)) +
  labs(x = 'pka1', y = '', fill = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(fill = FALSE)
g2 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = h)) +
  labs(x = '', y = '', fill = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(pkaX.grid) +
  geom_tile(mapping = aes(x = pka1, y = pka2, fill = l)) +
  labs(x = '', y = 'pka2 - pka1', fill = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(fill = FALSE)
g3 + g1 + g2

# Input variables: Concentrations
g1 = ggplot(conc.grid) +
  geom_raster(mapping = aes(x = logA, y = abs(Na.A), fill = p)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = 'log10 [Quinone]', y = '', fill = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_y_log10() + guides(fill = FALSE) +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 = ggplot(conc.grid) +
  geom_raster(mapping = aes(x = logA, y = abs(Na.A), fill = l)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = '', y = 'pH Corrector/Quinone', fill = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_y_log10() + guides(fill = FALSE) +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(conc.grid) +
  geom_raster(mapping = aes(x = logA, y = abs(Na.A), fill = h)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = '', y = '', fill = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_y_log10() +
  scale_fill_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 + g1 + g3

# Natural variables: pka
g1 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = p)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  labs(x = 'pka1', y = '', color = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(color = FALSE)
g2 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = h)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  labs(x = '', y = '', color = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(pkaX.grid) +
  geom_point(mapping = aes(x = pka1, y = pka1 + pka2, color = l)) +
  geom_hline(yintercept = c(10.3, 6.3)) +
  labs(x = '', y = 'pka2', color = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5) +
  guides(color = FALSE)
g3 + g1 + g2

# Input variables: Concentrations
g1 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*abs(Na.A), color = p)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = '[Quinone]', y = '', color = 'P[Flux > 0]', subtitle = 'Mean') +
  scale_y_log10() + scale_x_log10() + guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*abs(Na.A), color = l)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = '[Quinone]', y = 'pH Corrector', color = 'P[Flux > 0]', subtitle = '25th Percentile') +
  scale_y_log10() + scale_x_log10() + guides(color = FALSE) +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g3 = ggplot(conc.grid) +
  geom_point(mapping = aes(x = 10^logA, y = 10^logA*abs(Na.A), color = h)) +
  facet_grid(ph.cond ~ ., scales = 'free_y') +
  labs(x = '', y = '', color = 'P[Flux > 0]', subtitle = '75th Percentile') +
  scale_y_log10() + scale_x_log10() +
  scale_color_gradient2(limits = c(0, 1), 
                       low="navy", mid="white", high="red", midpoint = 0.5)
g2 + g1 + g3

rm(g1, g2, g3)
```

What is evident from these 2D partial component probability plots is that there are regions where the probability is very low, but they are not clearly delineated by single variable cutoffs.
The only case where there might be a cutoff is the concentration of quinone, where high (>1 M) concentrations lead to generally low probabilities of capture.
However, it is possible that this is from extrapolation errors due to how close this is to the edge of the search space causing fewer points to be sampled near there.

Insights from these 2D plots:

* Lower pKa1 values do, in fact, lead to better outcomes. In particular, the first pKa should be below 7.5 to guarantee capture. However, this is counter to the density function of the high flux conditions, which found typically higher pKa values led to better flux. This is reflected in the large variance in the high pKa estimates.
* There is a definitive band of pKa combinations that do not yield good results, mostly when the higher pKa value is around 10.3, i.e. the pKa of carbonate. This is likely because the buffer capacity benefit from the quinone is less important the more its buffer region overlaps with that of carbonate.
* Generally speaking, the concentration of additional acid or base has next to no impact on the flux. There is a slight trend towards lower concentrations, but it is difficult to tell if that is caused by the edge effects as observed with the concentration of quinone.
* There is a band of quinone concentrations of the order 0.1 to 1 M that more often than not leads to good outcomes. Below that region, the probability is typically low, while above that region, the variance is high.

# Refinement to optimal conditions

In addition to refining the model close to 0 to separate the conditions that are viable and inviable, the above process found some points that were previously unexplored which outperformed the Pareto front.
Therefore, the Pareto front needs to be updated.

```{r Updated Pareto Front}
# Load data
GPar.all = read.csv(file = 'GPar_90Cap_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
# Compare the matrices of just the outputs - need as a matri
test = as.matrix(GPar.all[GPar.all$Flux.mol.m2s > 0,names(GPar.all) %in% c('Energy.kJ.mol', 'Flux.mol.m2s')])
# For comparison, need both to minimize
test[,2] = -test[,2]

par.front = t(nondominated_points(points = t(test)))
par.front[,2] = -par.front[,2]
ggplot() +
  geom_vline(xintercept = 0.1*max(GPar.front$Flux.mol.m2s), color = 'black', linetype = 3) +
  geom_point(filter(GPar.all, Flux.mol.m2s > 0, Energy.kJ.mol < 100),
             mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol)) +
  # New front
  geom_line(data.frame(par.front), mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'new')) +
  geom_point(data.frame(par.front), mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'new')) +
  # Old front
  geom_line(GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'old')) +
  geom_point(GPar.front, mapping = aes(x = Flux.mol.m2s, y = Energy.kJ.mol, color = 'old')) +
  labs(x = 'CO2 flux (mol/m2/s)', y = 'Energy Demand (kJ/mol C)') +
  scale_color_manual(values = c('new' = 'red', 'old' = 'cyan'), 
                     labels = c('new' = 'New Front', 'old' = 'Old Front'),
                     name = '')
  

# Identify the conditions leading to the Pareto front
GPar.front = filter(GPar.all, Energy.kJ.mol %in% par.front[,1], Flux.mol.m2s %in% par.front[,2])
GPar.front = GPar.front[,!names(GPar.front) %in% 'X']
write.csv(GPar.front, file = 'GPar_fnt_data_90Cap.csv')

```

The energy target for CO2 capture from flue gas is approximately 25-30 kJe/mol C to achieve a < 35% increase in cost relative to a power plant not capturing carbon.
The majority of the Pareto front satisfies this criteria, and the points that do not are still relatively low (up to 37 kJe/mol C) and competitive with temperature-swing absorption.
The entire Pareto front also falls entirely above 10% of the maximum flux (vertical dotted line), suggesting that the flux does not have substantial variability.
The selection criteria is therefore going to be all points that meet both an energy and flux cutoff threshold set by the 1-variable optima.
To give some amount of tolerance, the flux cutoff is 10% of the maximum flux, and the energy cutoff is 40 kJ/mol C.

Based on prior tests with mathematical test functions, this type of criteria is both robust to modeling and converges relatively quickly.
An initial pass on the inputs conditions that meet this criteria:

```{r Optimal Capture Marginals Before Refinement}

```


### ----------------------
3. Calculatate the normalized distance in the objective space and f1/f2 prioritization of each point to the Pareto front. These two metrics will be used to establish the acceptable suboptimal performance and the relative importance of each input variable.

Normalization is defined such that (0,0) is the utopia point and (1,0) and (0,1) are the single-objective optimizations. The prioitization is the angle $\theta$ made between the point in the normalized objective space and the x-axis, i.e. the axis of purely prioritizing objective 2. This angle is re-mapped to a percentage from 0 to 100% prioritization of objective 2. The distance between each point and the Pareto front is the distance in the normalized space

```{r Map to Normalized Objective Space}
GPar.all = read.csv(file = 'GPar_all_data.csv')
GPar.front = read.csv(file = 'GPar_fnt_data.csv')
Pareto.budget = nrow(GPar.all) - sz^3
# Saved data have an index
GPar.all = GPar.all[, !(names(GPar.all) %in% c("X"))]
GPar.front = GPar.front[, !(names(GPar.front) %in% c("X"))]
# Normalized objective functions
n.obj = function(GPar.data, GPar.front){
  # Given dataframes that describe the entire dataset and the front, find the normalized (x,y)
  # Objective functions are named 'f1' and 'f2'
  
  # Normalize the objective outputs so that the utopia point is (0,0) and the nadir point is (1,1)
  f1.up = GPar.front$f1[which.min(GPar.front$f2)]
  f2.up = GPar.front$f2[which.min(GPar.front$f1)]
  GPar.data$f1.norm = (GPar.data$f1 - min(GPar.front$f1))/(f1.up - min(GPar.front$f1))
  GPar.data$f2.norm = (GPar.data$f2 - min(GPar.front$f2))/(f2.up - min(GPar.front$f2))
  # In the event that the optimum is better:
  GPar.data$f1.norm[GPar.data$f1.norm < 0] = 0
  GPar.data$f2.norm[GPar.data$f2.norm < 0] = 0
  return(GPar.data)
}
# Run normalization
GPar.all = n.obj(GPar.data = GPar.all, GPar.front = GPar.front)
GPar.front = n.obj(GPar.data = GPar.front, GPar.front = GPar.front)
GPar.front = GPar.front[order(GPar.front$f1.norm),]

# Calculate theta according to the angle. Remap from [0, pi/2] to [0, 100] for simplicity
GPar.all$theta = atan(GPar.all$f2.norm/GPar.all$f1.norm)*180/pi*10/9

# Calculate the normalized distance
n.dist = function(f1.norm, f2.norm, GPar.front){
  # Given the normalized coordinates (f1.norm, f2.norm) and the Pareto frontier estimate,
  # find the distance along the constant f2/f1 ratio line
  
  # Determine the two points on the Pareto front that define the relevant segment
  GPar.front$theta = atan(GPar.front$f2.norm / GPar.front$f1.norm)
  if(f1.norm < 0){f1.norm = 0}
  if(f2.norm < 0){f2.norm = 0}
  ratio = atan(f2.norm/f1.norm)
  
  # Check if the angle is the same as a point on the Pareto front
  if(any(abs(ratio - GPar.front$theta) < 1e-5)){
    pos = which.min(abs(ratio - GPar.front$theta))
    Par.x = GPar.front$f1.norm[pos]
    Par.y = GPar.front$f2.norm[pos]
  } else{ # Otherwise, two points are needed for linear interpolation
    # Break the dataframe into theta above and below
    Par.above = GPar.front[GPar.front$theta - ratio > 0,]
    Par.below = GPar.front[GPar.front$theta - ratio < 0,]
    # Find the point closest to the angle
    pos.above = which.min(abs(ratio - Par.above$theta))
    pos.below = which.min(abs(ratio - Par.below$theta))
    # Linear interpolation
    ln.x = c(Par.above$f1.norm[pos.above], Par.below$f1.norm[pos.below])
    ln.y = c(Par.above$f2.norm[pos.above], Par.below$f2.norm[pos.below])
    slp = diff(ln.y)/diff(ln.x)
    # Find the point on the segment with the same angle, ie. the same ratio.
    # Solving with this constraint has analytical solution:
    Par.x = (ln.y[1] - slp*ln.x[1]) / (f2.norm/f1.norm - slp)
    Par.y = slp*(Par.x - ln.x[1]) + ln.y[1]
  }
  
  # Linear distance to the front is the difference between distances to the origin
  dist = sqrt(f1.norm^2 + f2.norm^2) - sqrt(Par.x^2 + Par.y^2)
  return(dist)
}

# Run distance calculations in parallel to speed up
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.all)) %dopar%
  n.dist(f1.norm = GPar.all$f1.norm[row], f2.norm = GPar.all$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.all$dist = unlist(dist)
rm(dist, cl)


# Save the starting data to test multiple types of acceptance criteria
write.csv(x = GPar.all, file = 'GPar_all_start.csv')
write.csv(x = GPar.front, file = 'GPar_fnt_start.csv')

```


## II. Acceptable suboptimal performance (the "near-Pareto set")
4. Establish a selection criteria for acceptable suboptimal performance (eg. within some tolerance of the Pareto front).

For this example, defines $\delta$ as the distance between the point and the point on the Pareto frontier with the same value of theta. Accept only points with $\delta < 2$.

```{r Load data: Accept Distance less than 2}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: check if the first name is x1
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

```

5. Using Gaussian processes, create a new objective called the "sample utility" function that describes the probability of meeting the desired distance and objective function criteria, multiplied by the variance.

The sample utility function prioritizes points that are likely to describe the boundary (normalized distance equal to 1) with the greatest model uncertainty at the time. 
When the point is sampled, this means information gained is maximized. 
Points on the boundary have intermediate acceptance probabilities, ie. close to 0.5, while points with the greatest model uncertainty have the greatest standard error. 
Therefore, the utility function can be estimated as the product of the standard error, the probability of being accepted, and the probability of being rejected.


```{r}
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist.init, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.dist.init, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```

6. Sample new points iteratively by maximizing the sample utility function. Repeat until the computational budget has been expended or a stop criteria is met.

A typical stop criteria is based on the hypervolume fraction that is uncertain, i.e. an acceptance probability between 0.25 and 0.75. However, as the number of input variables increases, the number of sampled points required for an accurate Monte Carlo estimate increases dramatically. Instead, a single point criteria is more useful; in this case, the metric is the maximum sample utility in the optimization function. When the value drops by 2 orders of magnitude, then the next point is not likely to provide much improvement.

```{r Iteration loop: Accept distance less than 1, message=FALSE, warning=FALSE}
budget = 20

next.fit = start.fit
while(next.fit > start.fit*0.01 & max(GPar.all$order) < budget+Pareto.budget){
  # Add the  new point to the dataset
  GPar.all = rbind(GPar.all, GPar.new)
  
  # Create new Kriging model
  mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')
  
  # Run the optimization to find the best point
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj(x, model = mod.dist)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 50, maxiter = 50, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
  GPar.front = fill.sample.front.check(point.new = GPar.new, front.old = GPar.front)

  # Fill in the remaining calculations: normalized outputs, distance, theta, order
  GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front)
  cl <- makeCluster(2)
  registerDoParallel(cl)
  dist = foreach(row = 1:nrow(GPar.new)) %dopar%
    n.dist(f1.norm = GPar.new$f1.norm[row], f2.norm = GPar.new$f2.norm[row], GPar.front = GPar.front)
  stopCluster(cl)
  GPar.new$dist = unlist(dist)
  GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
  GPar.new$order = max(GPar.all$order) + 1
  
  # The cutoff point for the loop is 20 additional points or when the 
  # fitness value is less than 1% of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

GPar.all = rbind(GPar.all, GPar.new)
# Save the data for later
write.csv(GPar.all, 'GPar_Accept_Delta1.csv')

rm(point.next)
```

```{r}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Delta1.csv')
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')

lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.dist, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.dist, newdata = fine.grid, type = "UK")
fine.grid$dist.mean = res$mean
fine.grid$dist.sd = res$sd

# Probability that the distance is less than 1
fine.grid$dist.p1 = pnorm(q = 0, mean = fine.grid$dist.mean - 2, sd = fine.grid$dist.sd)
fine.grid$uncert = fine.grid$dist.p1*(1-fine.grid$dist.p1)+1e-10
ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$dist.sd)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = dist.sd, z = dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$dist.sd*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*dist.sd, z = uncert*dist.sd), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)

```
## III. Feature importance of each input variable
7. Calculate the distance between each point and the Pareto front in both the input and objective space.
8. Compare the distance in the objective space to the Pareto front and the distance in only a single variable. The most important variable is that which has the largest correlation coefficient.

```{r Feature Importance: Correlation Coefficient and Slope}
# Load data
GPar.all = read.csv(file = 'GPar_Accept_Delta1.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Remove leading indices: the names are:
nm = c("x1",  "x2", "x3", "f1", "f2", "order",  "f1.norm", "f2.norm", "dist", "theta")
GPar.all = GPar.all[,nm]
nm = c("x1",  "x2", "x3", "f1", "f2",  "f1.norm", "f2.norm")
GPar.front = GPar.front[,nm]

# Define the process used to collect the point
GPar.all$proc = 'Initial'
GPar.all$proc[GPar.all$order > 0] = 'Pareto'
GPar.all$proc[GPar.all$order > Pareto.budget] = 'Post'
# Break up the prioritization into favoring f1, f2, or a balance of the two
GPar.all$region = "f1"
GPar.all$region[GPar.all$theta < 200/3] = "50:50"
GPar.all$region[GPar.all$theta < 100/3] = "f2"

pt.size = 3
g1 = ggplot() +
  geom_point(data = filter(GPar.all, dist > 1), mapping = aes(x = theta, y = x1, shape = proc), color = 'red', size = pt.size, alpha = 0.5) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x1, color = dist, shape = proc), size = pt.size) +
  labs(x = "", y = expression("x"[1]), color = "Distance:\nObjective\nSpace") +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(shape = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g2 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x2, shape = proc), color = 'red', size = pt.size, alpha = 0.5) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x2, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[2]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g3 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x3, shape = proc), color = 'red', size = pt.size, alpha = 0.5) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x3, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[3]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g1 / g2 / g3

# Approximate the Pareto front with a smoothed line using a Savitzky-Golay filter
Pfront.x1 = loess(x1 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                                x1 = filter(GPar.all, dist == 0)$x1))
Pfront.x2 = loess(x2 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x2 = filter(GPar.all, dist == 0)$x2))
Pfront.x3 = loess(x3 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x3 = filter(GPar.all, dist == 0)$x3))

# Store the information in a dataframe for visualization
plt = data.frame(theta = seq(from = 0, to = 100, length.out = 100))
res1 = predict(object = Pfront.x1, newdata = plt)
res2 = predict(object = Pfront.x2, newdata = plt)
res3 = predict(object = Pfront.x3, newdata = plt)
plt$x1 = unname(res1); plt$x2 = unname(res2); plt$x3 = unname(res3)
# Check the Savitzkyâ€“Golay filter smoothing quality
gx1 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x1)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1))
gx2 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x2)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2))
gx3 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x3)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3))
gx1 / gx2 / gx3

# Apply the model to find the linear distance (regression)
GPar.all$x1.loess.Err = abs(GPar.all$x1 - predict(object = Pfront.x1, newdata = GPar.all$theta))
GPar.all$x2.loess.Err = abs(GPar.all$x2 - predict(object = Pfront.x2, newdata = GPar.all$theta))
GPar.all$x3.loess.Err = abs(GPar.all$x3 - predict(object = Pfront.x3, newdata = GPar.all$theta))

# Calculate the correlation coefficient of the log(objective distance) with the linear distance
x1.cor = cor(x = filter(GPar.all, dist > 0)$x1.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x2.cor = cor(x = filter(GPar.all, dist > 0)$x2.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x3.cor = cor(x = filter(GPar.all, dist > 0)$x3.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
# Calculate the slope of the linear relationship
x1.slp = lm((dist) ~ x1.loess.Err, data = filter(GPar.all, dist > 0))
x2.slp = lm((dist) ~ x2.loess.Err, data = filter(GPar.all, dist > 0))
x3.slp = lm((dist) ~ x3.loess.Err, data = filter(GPar.all, dist > 0))
x1.slp = unname(x1.slp$coefficients[2]); x2.slp = unname(x2.slp$coefficients[2])
x3.slp = unname(x3.slp$coefficients[2])

sz = 3
# Store information in a dataframe for plotting
GPar.cor.plt = data.frame(obj.dist = rep(GPar.all$dist, 3),
                          inp.dist = c(GPar.all$x1.loess.Err, GPar.all$x2.loess.Err, GPar.all$x3.loess.Err),
                          var = c(rep('x1', nrow(GPar.all)), rep('x2', nrow(GPar.all)), rep('x3', nrow(GPar.all))),
                          region = rep(GPar.all$region, 3),
                          val = c(GPar.all$x1, GPar.all$x2, GPar.all$x3))
var.labs <- c(paste('x1, r = ', round(x1.cor, 3), '; slope = ', round(x1.slp, 3)), 
              paste('x2, r = ', round(x2.cor, 3), '; slope = ', round(x2.slp, 3)),
              paste('x3, r = ', round(x3.cor, 3), '; slope = ', round(x3.slp, 3)))
names(var.labs) <- c("x1", "x2", "x3")

ggplot(GPar.cor.plt) +
  geom_point(mapping = aes(x = inp.dist, y = (obj.dist), shape = region, color = val), size = sz) +
  facet_grid(~var, labeller = labeller(var = var.labs)) + 
  labs(x = 'Distance: Input Space', y = expression('log'[10]*' Distance: Objective Space'), shape = 'Priority',
       color = 'Input Value')

rm(g1, g2, g3, gx1, gx2, gx3, Pfront.x1, Pfront.x2, Pfront.x3, plt, GPar.cor.plt)
```

Based on this regression-based interpretation of the ranking, it appears that x3 is more important than x2, which is more important that x1. While one would expect x2 and x3 to be of equivalent importance, it is likely that the exact points that were sampled biased more towards showing a trend with x3 over x2.

9. Calculate the conditional probability that a point will fall within the near-Pareto set given only information about the most important input variable.

From the above calculations, the most important variable is x1, as it has a stronger relationship between distance in the objective space and distance in the input space (i.e. a larger perturbation in the input will lead to a less optimal result).

For the purposes of illustration, both P[accept | $x_1$] and P[accept | $x_2$] will be shown to show how the regression is consistent with the probabilities.
The conditional probability is calculated by marginalization of the GP probabilities. From the GP, the value of P[accept | $x_1, x_2$] can be calculated, and the relationship between P[accept | $x_1, x_2$] and  P[accept | $x_1$] is the integral:

P[accept | $x_1$] = integral( P[accept | $x_1, x_2$]  P[$x_2$] $dx_2$ )

In the absence of data to inform the distribution of $x_2$, this distribution is set as a uniform distribution on the range used in the optimization.

```{r Marginalization}
# Use the final GP model with the full dataset
mod.dist = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'dist')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 500 samples each.
resolution = 50; MCsamp = 1500
x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2); x3.rng = range(GPar.all$x3)
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)

Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()

for(i in 1:resolution){
  # x1
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), var = 'x1', cond = 'none'))
  
  # x2
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'none'))
  
  # x3
  fill.frame = data.frame(x3 = x3.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), var = 'x3', cond = 'none'))
}

ggplot() +
  geom_path(data = Infer.x1, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x2, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x3, mapping = aes(x = x, y = prob, color = var)) +
  labs(x = expression('x'), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 2')) +
  scale_color_manual(name = 'Variable', values = c('x1' = 'red', 'x2' = 'blue', 'x3' = 'purple'), 
                    labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2]),'x3' = expression('x'[3]))) +
  theme_bw()
```

Consistent with the correlation coefficients, the probability of acceptance when only $x_2$ or $x_3$ is known has a higher and sharper peak than $x_1$. The reason for $x_3$'s relative dominance is clear here: the sampling for $x_3$ apparently did not capture the local minima, whereas it did capture the local minima from $x_2$.

10. Calculate the same conditional probability, but given the that the most important variable falls within its optimal range and the second most important variable is known.
11. Repeat step 10 by continuing to constrain the optimal range of the previous variables until all conditional probabilities have been found. This sequential analysis describes the characteristics of the most promising inputs for the near-Pareto set.

For the 3-input set, there are 6 possible pairwise conditionals, where one variable is known precisely and the other falls within its single-variable optimal range, eg. accept | $x_2, x_1 = x_{1,opt}$

For the purposes of comparison, the 'best' permutation (optimize x3, then x2, then x1) will be compared to the worst permutation (optimize x1, then x2, then x3). These orders are based on the importance rankings.


```{r Partial Conditional Probabilities}
# Determining the ranges
# Function to find the optimal range given the probability of acceptance
p.lims = function(data.prob, p.target){
  # data.prob is a dataframe with variables x and prob, which are the values and probability of acceptance
  
  # Limits are the range where the probability is at least half the maximim
  # p.target = 0.5*max(data.prob$prob)
  
  # Initial limits
  x.lims = range(filter(data.prob, prob >= p.target)$x)
  
  # Linear interpolate to find the proper point, assuming that the limit is not already the minimum possible value
  if(x.lims[1] > min(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[1]))
    sub = data.prob[c(pos-1, pos), ]
    mod = lm(x~prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[1] = res
  }
  if(x.lims[2] < max(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[2]))
    sub = data.prob[c(pos, pos+1), ]
    mod = lm(x ~ prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[2] = res
    rm(mod)
  }
  return(x.lims)
}
x1.lims = p.lims(data.prob = Infer.x1, p.target = 0.5*max(Infer.x1$prob))
x3.lims = p.lims(data.prob = Infer.x3, p.target = 0.5*max(Infer.x3$prob))

## Given that x1 or x3 is optimal, find the next variable in the series: x2
resolution = 50; MCsamp = 1500
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x2.x1 = data.frame(); Infer.x2.x3 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x1'))
  # x2 | x3
  fill.frame = data.frame(x2 = x2.seq[i], x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x2.x3 = rbind(Infer.x2.x3, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x3'))
}

ggplot(rbind(Infer.x2.x3, Infer.x2.x1)) +
  geom_path(mapping = aes(x = x, y = prob, color = cond)) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Given optimal') +
  theme_bw()

# Instead of a simple range, the multimodal peaks of these probability curves mean a simple min/max bound can be used. While a nearest-neighbor approximation would work, a faster solution is to find 3 separate ranges.
x2.lims.x3.rng1 = p.lims(data.prob = filter(Infer.x2.x3, x < 0.25), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x3.rng2 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x3.rng3 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x1.rng1 = p.lims(data.prob = filter(Infer.x2.x1, x < 0.25), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
x2.lims.x1.rng2 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
x2.lims.x1.rng3 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
# The number of points to collect from each range is proportional to their relative widths

## Given that x1 or x3 is optimal, find the next variable in the series: x2
# resolution = 50; MCsamp = 1500
MCsamp.x2.x3.rng1 = round(1500*diff(x2.lims.x3.rng1)/
                            (diff(x2.lims.x3.rng1) + diff(x2.lims.x3.rng2) + diff(x2.lims.x3.rng3)))
MCsamp.x2.x3.rng2 = round(1500*diff(x2.lims.x3.rng2)/
                            (diff(x2.lims.x3.rng1) + diff(x2.lims.x3.rng2) + diff(x2.lims.x3.rng3)))
MCsamp.x2.x3.rng3 = 1500 - MCsamp.x2.x3.rng1 - MCsamp.x2.x3.rng2
MCsamp.x2.x1.rng1 = round(1500*diff(x2.lims.x1.rng1)/
                            (diff(x2.lims.x1.rng1) + diff(x2.lims.x1.rng2) + diff(x2.lims.x1.rng3)))
MCsamp.x2.x1.rng2 = round(1500*diff(x2.lims.x1.rng2)/
                            (diff(x2.lims.x1.rng1) + diff(x2.lims.x1.rng2) + diff(x2.lims.x1.rng3)))
MCsamp.x2.x1.rng3 = 1500 - MCsamp.x2.x1.rng1 - MCsamp.x2.x1.rng2

x1.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x3.x2.x1 = data.frame(); Infer.x1.x2.x3 = data.frame()
for(i in 1:resolution){
  # x3 | x2, x1
  x2.fill = c(runif(n = MCsamp.x2.x1.rng1, min = min(x2.lims.x1.rng1), max = max(x2.lims.x1.rng1)),
              runif(n = MCsamp.x2.x1.rng2, min = min(x2.lims.x1.rng2), max = max(x2.lims.x1.rng2)),
              runif(n = MCsamp.x2.x1.rng3, min = min(x2.lims.x1.rng3), max = max(x2.lims.x1.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = x3.seq[i])
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x3.x2.x1 = rbind(Infer.x3.x2.x1, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x3', cond = 'x2, x1'))
  # x1 | x2, x3
  x2.fill = c(runif(n = MCsamp.x2.x3.rng1, min = min(x2.lims.x3.rng1), max = max(x2.lims.x3.rng1)),
              runif(n = MCsamp.x2.x3.rng2, min = min(x2.lims.x3.rng2), max = max(x2.lims.x3.rng2)),
              runif(n = MCsamp.x2.x3.rng3, min = min(x2.lims.x3.rng3), max = max(x2.lims.x3.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = x1.seq[i])
  res = predict(object = mod.dist, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res$mean - 2, sd = res$sd)
  Infer.x1.x2.x3 = rbind(Infer.x1.x2.x3, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x1', cond = 'x2, x3'))
}

# Combine results for visualization
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3, Infer.x2.x1, Infer.x2.x3, 
                  Infer.x1.x2.x3, Infer.x3.x2.x1);
write.csv(Infer.plt, 'Marginals_Accept_Delta1.csv')
rm(res, x2.fill, fill.frame)
```


```{r}
Infer.plt = read.csv(file = 'Marginals_Accept_Delta1.csv')

ggplot(filter(Infer.plt, cond == 'none')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(var))) +
  labs(x = expression('x'), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, var == 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, cond == 'x2, x3' | cond == 'x2, x1' | cond == 'none', var != 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  facet_wrap(~var, nrow = 2) +
  labs(x = 'x', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()


```
```{r}
rm(list = ls(pattern = "^Infer\\."))
```

While the peak probabilities after 2 conditionals are similar (around 80% probability at the best input value) regardless of order, it becomes more obvious the rationale for picking variables from most to least important when looking at $x_2 | x_1$ compared to $x_2 | x_3$. When the more important $x_3$ is known, the probability of acceptance is very high (> 75% at the peak), but when $x_1$ is known, the probability of acceptance for a specific $x_2$ value does not change from not knowing any additional information.

## Alternative Acceptance Criteria
For the purpose of illustration, the above method will be repeated with two other acceptance criteria:
* Objective function values below a specific threshold.
* Within a specific normalized distance of the utopia point and a specified prioritization of the two objective functions.

### Threshold cutoff
For simplicity, the cutoff values are the normalized values of 1 for both objectives. This is criteria, the acceptable points are those that fall within the normalized box defined by the utopia point (0,0) and the pseudo-nadir point (1,1).

Load the original dataset.

```{r Threshold: Load original data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: check if the first name is x1
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

```

```{r Treshold: First iteration, message=FALSE, warning=FALSE}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 covtyp = 'gauss', # Gaussian uncertainty
                 optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50), # Increase robustness
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}
fill.sample.obj2 = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), 
  # and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x 
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 2, sd = res.f1$sd) * 
    pnorm(q = 0, mean = res.f2$mean - 2, sd = res.f2$sd)
  
  # Variance based on propagation of errors, assuming independent measures
  # sd = (res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2)
  sd = sqrt(res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2)

  # Objective result
  return(sd*(prob*(1-prob) + 0.25/9))
}

# Search for the first point
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.obj2(x, model.f1 = mod.f1, model.f2 = mod.f2)},
             lower = c(0, 0, 0), upper = c(1, 1, 1),
             popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
res = ZDT4.mod(x = as.matrix(GPar.new))
GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
GPar.front = fill.sample.front.check(point.new = GPar.new, front.old = GPar.front)
# Fill in the remaining calculations: normalized outputs, distance, theta, order
GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.new)) %dopar%
  n.dist(f1.norm = GPar.new$f1.norm[row], f2.norm = GPar.new$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.new$dist = unlist(dist)
GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
GPar.new$order = max(GPar.all$order) + 1

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.f1.init = mod.f1
mod.f2.init = mod.f2
```

Visualize the first loop again to show that the set of the acceptable points is different due to the different criteria.

```{r Threshold: First Iteration}
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.f1.init, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.f1.init, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2.init, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```


```{r Threshold: Iterations, message=FALSE, warning=FALSE}
budget = 20
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Add the  new point to the dataset
  GPar.all = rbind(GPar.all, GPar.new)
  
  # Create a Kriging model for the distance
  mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
  mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj2(x, model.f1 = mod.f1, model.f2 = mod.f2)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
  GPar.front = fill.sample.front.check(point.new = GPar.new, front.old = GPar.front)

  # Fill in the remaining caluclations: normalized outputs, distance, theta, order
  GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front)
  cl <- makeCluster(2)
  registerDoParallel(cl)
  dist = foreach(row = 1:nrow(GPar.new)) %dopar%
    n.dist(f1.norm = GPar.new$f1.norm[row], f2.norm = GPar.new$f2.norm[row], GPar.front = GPar.front)
  stopCluster(cl)
  GPar.new$dist = unlist(dist)
  GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
  GPar.new$order = max(GPar.all$order) + 1
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

GPar.all = rbind(GPar.all, GPar.new)
write.csv(GPar.all, 'GPar_Accept_Threshold.csv')

rm(res)
```

```{r Treshold: Visualize Iterations}
# Plot results on top of the most recent Kriging model
GPar.all = read.csv(file = 'GPar_Accept_Threshold.csv')
# Create a Kriging model for the distance
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x3 = c(GPar.new$x3),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x2 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x3', 'x2')
# Apply Kriging function of distance
res = predict(object = mod.f1, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

ncolor = 7
g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x2', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x2, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x2, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x2), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr",  name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')
(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

## x3 slices
lower = c(0, 0.3, 0); upper = c(1, 0.7, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(seq(from = lower[3], to = upper[3], length.out = 50)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')
# Apply Kriging function of distance
res = predict(object = mod.f1, newdata = fine.grid, type = "UK")
fine.grid$f1.mean = res$mean
fine.grid$f1.sd = res$sd

res = predict(object = mod.f2, newdata = data.frame(x1 = fine.grid$x1, 
                     x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$f2.mean = res$mean
fine.grid$f2.sd = res$sd

# Probability that the distance is less than 1
fine.grid$prob = pnorm(q = 0, mean = fine.grid$f1.mean - 2, sd = fine.grid$f1.sd) * 
  pnorm(q = 0, mean = fine.grid$f2.mean - 2, sd = fine.grid$f2.sd)
fine.grid$uncert = fine.grid$prob*(1-fine.grid$prob)+1e-10

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$f1.sd/max(fine.grid$f1.sd))^2 + (fine.grid$f2.sd/max(fine.grid$f2.sd))^2

g1 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert, z = uncert), 
                      breaks = seq(from = 0, to = 0.25, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = '', y = 'x3', title = 'Uncertainty')
rng = range(fine.grid$info)
g2 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = info, z = info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  labs(x = 'x1', y = '', title = 'Information Gain')
rng = range(fine.grid$info*fine.grid$uncert)
g3 = ggplot(fine.grid) +
  geom_contour_filled(mapping = aes(x = x1, y = x3, color = uncert*info, z = uncert*info), 
                      breaks = seq(from = rng[1]*0.99, to = rng[2]*1.01, length.out = ncolor), alpha = 0.5) +
  geom_point(data = GPar.all, mapping = aes(x = x1, y = x3, color = order), size = 1.5, alpha = 0.5) +
  geom_point(data = GPar.new, mapping = aes(x = x1, y = x3), color = 'red', size = 3) +
  guides(color = FALSE) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr", name = '')  +
  labs(x = '', y = '', title = 'Sample Utility')

(g1 + guides(fill = FALSE)) + (g2 + guides(fill = FALSE)) + g3

rm(g1, g2, g3, fine.grid)
```

As indicated by this acceptance criteria, the boundary of the set it well defined with just the Pareto frontier, so additional iterations are not as necessary. Continuing with the procedure to show the feature importance and acceptance probability procedures.

```{r Threshold Feature Importance: Correlation Coefficient and Slope}
# Load data
GPar.all = read.csv(file = 'GPar_Accept_Threshold.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Remove leading indices: the names are:
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

# Define the process used to collect the point
GPar.all$proc = 'Initial'
GPar.all$proc[GPar.all$order > 0] = 'Pareto'
GPar.all$proc[GPar.all$order > Pareto.budget] = 'Post'
# Break up the prioritization into favoring f1, f2, or a balance of the two
GPar.all$region = "f1"
GPar.all$region[GPar.all$theta < 200/3] = "50:50"
GPar.all$region[GPar.all$theta < 100/3] = "f2"

pt.size = 3
g1 = ggplot() +
  geom_point(data = filter(GPar.all, dist > 1), mapping = aes(x = theta, y = x1, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x1, color = dist, shape = proc), size = pt.size) +
  labs(x = "", y = expression("x"[1]), color = "Distance:\nObjective\nSpace") +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(shape = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g2 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x2, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x2, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[2]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g3 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x3, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x3, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[3]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g1 / g2 / g3

# Approximate the Pareto front with a smoothed line using a Savitzky-Golay filter
Pfront.x1 = loess(x1 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                                x1 = filter(GPar.all, dist == 0)$x1))
Pfront.x2 = loess(x2 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x2 = filter(GPar.all, dist == 0)$x2))
Pfront.x3 = loess(x3 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x3 = filter(GPar.all, dist == 0)$x3))

# Store the information in a dataframe for visualization
plt = data.frame(theta = seq(from = 0, to = 100, length.out = 100))
res1 = predict(object = Pfront.x1, newdata = plt)
res2 = predict(object = Pfront.x2, newdata = plt)
res3 = predict(object = Pfront.x3, newdata = plt)
plt$x1 = unname(res1); plt$x2 = unname(res2); plt$x3 = unname(res3)
# Check the Savitzkyâ€“Golay filter smoothing quality
gx1 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x1)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1))
gx2 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x2)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2))
gx3 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x3)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3))
gx1 / gx2 / gx3

# Apply the model to find the linear distance (regression)
GPar.all$x1.loess.Err = abs(GPar.all$x1 - predict(object = Pfront.x1, newdata = GPar.all$theta))
GPar.all$x2.loess.Err = abs(GPar.all$x2 - predict(object = Pfront.x2, newdata = GPar.all$theta))
GPar.all$x3.loess.Err = abs(GPar.all$x3 - predict(object = Pfront.x3, newdata = GPar.all$theta))

# Calculate the correlation coefficient of the log(objective distance) with the linear distance
x1.cor = cor(x = filter(GPar.all, dist > 0)$x1.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x2.cor = cor(x = filter(GPar.all, dist > 0)$x2.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x3.cor = cor(x = filter(GPar.all, dist > 0)$x3.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
# Calculate the slope of the linear relationship
x1.slp = lm((dist) ~ x1.loess.Err, data = filter(GPar.all, dist > 0))
x2.slp = lm((dist) ~ x2.loess.Err, data = filter(GPar.all, dist > 0))
x3.slp = lm((dist) ~ x3.loess.Err, data = filter(GPar.all, dist > 0))
x1.slp = unname(x1.slp$coefficients[2]); x2.slp = unname(x2.slp$coefficients[2])
x3.slp = unname(x3.slp$coefficients[2])

sz = 3
# Store information in a dataframe for plotting
GPar.cor.plt = data.frame(obj.dist = rep(GPar.all$dist, 3),
                          inp.dist = c(GPar.all$x1.loess.Err, GPar.all$x2.loess.Err, GPar.all$x3.loess.Err),
                          var = c(rep('x1', nrow(GPar.all)), rep('x2', nrow(GPar.all)), rep('x3', nrow(GPar.all))),
                          region = rep(GPar.all$region, 3),
                          val = c(GPar.all$x1, GPar.all$x2, GPar.all$x3))
var.labs <- c(paste('x1, r = ', round(x1.cor, 3), '; slope = ', round(x1.slp, 3)), 
              paste('x2, r = ', round(x2.cor, 3), '; slope = ', round(x2.slp, 3)),
              paste('x3, r = ', round(x3.cor, 3), '; slope = ', round(x3.slp, 3)))
names(var.labs) <- c("x1", "x2", "x3")

ggplot(GPar.cor.plt) +
  geom_point(mapping = aes(x = inp.dist, y = (obj.dist), shape = region, color = val), size = sz) +
  facet_grid(~var, labeller = labeller(var = var.labs)) + 
  labs(x = 'Distance: Input Space', y = expression('log'[10]*' Distance: Objective Space'), shape = 'Priority',
       color = 'Input Value')

rm(g1, g2, g3, gx1, gx2, gx3, Pfront.x1, Pfront.x2, Pfront.x3, plt, GPar.cor.plt)
```

As with the Pareto distance, $x_2$ and $x_3$ should have equal importance, but $x_3$ appears to be more important given the dataset bias.

```{r Threshold: Marginalization}
# Use the final GP model with the full dataset
mod.f1 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f1.norm')
mod.f2 = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'f2.norm')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 500 samples each.
resolution = 50; MCsamp = 1500
x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2); x3.rng = range(GPar.all$x3)
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)

Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()

for(i in 1:resolution){
  # x1
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), var = 'x1', cond = 'none'))
  
  # x2
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'none'))
  
  # x3
  fill.frame = data.frame(x3 = x3.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), var = 'x3', cond = 'none'))
}

ggplot() +
  geom_path(data = Infer.x1, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x2, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x3, mapping = aes(x = x, y = prob, color = var)) +
  labs(x = expression('x'), y = 'Probability of Acceptance', subtitle = expression('Accept if f'[1]*' < 1 & f'[2]*' < 1')) +
  scale_color_manual(name = 'Variable', values = c('x1' = 'red', 'x2' = 'blue', 'x3' = 'maroon'), 
                    labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2]),'x3' = expression('x'[3]))) +
  theme_bw()

```

Results for the importance rankings based on probability with this criteria is similar to the profiles for Pareto distance, although the peaks appear sharper.

```{r Threshold: Partial Conditional Probabilities}
# Determining the ranges
# Function to find the optimal range given the probability of acceptance
p.lims = function(data.prob, p.target){
  # data.prob is a dataframe with variables x and prob, which are the values and probability of acceptance
  
  # Limits are the range where the probability is at least half the maximim
  # p.target = 0.5*max(data.prob$prob)
  
  # Initial limits
  x.lims = range(filter(data.prob, prob >= p.target)$x)
  
  # Linear interpolate to find the proper point, assuming that the limit is not already the minimum possible value
  if(x.lims[1] > min(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[1]))
    sub = data.prob[c(pos-1, pos), ]
    mod = lm(x~prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[1] = res
  }
  if(x.lims[2] < max(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[2]))
    sub = data.prob[c(pos, pos+1), ]
    mod = lm(x ~ prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[2] = res
    rm(mod)
  }
  return(x.lims)
}
x1.lims = p.lims(data.prob = Infer.x1, p.target = 0.5*max(Infer.x1$prob))
x3.lims = p.lims(data.prob = Infer.x3, p.target = 0.5*max(Infer.x3$prob))

## Given that x1 or x3 is optimal, find the next variable in the series: x2
resolution = 50; MCsamp = 1500
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x2.x1 = data.frame(); Infer.x2.x3 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x1'))
  # x2 | x3
  fill.frame = data.frame(x2 = x2.seq[i], x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x2.x3 = rbind(Infer.x2.x3, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x3'))
}

ggplot(rbind(Infer.x2.x3, Infer.x2.x1)) +
  geom_path(mapping = aes(x = x, y = prob, color = cond)) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Given optimal') +
  theme_bw()

# Instead of a simple range, the multimodal peaks of these probability curves mean a simple min/max bound can be used. While a nearest-neighbor approximation would work, a faster solution is to find 3 separate ranges.
x2.lims.x3.rng1 = p.lims(data.prob = filter(Infer.x2.x3, x < 0.25), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x3.rng2 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x3.rng3 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x1.rng1 = p.lims(data.prob = filter(Infer.x2.x1, x < 0.25), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
x2.lims.x1.rng2 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
x2.lims.x1.rng3 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
# The number of points to collect from each range is proportional to their relative widths

## Given that x1 or x3 is optimal, find the next variable in the series: x2
# resolution = 50; MCsamp = 1500
MCsamp.x2.x3.rng1 = round(1500*diff(x2.lims.x3.rng1)/
                            (diff(x2.lims.x3.rng1) + diff(x2.lims.x3.rng2) + diff(x2.lims.x3.rng3)))
MCsamp.x2.x3.rng2 = round(1500*diff(x2.lims.x3.rng2)/
                            (diff(x2.lims.x3.rng1) + diff(x2.lims.x3.rng2) + diff(x2.lims.x3.rng3)))
MCsamp.x2.x3.rng3 = 1500 - MCsamp.x2.x3.rng1 - MCsamp.x2.x3.rng2
MCsamp.x2.x1.rng1 = round(1500*diff(x2.lims.x1.rng1)/
                            (diff(x2.lims.x1.rng1) + diff(x2.lims.x1.rng2) + diff(x2.lims.x1.rng3)))
MCsamp.x2.x1.rng2 = round(1500*diff(x2.lims.x1.rng2)/
                            (diff(x2.lims.x1.rng1) + diff(x2.lims.x1.rng2) + diff(x2.lims.x1.rng3)))
MCsamp.x2.x1.rng3 = 1500 - MCsamp.x2.x1.rng1 - MCsamp.x2.x1.rng2

x1.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x3.x2.x1 = data.frame(); Infer.x1.x2.x3 = data.frame()
for(i in 1:resolution){
  # x3 | x2, x1
  x2.fill = c(runif(n = MCsamp.x2.x1.rng1, min = min(x2.lims.x1.rng1), max = max(x2.lims.x1.rng1)),
              runif(n = MCsamp.x2.x1.rng2, min = min(x2.lims.x1.rng2), max = max(x2.lims.x1.rng2)),
              runif(n = MCsamp.x2.x1.rng3, min = min(x2.lims.x1.rng3), max = max(x2.lims.x1.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = x3.seq[i])
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x3.x2.x1 = rbind(Infer.x3.x2.x1, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x3', cond = 'x2, x1'))
  # x1 | x2, x3
  x2.fill = c(runif(n = MCsamp.x2.x3.rng1, min = min(x2.lims.x3.rng1), max = max(x2.lims.x3.rng1)),
              runif(n = MCsamp.x2.x3.rng2, min = min(x2.lims.x3.rng2), max = max(x2.lims.x3.rng2)),
              runif(n = MCsamp.x2.x3.rng3, min = min(x2.lims.x3.rng3), max = max(x2.lims.x3.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = x1.seq[i])
  res1 = predict(object = mod.f1, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.f2, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * pnorm(q = 0, mean = res2$mean - 2, sd = res2$sd)
  Infer.x1.x2.x3 = rbind(Infer.x1.x2.x3, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x1', cond = 'x2, x3'))
}

# Combine results for visualization
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3, Infer.x2.x1, Infer.x2.x3, 
                  Infer.x1.x2.x3, Infer.x3.x2.x1);
write.csv(Infer.plt, 'Marginals_Accept_Threshold.csv')
rm(res1, res2, x2.fill, fill.frame)
```

```{r}
Infer.plt = read.csv(file = 'Marginals_Accept_Threshold.csv')

ggplot(filter(Infer.plt, cond == 'none')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(var))) +
  labs(x = expression('x'), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, cond == 'x2, x3' | cond == 'x2, x1' | cond == 'none', var != 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  facet_wrap(~var, nrow = 2) +
  labs(x = 'x', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, var == 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

```

```{r}
rm(list = ls(pattern = "^Infer\\."))

```

### Utopia Distance and Prioritization
Since the utopia point is set to (0,0), the distance to the utopia point is simple to calculate from the normalized coordinates. Acceptable points are those that prioritize the first objective by at least 80%, which can be determined by the angle theta.

Load the original dataset.

```{r Utopia Distance: Load Data}
# Load data
GPar.all = read.csv(file = 'GPar_all_start.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')

# Remove leading indices: the names are:
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}
```

```{r Utopia Distance: First Iteration, message=FALSE, warning=FALSE}
fill.sample.mod = function(GPar.data, input.name, output.name){
  # Calculate the GP model to use. 
  # Using the km function, but applies checks on the system to make sure that 
  # the model uncertainty matches expectations based on GP, ie. it did not
  # fail to converge.
  
  # Based on testing, the model is bad when the 10% percentile and 90% percentile 
  # of the standard deviation are of the same order of magnitude. This is easiest
  # checked if the difference between the 10th and 90th percentile
  # is larger than the difference between the 25th and 75th.
  pt10 = 1; pt90 = 1; pt25 = 1; pt75 = 1
  while(log10(pt90/pt10) <= log10(pt75/pt25)){
    mod.out = km(design = GPar.data[, input.name], response = GPar.data[, output.name], 
                 covtyp = 'gauss', # Gaussian uncertainty
                 optim.method = 'gen', # Genetic algorithm optimization
                 control = list(trace = FALSE, # Turn off tracking to simplify output
                                pop.size = 50), # Increase robustness
                 nugget = 1e-6, # Avoid eigenvalues of 0
                 )
    
    # Randomly sample 200 points from the search space
    pt = 200; i = 1
    lims = range(GPar.data[,input.name[i]])
    samp = data.frame(runif(n = pt, min = lims[1], max = lims[2]))
    for(i in 2:length(input.name)){
      lims = range(GPar.data[,input.name[i]])
      samp[,i] = runif(n = pt, min = lims[1], max = lims[2])
    }
    names(samp) = input.name
    
    # Find model output to find the percentile ranks for this iteration
    res = predict(object = mod.out, newdata = samp, type = 'UK')
    pt10 = quantile(res$sd, 0.10); pt90 = quantile(res$sd, 0.90)
    pt25 = quantile(res$sd, 0.25); pt75 = quantile(res$sd, 0.75)
  }
  return(mod.out)
}
fill.sample.obj3 = function(x, model.f1, model.f2){
  # Given data that contains: function evaluations (f1, f2) at inputs (x1, x2), 
  # and the normalized distance of (f1, f2) to the Pareto front
  # the function will output the objective function evaluated at x = (x1, x2)
  
  # Evaluate the Kriging model function at x. In this case, f1 and f2 are models for utopia distance and angle.
  res.f1 = predict(object = model.f1, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  res.f2 = predict(object = model.f2, newdata = data.frame(x1 = x[1], x2 = x[2], x3 = x[3]), type = 'UK')
  
  # Probability distribution fits a Gaussian distribution
  prob = pnorm(q = 0, mean = res.f1$mean - 2, sd = res.f1$sd) * 
    (1 - pnorm(q = 0, mean = res.f2$mean - 20, sd = res.f2$sd))
  
  # Variance based on propagation of errors
  sd = sqrt(res.f1$sd^2*res.f2$mean^2 + res.f2$sd^2*res.f1$mean^2)

  # Objective result
  return(sd*(prob*(1-prob) + 0.25/9))
}

# Search for the first point
GPar.all$rad = sqrt(GPar.all$f1.norm^2 + GPar.all$f2.norm^2)
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')
GA.pred = ga(type = 'real-valued',
             fitness = function(x){fill.sample.obj3(x, model.f1 = mod.rad, model.f2 = mod.ang)},
             lower = c(0, 0, 0), upper = c(1, 1, 1),
             popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
             parallel = 2)
# Next point to sample is the solution to this optimization
point.next = GA.pred@solution[1,]

# Account for the possibility that the genetic algorithm converges on multiple equivalent points
GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
res = ZDT4.mod(x = as.matrix(GPar.new))
GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
GPar.front = fill.sample.front.check(point.new = GPar.new, front.old = GPar.front)
# Fill in the remaining caluclations: normalized outputs, distance, theta, order
GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front)
cl <- makeCluster(2)
registerDoParallel(cl)
dist = foreach(row = 1:nrow(GPar.new)) %dopar%
  n.dist(f1.norm = GPar.new$f1.norm[row], f2.norm = GPar.new$f2.norm[row], GPar.front = GPar.front)
stopCluster(cl)
GPar.new$dist = unlist(dist)
GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
GPar.new$order = max(GPar.all$order) + 1
GPar.new$rad = sqrt(GPar.new$f1.norm^2 + GPar.new$f2.norm^2)

# The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
start.fit = max(GA.pred@fitness)
# Store first point for plotting
GPar.new1 = GPar.new
rm(dist)

# Store the initial mod.dist for later
mod.rad.init = mod.rad
mod.ang.init = mod.ang

# Grid of the relevant region to visualize
lower = c(0, 0, 0); upper = c(1, 1, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2, seq(from = lower[2], to = upper[2], length.out = 25)),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(GPar.new$x3, seq(from = lower[3], to = upper[3], length.out = 25)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Apply Kriging functions
res = predict(object = mod.rad.init, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang.init, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
               mapping = aes(x = x1, y = x2, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info*(prob)*(1-prob), color = info*(prob)*(1-info)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3),
               mapping = aes(x = x1, y = x3, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[3]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info*(prob)*(1-prob), color = info*(prob)*(1-prob)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = GPar.new1, mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", 
       shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

# Sometimes the grid scan finds a point that is slightly better than the perceived optimum due to the convergence criteria. In that event, replace the starting point for the cutoff criteria for iteration
start.fit = max(start.fit, max(fine.grid$inf*fine.grid$prob*(1-fine.grid$prob)))

rm(g1, g2, g3, fine.grid)
```

```{r Utopia Distance: Repeated Iterations, message=FALSE, warning=FALSE}
budget = 20
next.fit = start.fit
while(next.fit > start.fit*0.005 & max(GPar.all$order) < budget+Pareto.budget){
  # Add the  new point to the dataset
  GPar.all = rbind(GPar.all, GPar.new)
  
  # Create a Kriging model for the distance
  mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
  mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')
  GA.pred = ga(type = 'real-valued',
               fitness = function(x){fill.sample.obj3(x, model.f1 = mod.rad, model.f2 = mod.ang)},
               lower = c(0, 0, 0), upper = c(1, 1, 1),
               popSize = 100, maxiter = 100, run = 10, monitor = FALSE,
               parallel = 2)
  
  # Next point to sample is the solution to this optimization. 
  # Account for the possibility that the genetic algorithm converges on multiple equivalent points
  point.next = GA.pred@solution[1,]
  
  # Find values of the selected point
  GPar.new = data.frame(x1 = point.next[1], x2 = point.next[2], x3 = point.next[3])
  res = ZDT4.mod(x = as.matrix(GPar.new))
  GPar.new$f1 = res[,1]; GPar.new$f2 = res[,2]
  GPar.front = fill.sample.front.check(point.new = GPar.new, front.old = GPar.front)
  
  # Fill in the remaining caluclations: normalized outputs, distance, theta, order
  GPar.new = n.obj(GPar.data = GPar.new, GPar.front = GPar.front)
  cl <- makeCluster(2)
  registerDoParallel(cl)
  dist = foreach(row = 1:nrow(GPar.new)) %dopar%
    n.dist(f1.norm = GPar.new$f1.norm[row], f2.norm = GPar.new$f2.norm[row], GPar.front = GPar.front)
  stopCluster(cl)
  GPar.new$dist = unlist(dist)
  GPar.new$theta = atan(GPar.new$f2.norm/GPar.new$f1.norm)*180/pi*10/9
  GPar.new$order = max(GPar.all$order) + 1
  GPar.new$rad = sqrt(GPar.new$f1.norm^2 + GPar.new$f2.norm^2)
  
  # The cutoff point for the loop is 20 additional points or when the fitness value is less than half of this starting fitness value
  next.fit = max(GA.pred@fitness)
}

GPar.all = rbind(GPar.all, GPar.new)

write.csv(GPar.all, 'GPar_Accept_Radius.csv')

rm(res, cl, point.next)
```

```{r}
GPar.all = read.csv(file = 'GPar_Accept_Radius.csv')
# Plot results on top of the most recent Kriging model
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

# Grid of the relevant region to visualize
lower = c(0, 0, 0); upper = c(1, 1, 1)
fine.grid = expand.grid(x1 = seq(from = lower[1], to = upper[1], length.out = 50), 
                        x2 = c(GPar.new$x2, seq(from = lower[2], to = upper[2], length.out = 25)),
                        # x3 = c(0, 0.01, 0.1, 0.5, 1))
                        x3 = c(GPar.new$x3, seq(from = lower[3], to = upper[3], length.out = 25)))
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Apply Kriging functions
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd

res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd

# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

# Information gain = variance = (var1*mean2)^2 + (var2*mean1)^2
fine.grid$info = (fine.grid$rad.sd/max(fine.grid$rad.sd))^2 + (fine.grid$ang.sd/max(fine.grid$ang.sd))^2

ncolor = 7; pt.size = 2.5
g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
               mapping = aes(x = x1, y = x2, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[2]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x3 == GPar.new$x3, x2 != GPar.new$x2), 
                      mapping = aes(x = x1, y = x2, z = info*(prob)*(1-prob), color = info*(prob)*(1-info)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x2, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x2, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x2, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

g1 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = (prob)*(1-prob)-1e-10,
                                    color = (prob)*(1-prob)-1e-10), alpha = 0.5,
                      breaks = seq(from = -0.01, to = 0.25, length.out = ncolor)
                      )+
  geom_contour(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3),
               mapping = aes(x = x1, y = x3, z = prob), breaks = c(0.25, 0.75), color = 'blue') +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = "", y = expression("x"[3]), subtitle = 'Uncertainty') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g2 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info, color = info), bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(palette = "PuOr") +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  labs(x = expression("x"[1]), y = "", subtitle = 'Information Gain') +  guides(fill = FALSE, color = FALSE, shape = FALSE)

g3 = ggplot() +
  geom_contour_filled(data = filter(fine.grid, x2 == GPar.new$x2, x3 != GPar.new$x3), 
                      mapping = aes(x = x1, y = x3, z = info*(prob)*(1-prob), color = info*(prob)*(1-prob)),
                      bins = ncolor, alpha = 0.5)+
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], order == 0),
             mapping = aes(x = x1, y = x3, color = 'initial', shape = 'initial'), size = pt.size) +
  geom_point(data = filter(GPar.all, x1 <= upper[1], x1 >= lower[1], x2 <= upper[2], x2 >= lower[2], 
                           order > 0, order < Pareto.budget+1),
             mapping = aes(x = x1, y = x3, color = 'Pareto', shape = 'Pareto'), size = pt.size) +
  geom_point(data = filter(GPar.all, order > Pareto.budget), 
             mapping = aes(x = x1, y = x3, color = 'Post', shape = 'Post'), size = pt.size) +
  scale_fill_brewer(labels = c("Low", rep("", ncolor-3), "High"), palette = "PuOr")  +
  scale_color_manual(values = c('initial' = 'black', 'Pareto' = 'darkorchid3', 'Post' = 'red3'),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  scale_shape_manual(values = c('initial' = 15, 'Pareto' = 16, 'Post' = 17),
                     labels = c('Initial', 'Pareto', 'Post'), breaks = c('initial', 'Pareto', 'Post')) +
  labs(x = "", y = "", subtitle = "Sample Utility", fill = "", 
       shape = 'Collection\nProcess', color = 'Collection\nProcess') +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  guides(color=guide_legend(override.aes=list(fill=NA))) # Remove fill in the legend

g1 + g2 + g3

# x3 slices
sz.fine = 100
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = sz.fine)
x3.rng = seq(from = 0, to = 1, length.out = 9) # Simple slice for visualization
fine.grid = expand.grid(x1.rng, x2.rng, x3.rng)
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Calculate
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd
# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x2, color = prob)) +
  labs(x = expression('x'[1]), y = expression('x'[2]), color = expression('P[accept]')) +
  theme_classic() + facet_wrap(~x3) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

# x2 slices
sz.fine = 100
x1.rng = seq(from = 0, to = 1, length.out = sz.fine)
x3.rng = seq(from = 0, to = 1, length.out = sz.fine)
x2.rng = seq(from = 0, to = 1, length.out = 9) # Simple slice for visualization
fine.grid = expand.grid(x1.rng, x2.rng, x3.rng)
fine.grid = fine.grid[,1:3]
names(fine.grid) = c('x1', 'x2', 'x3')

# Calculate
res = predict(object = mod.rad, newdata = fine.grid, type = "UK")
fine.grid$rad.mean = res$mean
fine.grid$rad.sd = res$sd
res = predict(object = mod.ang, newdata = data.frame(x1 = fine.grid$x1, 
                      x2 = fine.grid$x2, x3 = fine.grid$x3), type = "UK")
fine.grid$ang.mean = res$mean
fine.grid$ang.sd = res$sd
# Uncertainty = P(success)*(1 - P(success))
fine.grid$prob = pnorm(q = 0, mean = fine.grid$rad.mean - 2, sd = fine.grid$rad.sd) * 
  (1 - pnorm(q = 0, mean = fine.grid$ang.mean - 20, sd = fine.grid$ang.sd))

ggplot() +
  geom_point(data = fine.grid, mapping = aes(x = x1, y = x3, color = prob)) +
  labs(x = expression('x'[1]), y = expression('x'[3]), color = expression('P[accept]')) +
  theme_classic() + facet_wrap(~x2) + 
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))

rm(g1, g2, g3, fine.grid, res)
```


```{r Utopia distance Feature Importance: Correlation Coefficient and Slope}
# Load data
GPar.all = read.csv(file = 'GPar_Accept_Radius.csv')
GPar.front = read.csv(file = 'GPar_fnt_start.csv')
# Remove leading indices: the names are:
while(names(GPar.all)[1] != 'x1'){
  GPar.all = GPar.all[,2:length(names(GPar.all))]
}
while(names(GPar.front)[1] != 'x1'){
  GPar.front = GPar.front[,2:length(names(GPar.front))]
}

# Define the process used to collect the point
GPar.all$proc = 'Initial'
GPar.all$proc[GPar.all$order > 0] = 'Pareto'
GPar.all$proc[GPar.all$order > Pareto.budget] = 'Post'
# Break up the prioritization into favoring f1, f2, or a balance of the two
GPar.all$region = "f1"
GPar.all$region[GPar.all$theta < 200/3] = "50:50"
GPar.all$region[GPar.all$theta < 100/3] = "f2"

pt.size = 3
g1 = ggplot() +
  geom_point(data = filter(GPar.all, dist > 1), mapping = aes(x = theta, y = x1, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x1, color = dist, shape = proc), size = pt.size) +
  labs(x = "", y = expression("x"[1]), color = "Distance:\nObjective\nSpace") +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(shape = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g2 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x2, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x2, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[2]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g3 = ggplot() +
  geom_point(data = filter(GPar.all, dist >  1), mapping = aes(x = theta, y = x3, shape = proc), color = 'red', size = pt.size) +
  geom_smooth(data = filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3), color = "black", level = 0) +
  geom_point(data = filter(GPar.all, dist <= 1), mapping = aes(x = theta, y = x3, color = dist, shape = proc), size = pt.size) +
  labs(x = "Trade-Off", y = expression("x"[3]), color = "", shape = 'Collection\nProcess') +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100), labels = c(expression("f"[2]*" min"), "", "50:50", "", expression("f"[1]*" min")), 
                     expand = c(0.01, 0.01)) +
  guides(color = FALSE) + theme_bw() + scale_y_continuous(expand = c(0.05, 0.05), limits = c(0, 1))
g1 / g2 / g3

# Approximate the Pareto front with a smoothed line using a Savitzky-Golay filter
Pfront.x1 = loess(x1 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                                x1 = filter(GPar.all, dist == 0)$x1))
Pfront.x2 = loess(x2 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x2 = filter(GPar.all, dist == 0)$x2))
Pfront.x3 = loess(x3 ~ theta, data = data.frame(theta = filter(GPar.all, dist == 0)$theta, 
                                               x3 = filter(GPar.all, dist == 0)$x3))

# Store the information in a dataframe for visualization
plt = data.frame(theta = seq(from = 0, to = 100, length.out = 100))
res1 = predict(object = Pfront.x1, newdata = plt)
res2 = predict(object = Pfront.x2, newdata = plt)
res3 = predict(object = Pfront.x3, newdata = plt)
plt$x1 = unname(res1); plt$x2 = unname(res2); plt$x3 = unname(res3)
# Check the Savitzkyâ€“Golay filter smoothing quality
gx1 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x1)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x1))
gx2 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x2)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x2))
gx3 = ggplot() +
  geom_path(plt, mapping = aes(x = theta, y = x3)) +
  geom_point(filter(GPar.all, dist == 0), mapping = aes(x = theta, y = x3))
gx1 / gx2 / gx3

# Apply the model to find the linear distance (regression)
GPar.all$x1.loess.Err = abs(GPar.all$x1 - predict(object = Pfront.x1, newdata = GPar.all$theta))
GPar.all$x2.loess.Err = abs(GPar.all$x2 - predict(object = Pfront.x2, newdata = GPar.all$theta))
GPar.all$x3.loess.Err = abs(GPar.all$x3 - predict(object = Pfront.x3, newdata = GPar.all$theta))

# Calculate the correlation coefficient of the log(objective distance) with the linear distance
x1.cor = cor(x = filter(GPar.all, dist > 0)$x1.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x2.cor = cor(x = filter(GPar.all, dist > 0)$x2.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
x3.cor = cor(x = filter(GPar.all, dist > 0)$x3.loess.Err, 
             y = (filter(GPar.all, dist > 0)$dist), 
             method = 'pearson')
# Calculate the slope of the linear relationship
x1.slp = lm((dist) ~ x1.loess.Err, data = filter(GPar.all, dist > 0))
x2.slp = lm((dist) ~ x2.loess.Err, data = filter(GPar.all, dist > 0))
x3.slp = lm((dist) ~ x3.loess.Err, data = filter(GPar.all, dist > 0))
x1.slp = unname(x1.slp$coefficients[2]); x2.slp = unname(x2.slp$coefficients[2])
x3.slp = unname(x3.slp$coefficients[2])

sz = 3
# Store information in a dataframe for plotting
GPar.cor.plt = data.frame(obj.dist = rep(GPar.all$dist, 3),
                          inp.dist = c(GPar.all$x1.loess.Err, GPar.all$x2.loess.Err, GPar.all$x3.loess.Err),
                          var = c(rep('x1', nrow(GPar.all)), rep('x2', nrow(GPar.all)), rep('x3', nrow(GPar.all))),
                          region = rep(GPar.all$region, 3),
                          val = c(GPar.all$x1, GPar.all$x2, GPar.all$x3))
var.labs <- c(paste('x1, r = ', round(x1.cor, 3), '; slope = ', round(x1.slp, 3)), 
              paste('x2, r = ', round(x2.cor, 3), '; slope = ', round(x2.slp, 3)),
              paste('x3, r = ', round(x3.cor, 3), '; slope = ', round(x3.slp, 3)))
names(var.labs) <- c("x1", "x2", "x3")

ggplot(GPar.cor.plt) +
  geom_point(mapping = aes(x = inp.dist, y = (obj.dist), shape = region, color = val), size = sz) +
  facet_grid(~var, labeller = labeller(var = var.labs)) + 
  labs(x = 'Distance: Input Space', y = expression('log'[10]*' Distance: Objective Space'), shape = 'Priority',
       color = 'Input Value')

rm(g1, g2, g3, gx1, gx2, gx3, Pfront.x1, Pfront.x2, Pfront.x3, plt, GPar.cor.plt)

```


```{r Utopia distance: Marginalization}
# Use the final GP model with the full dataset
mod.rad = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'rad')
mod.ang = fill.sample.mod(GPar.data = GPar.all, input.name = c('x1', 'x2', 'x3'), output.name = 'theta')

# Obtain marginalized probabilities with a fine resolution grid (50 points). Calculate the integral with a Monte Carlo sampling of 500 samples each.
resolution = 50; MCsamp = 1500
x1.rng = range(GPar.all$x1); x2.rng = range(GPar.all$x2); x3.rng = range(GPar.all$x3)
x1.seq = seq(from = min(x1.rng), to = max(x1.rng), length.out = resolution)
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x3.rng), to = max(x3.rng), length.out = resolution)

Infer.x1 = data.frame(); Infer.x2 = data.frame(); Infer.x3 = data.frame()

for(i in 1:resolution){
  # x1
  fill.frame = data.frame(x1 = x1.seq[i], x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x1 = rbind(Infer.x1, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), var = 'x1', cond = 'none'))
  
  # x2
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x2 = rbind(Infer.x2, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'none'))
  
  # x3
  fill.frame = data.frame(x3 = x3.seq[i], x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)),
                          x2 = runif(n = MCsamp, min = min(x2.rng), max = max(x2.rng)))
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x3 = rbind(Infer.x3, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), var = 'x3', cond = 'none'))
}

ggplot() +
  geom_path(data = Infer.x1, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x2, mapping = aes(x = x, y = prob, color = var)) +
  geom_path(data = Infer.x3, mapping = aes(x = x, y = prob, color = var)) +
  labs(x = expression('x'), y = 'Probability of Acceptance', 
       subtitle = expression('Accept if r < 1 & > 80% f'[1]*' priority')) +
  scale_color_manual(name = 'Variable', values = c('x1' = 'red', 'x2' = 'blue', 'x3' = 'maroon'), 
                    labels = c('x1' = expression('x'[1]), 'x2' = expression('x'[2]),'x3' = expression('x'[3]))) +
  theme_bw()

```

Interestingly, while there are multiple peaks for $x_2$, it is only the peak on the right. This is due to the prioritization of $F_1$ compared to $F_2$ and not an issue with the model. The fact that $x_2$ and $x_3$ are treated differently is still an issue.

```{r Utopia distance: Partial Conditional Probabilities}
# Determining the ranges
# Function to find the optimal range given the probability of acceptance
p.lims = function(data.prob, p.target){
  # data.prob is a dataframe with variables x and prob, which are the values and probability of acceptance
  
  # Limits are the range where the probability is at least half the maximim
  # p.target = 0.5*max(data.prob$prob)
  
  # Initial limits
  x.lims = range(filter(data.prob, prob >= p.target)$x)
  
  # Linear interpolate to find the proper point, assuming that the limit is not already the minimum possible value
  if(x.lims[1] > min(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[1]))
    sub = data.prob[c(pos-1, pos), ]
    mod = lm(x~prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[1] = res
  }
  if(x.lims[2] < max(data.prob$x)){
    pos = which.min(abs(data.prob$x - x.lims[2]))
    sub = data.prob[c(pos, pos+1), ]
    mod = lm(x ~ prob, data = sub)
    res = predict(object = mod, newdata = data.frame(prob = p.target))
    x.lims[2] = res
    rm(mod)
  }
  return(x.lims)
}
x1.lims = p.lims(data.prob = Infer.x1, p.target = 0.5*max(Infer.x1$prob))
x3.lims = p.lims(data.prob = Infer.x3, p.target = 0.5*max(Infer.x3$prob))

## Given that x1 or x3 is optimal, find the next variable in the series: x2
resolution = 50; MCsamp = 1500
x2.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x2.x1 = data.frame(); Infer.x2.x3 = data.frame()

for(i in 1:resolution){
  # x2 | x1
  fill.frame = data.frame(x2 = x2.seq[i], x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = runif(n = MCsamp, min = min(x3.rng), max = max(x3.rng)))
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x2.x1 = rbind(Infer.x2.x1, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x1'))
  # x2 | x3
  fill.frame = data.frame(x2 = x2.seq[i], x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = runif(n = MCsamp, min = min(x1.rng), max = max(x1.rng)))
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x2.x3 = rbind(Infer.x2.x3, data.frame(x = x2.seq[i], prob = mean(fill.frame$p.del1), var = 'x2', cond = 'x3'))
}

ggplot(rbind(Infer.x2.x3, Infer.x2.x1)) +
  geom_path(mapping = aes(x = x, y = prob, color = cond)) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'),
       color = 'Given optimal') +
  theme_bw()

# Instead of a simple range, the multimodal peaks of these probability curves mean a simple min/max bound can be used. While a nearest-neighbor approximation would work, a faster solution is to find 3 separate ranges.
x2.lims.x3.rng2 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x3.rng3 = p.lims(data.prob = filter(Infer.x2.x3, x > 0.75), 
                         p.target = max(Infer.x2.x3$prob)*0.5)
x2.lims.x1.rng2 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.25, x < 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
x2.lims.x1.rng3 = p.lims(data.prob = filter(Infer.x2.x1, x > 0.75), 
                         p.target = max(Infer.x2.x1$prob)*0.5)
# The number of points to collect from each range is proportional to their relative widths

## Given that x1 or x3 is optimal, find the next variable in the series: x2
# resolution = 50; MCsamp = 1500
MCsamp.x2.x3.rng2 = round(1500*diff(x2.lims.x3.rng2)/
                            (diff(x2.lims.x3.rng1) + diff(x2.lims.x3.rng2) + diff(x2.lims.x3.rng3)))
MCsamp.x2.x3.rng3 = 1500 - MCsamp.x2.x3.rng2
MCsamp.x2.x1.rng2 = round(1500*diff(x2.lims.x1.rng2)/
                            (diff(x2.lims.x1.rng1) + diff(x2.lims.x1.rng2) + diff(x2.lims.x1.rng3)))
MCsamp.x2.x1.rng3 = 1500 - MCsamp.x2.x1.rng2

x1.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)
x3.seq = seq(from = min(x2.rng), to = max(x2.rng), length.out = resolution)

# Single conditional: x2|x1 or x2|x3
Infer.x3.x2.x1 = data.frame(); Infer.x1.x2.x3 = data.frame()
for(i in 1:resolution){
  # x3 | x2, x1
  x2.fill = c(runif(n = MCsamp.x2.x1.rng2, min = min(x2.lims.x1.rng2), max = max(x2.lims.x1.rng2)),
              runif(n = MCsamp.x2.x1.rng3, min = min(x2.lims.x1.rng3), max = max(x2.lims.x1.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x1 = runif(n = MCsamp, min = min(x1.lims), max = max(x1.lims)),
                          x3 = x3.seq[i])
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x3.x2.x1 = rbind(Infer.x3.x2.x1, data.frame(x = x3.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x3', cond = 'x2, x1'))
  # x1 | x2, x3
  x2.fill = c(runif(n = MCsamp.x2.x3.rng2, min = min(x2.lims.x3.rng2), max = max(x2.lims.x3.rng2)),
              runif(n = MCsamp.x2.x3.rng3, min = min(x2.lims.x3.rng3), max = max(x2.lims.x3.rng3)))
  fill.frame = data.frame(x2 = x2.fill, x3 = runif(n = MCsamp, min = min(x3.lims), max = max(x3.lims)),
                          x1 = x1.seq[i])
  res1 = predict(object = mod.rad, newdata = fill.frame, type = 'UK')
  res2 = predict(object = mod.ang, newdata = fill.frame, type = 'UK')
  fill.frame$p.del1 = pnorm(q = 0, mean = res1$mean - 2, sd = res1$sd) * (1 - pnorm(q = 0, mean = res2$mean - 20, sd = res2$sd))
  Infer.x1.x2.x3 = rbind(Infer.x1.x2.x3, data.frame(x = x1.seq[i], prob = mean(fill.frame$p.del1), 
                                                    var = 'x1', cond = 'x2, x3'))
}

# Combine results for visualization
Infer.plt = rbind(Infer.x1, Infer.x2, Infer.x3, Infer.x2.x1, Infer.x2.x3, 
                  Infer.x1.x2.x3, Infer.x3.x2.x1);
write.csv(Infer.plt, 'Marginals_Accept_Radius.csv')
rm(res1, res2, x2.fill, fill.frame)
```



```{r}
Infer.plt = read.csv(file = 'Marginals_Accept_Radius.csv')

ggplot(filter(Infer.plt, cond == 'none')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(var))) +
  labs(x = expression('x'), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Variable') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, cond == 'x2, x3' | cond == 'x2, x1' | cond == 'none', var != 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  facet_wrap(~var, nrow = 2) +
  labs(x = 'x', y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

ggplot(filter(Infer.plt, var == 'x2')) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  labs(x = expression('x'[2]), y = 'Probability of Acceptance', subtitle = expression('Accept if '*delta*' < 1'), 
       color = 'Partial Conditions') +
  scale_y_continuous(expand = c(0, 0.05)) + scale_x_continuous(expand = c(0, 0)) +
  theme_bw()

```

## Comparison of selection criteria by their marginals

```{r}
Margin.del = read.csv(file = 'Marginals_Accept_Delta1.csv')
Margin.cut = read.csv(file = 'Marginals_Accept_Threshold.csv')
Margin.rad = read.csv(file = 'Marginals_Accept_Radius.csv')

Margin.del$criteria = 'Normalized Distance'
Margin.cut$criteria = 'Objective Value'
Margin.rad$criteria = 'Utopia Distance and F1 Priority'

Margin = rbind(Margin.del, Margin.cut, Margin.rad)

ggplot(Margin) +
  geom_path(mapping = aes(x = x, y = prob, color = as.factor(cond))) +
  facet_grid(var~criteria) + 
  labs(x = 'Input Value', y = 'Probability of Acceptance', subtitle = 'Candidate Screening', 
       color = 'Partial\nConditions') +
  scale_y_continuous(expand = c(0, 0.05)) + 
  scale_x_continuous(expand = c(0, 0.05), labels = c('0.0', '0.25', '0.5', '0.75', '1.0')) +
  theme_bw() + theme(panel.spacing.x = unit(0.5, "lines"))

```

